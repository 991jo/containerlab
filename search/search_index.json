{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"With the growing number of containerized Network Operating Systems grows the demand to easily run them in the user-defined, versatile lab topologies. Unfortunately, container orchestration tools like docker/podman/etc are not a good fit for that purpose, as they do not allow a user to easily create p2p connections between the containers. Containerlab provides a framework for orchestrating networking labs with containers. It starts the containers, builds a virtual wiring between them to create lab topologies of users choice and manages labs lifecycle. Containerlab focuses on containerized Network Operating Systems which are typically used to test network features and designs, such as: Nokia SR-Linux Arista cEOS Azure SONiC Juniper cRPD In addition to native containerized NOSes, containerlab can launch traditional virtual-machine based routers using vrnetlab integration : Nokia virtual SR OS (vSim/VSR) Juniper vMX Cisco IOS XRv9k And, of course, containerlab is perfectly capable of wiring up arbitrary linux containers which can host your network applications, virtual functions or simply be a test client. With all that, containerlab provides a single IaaC interface to manage labs which can span all the needed variants of nodes: Features # IaaC approach Declarative way of defining the labs by means of the topology definition files . Network Operating Systems centric Focus on containerized Network Operating Systems. The sophisticated startup requirements of various NOS containers are abstracted with kinds which allows the user to focus on the use cases, rather than infrastructure. Multi-vendor, multi-platform With the vrnetlab integration it is possible to get the best of two worlds - running virtualized and containerized nodes alike with the same IaaC approach and workflows. Lab orchestration Starting the containers and interconnecting them alone is already good, but containerlab packages even more features like managing lab lifecycle: deploy , destroy , save , inspect , graph operations. Scaled labs generator With generate command containerlab makes it possible to define/launch CLOS-based topologies of arbitrary scale. Just say how many tiers you need and how big each tier is, the rest will be done in a split second. Simplicity and convenience are keys Starting from frictionless installation and upgrade capabilities and ranging to the behind-the-scenes link wiring machinery , containerlab does its best for you to focus on the use cases, rather than infrastructure setup. Fast Blazing fast way to create container based labs on any Debian- or RHEL-like system. Automated TLS certificates provisioning The nodes which require TLS certs will get them automatically on start. Documentation is a first-class citizen We do not let our users guess by making a complete, concise and clean documentation . Lab catalog The \"most-wanted\" lab topologies are documented and included with containerlab installation. Based on this cherry-picked selection you can start crafting the labs answering your needs. Use cases # Labs and Demos Containerlab was meant to be a tool for provisioning networking labs built with containers. It is free, open and ubiquitous. No software apart from Docker is required! As with any lab environment it allows the users to validate features, topologies, perform interop testing, datapath testing, etc. It is also a perfect companion for your next demo. Deploy the lab fast, with all its configuration stored as a code -> destroy when done. If needed, repeat. Testing and CI Because of the containerlab's single-binary packaging and code-based lab definition files, it was never that easy to spin up a test bed for CI. Telemetry validation By coupling with modern telemetry stacks containerlab labs make a perfect fit for Telemetry use cases validation. Spin up a lab with containerized network functions with a telemetry on the side, and validate/demonstrate comprehensive telemetry use cases.","title":"Home"},{"location":"#features","text":"IaaC approach Declarative way of defining the labs by means of the topology definition files . Network Operating Systems centric Focus on containerized Network Operating Systems. The sophisticated startup requirements of various NOS containers are abstracted with kinds which allows the user to focus on the use cases, rather than infrastructure. Multi-vendor, multi-platform With the vrnetlab integration it is possible to get the best of two worlds - running virtualized and containerized nodes alike with the same IaaC approach and workflows. Lab orchestration Starting the containers and interconnecting them alone is already good, but containerlab packages even more features like managing lab lifecycle: deploy , destroy , save , inspect , graph operations. Scaled labs generator With generate command containerlab makes it possible to define/launch CLOS-based topologies of arbitrary scale. Just say how many tiers you need and how big each tier is, the rest will be done in a split second. Simplicity and convenience are keys Starting from frictionless installation and upgrade capabilities and ranging to the behind-the-scenes link wiring machinery , containerlab does its best for you to focus on the use cases, rather than infrastructure setup. Fast Blazing fast way to create container based labs on any Debian- or RHEL-like system. Automated TLS certificates provisioning The nodes which require TLS certs will get them automatically on start. Documentation is a first-class citizen We do not let our users guess by making a complete, concise and clean documentation . Lab catalog The \"most-wanted\" lab topologies are documented and included with containerlab installation. Based on this cherry-picked selection you can start crafting the labs answering your needs.","title":"Features"},{"location":"#use-cases","text":"Labs and Demos Containerlab was meant to be a tool for provisioning networking labs built with containers. It is free, open and ubiquitous. No software apart from Docker is required! As with any lab environment it allows the users to validate features, topologies, perform interop testing, datapath testing, etc. It is also a perfect companion for your next demo. Deploy the lab fast, with all its configuration stored as a code -> destroy when done. If needed, repeat. Testing and CI Because of the containerlab's single-binary packaging and code-based lab definition files, it was never that easy to spin up a test bed for CI. Telemetry validation By coupling with modern telemetry stacks containerlab labs make a perfect fit for Telemetry use cases validation. Spin up a lab with containerized network functions with a telemetry on the side, and validate/demonstrate comprehensive telemetry use cases.","title":"Use cases"},{"location":"install/","text":"Containerlab is distributed as a Linux deb/rpm package and can be installed on any Debian- or RHEL-like distributive in a matter of a few seconds. Pre-requisites # The following requirements must be satisfied in order to let containerlab tool run successfully: A user should have sudo privileges to run containerlab. Docker must be installed. Import container images (e.g. Nokia SR Linux, Arista cEOS) which are not downloadable from a container registry. Containerlab will try to pull images at runtime if they do not exist locally. Install script # Containerlab can be installed using the installation script which detects the operating system type and installs the relevant package: Note Containerlab is distributed via deb/rpm packages, thus only Debian- and RHEL-like distributives are supported. # download and install the latest release (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" # download a specific version - 0.10.3 (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" -- -v 0 .10.3 # with wget bash -c \" $( wget -qO - https://get-clab.srlinux.dev ) \" Package managers # It is possible to install official containerlab releases via public APT/YUM repository. APT echo \"deb [trusted=yes] https://apt.fury.io/netdevops/ /\" | \\ sudo tee -a /etc/apt/sources.list.d/netdevops.list apt update && apt install containerlab YUM yum-config-manager --add-repo=https://yum.fury.io/netdevops/ && \\ echo \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/yum.fury.io_netdevops_.repo yum install containerlab Alternative installation options Alternatively, users can manually download the deb/rpm package from the Github releases page. example: # manually install latest release with package managers LATEST = $( curl -s https://github.com/srl-wim/container-lab/releases/latest | sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/' ) # with yum yum install \"https://github.com/srl-wim/container-lab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _linux_amd64.rpm\" # with dpkg curl -sL -o /tmp/clab.deb \"https://github.com/srl-wim/container-lab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _linux_amd64.deb\" && dpkg -i /tmp/clab.deb # install specific release with yum yum install https://github.com/srl-wim/container-lab/releases/download/v0.7.0/containerlab_0.7.0_linux_386.rpm The package installer will put the containerlab binary in the /usr/bin directory as well as create the /usr/bin/clab -> /usr/bin/containerlab symlink. The symlink allows the users to save on typing when they use containerlab: clab <command> . Upgrade # To upgrade containerlab to the latest available version issue the following command: containerlab version upgrade This command will fetch the installation script and will upgrade the tool to its most recent version. or leverage apt / yum utilities if containerlab repo was added as explained in the Package managers section.","title":"Installation"},{"location":"install/#pre-requisites","text":"The following requirements must be satisfied in order to let containerlab tool run successfully: A user should have sudo privileges to run containerlab. Docker must be installed. Import container images (e.g. Nokia SR Linux, Arista cEOS) which are not downloadable from a container registry. Containerlab will try to pull images at runtime if they do not exist locally.","title":"Pre-requisites"},{"location":"install/#install-script","text":"Containerlab can be installed using the installation script which detects the operating system type and installs the relevant package: Note Containerlab is distributed via deb/rpm packages, thus only Debian- and RHEL-like distributives are supported. # download and install the latest release (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" # download a specific version - 0.10.3 (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" -- -v 0 .10.3 # with wget bash -c \" $( wget -qO - https://get-clab.srlinux.dev ) \"","title":"Install script"},{"location":"install/#package-managers","text":"It is possible to install official containerlab releases via public APT/YUM repository. APT echo \"deb [trusted=yes] https://apt.fury.io/netdevops/ /\" | \\ sudo tee -a /etc/apt/sources.list.d/netdevops.list apt update && apt install containerlab YUM yum-config-manager --add-repo=https://yum.fury.io/netdevops/ && \\ echo \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/yum.fury.io_netdevops_.repo yum install containerlab Alternative installation options Alternatively, users can manually download the deb/rpm package from the Github releases page. example: # manually install latest release with package managers LATEST = $( curl -s https://github.com/srl-wim/container-lab/releases/latest | sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/' ) # with yum yum install \"https://github.com/srl-wim/container-lab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _linux_amd64.rpm\" # with dpkg curl -sL -o /tmp/clab.deb \"https://github.com/srl-wim/container-lab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _linux_amd64.deb\" && dpkg -i /tmp/clab.deb # install specific release with yum yum install https://github.com/srl-wim/container-lab/releases/download/v0.7.0/containerlab_0.7.0_linux_386.rpm The package installer will put the containerlab binary in the /usr/bin directory as well as create the /usr/bin/clab -> /usr/bin/containerlab symlink. The symlink allows the users to save on typing when they use containerlab: clab <command> .","title":"Package managers"},{"location":"install/#upgrade","text":"To upgrade containerlab to the latest available version issue the following command: containerlab version upgrade This command will fetch the installation script and will upgrade the tool to its most recent version. or leverage apt / yum utilities if containerlab repo was added as explained in the Package managers section.","title":"Upgrade"},{"location":"quickstart/","text":"Installation # Getting containerlab is as easy as it gets. Thanks to the trivial installation procedure it can be set up in a matter of a few seconds on any RHEL or Debian based OS 1 . # download and install the latest release sudo curl -sL https://get-clab.srlinux.dev | sudo bash Topology definition file # Once installed, containerlab manages the labs defined in the so-called topology definition files . A user can write a topology definition file of their own, or use the various examples we provide within the containerlab package. In this tutorial we will be using one of the provided labs which consists of two Nokia SR Linux nodes connected one to another. The lab topology is defined in the srl02.yml file. To make use of this lab example, we need to copy the corresponding files to our working directory: # create a directory for containerlab mkdir ~/clab-quickstart cd ~/clab-quickstart # copy over the srl02 lab files cp -a /etc/containerlab/lab-examples/srl02/* . Let's have a look at how this lab's topology is defined: # contents of srl02.yml file # topology documentation: http://containerlab.srlinux.dev/lab-examples/two-srls/ name : srl02 topology : kinds : srl : type : ixr6 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] Info A topology definition deep-dive document provides a complete reference of the topology definition syntax. In the quickstart we keep it short, glancing over the key components of the file. Each lab/topology has a name . The lab topology is defined under the topology element. Topology is a set of nodes and links between them. The nodes are always of a certain [ kind ]manual/kinds/kinds.md). The kind defines the node configuration and behavior. Containerlab supports a fixed number of kinds . In the example above, the srl kind is one of the supported kinds and it has been provided with a few additional options in the topology.kinds.srl element. nodes are interconnected with links . Each link is defined by a set of endpoints . Container image # One of node's most important properties is the container image they use. In the example above the container image is set under the srl kind. Effectively, the nodes of srl kind will inherit this property and will use the srlinux image to boot from. The image name follows the same rules as the images you use with, for example, Docker client or k8s pods. For example, the provided srlinux image name assumes that the tag of the image is latest . Container images versions The provided lab examples use the images without a tag, i.e. image: srlinux . This means that the image with a latest tag must exist. A user needs to tag the image if the latest tag is missing. For example: docker tag srlinux:20.6.1-286 srlinux:latest License files # For the nodes/kinds which require a license to run (like Nokia SR Linux) the license element must specify a path to a valid license file. In the example we work with, the license path is set to license.key for srl kind. topology : kinds : srl : type : ixr6 image : srlinux license : license.key That means that containerlab will look for this file by the ${PWD}/license.key path. Before deploying our lab, we need to copy the file in the ~/clab-quickstart directory to make it available by the specified path. Deploying a lab # Now when we know what a basic topology file consists of, sorted out the container image name and license file, we can deploy this lab. To keep things easy and guessable, the command to deploy a lab is called deploy . # checking that topology and license files are present in ~/clab-quickstart \u276f ls license.key srl02.yml # checking that srlinux(:latest) image is available \u276f docker image ls srlinux:latest REPOSITORY TAG IMAGE ID CREATED SIZE srlinux latest 79019d14cfc7 3 months ago 1 .32GB # start the lab deployment by referencing the topology file containerlab deploy --topo srl02.yml After a couple of seconds you will see the summary of the deployed nodes: +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srl02-srl1 | dd5c5a8dc51a | srlinux | srl | | running | 172.20.20.5/24 | 2001:172:20:20::5/80 | | 2 | clab-srl02-srl2 | b623b3957f8f | srlinux | srl | | running | 172.20.20.4/24 | 2001:172:20:20::4/80 | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ The node name presented in the summary table is the fully qualified node name, it is built using the following pattern: clab-{{lab_name}}-{{node_name}} . Connecting to the nodes # Since the topology nodes are regular containers, you can connect to them just like to any other container. docker exec -it clab-srl02-srl1 bash For containerized network OSes like Nokia SR Linux or Arista cEOS you can connect with SSH by either using the management address assigned to the container: \u276f ssh admin@172.20.20.3 admin@172.20.20.3's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:srl1# or by using node's fully qualified names, for which containerlab creates /etc/hosts entries: ssh admin@clab-srl02-srl1 The following tab view aggregates the ways to open the NOS CLI per supported device: Nokia SR Linux # access CLI docker exec -it <name> sr_cli # access bash docker exec -it <name> bash Arista cEOS # access CLI docker exec -it <name> Cli Destroying a lab # To remove the lab, use the destroy command that takes a topology file as an argument: containerlab destroy --topo srl02.yml What next? # To get a broader view on the containerlab features and components, refer to the User manual section. Do not forget to check out the Lab examples section where we provide complete and ready-to-run topology definition files. This is a great starting point to explore containerlab by doing. Make sure to satisfy lab host pre-requisites \u21a9","title":"Quick start"},{"location":"quickstart/#installation","text":"Getting containerlab is as easy as it gets. Thanks to the trivial installation procedure it can be set up in a matter of a few seconds on any RHEL or Debian based OS 1 . # download and install the latest release sudo curl -sL https://get-clab.srlinux.dev | sudo bash","title":"Installation"},{"location":"quickstart/#topology-definition-file","text":"Once installed, containerlab manages the labs defined in the so-called topology definition files . A user can write a topology definition file of their own, or use the various examples we provide within the containerlab package. In this tutorial we will be using one of the provided labs which consists of two Nokia SR Linux nodes connected one to another. The lab topology is defined in the srl02.yml file. To make use of this lab example, we need to copy the corresponding files to our working directory: # create a directory for containerlab mkdir ~/clab-quickstart cd ~/clab-quickstart # copy over the srl02 lab files cp -a /etc/containerlab/lab-examples/srl02/* . Let's have a look at how this lab's topology is defined: # contents of srl02.yml file # topology documentation: http://containerlab.srlinux.dev/lab-examples/two-srls/ name : srl02 topology : kinds : srl : type : ixr6 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] Info A topology definition deep-dive document provides a complete reference of the topology definition syntax. In the quickstart we keep it short, glancing over the key components of the file. Each lab/topology has a name . The lab topology is defined under the topology element. Topology is a set of nodes and links between them. The nodes are always of a certain [ kind ]manual/kinds/kinds.md). The kind defines the node configuration and behavior. Containerlab supports a fixed number of kinds . In the example above, the srl kind is one of the supported kinds and it has been provided with a few additional options in the topology.kinds.srl element. nodes are interconnected with links . Each link is defined by a set of endpoints .","title":"Topology definition file"},{"location":"quickstart/#container-image","text":"One of node's most important properties is the container image they use. In the example above the container image is set under the srl kind. Effectively, the nodes of srl kind will inherit this property and will use the srlinux image to boot from. The image name follows the same rules as the images you use with, for example, Docker client or k8s pods. For example, the provided srlinux image name assumes that the tag of the image is latest . Container images versions The provided lab examples use the images without a tag, i.e. image: srlinux . This means that the image with a latest tag must exist. A user needs to tag the image if the latest tag is missing. For example: docker tag srlinux:20.6.1-286 srlinux:latest","title":"Container image"},{"location":"quickstart/#license-files","text":"For the nodes/kinds which require a license to run (like Nokia SR Linux) the license element must specify a path to a valid license file. In the example we work with, the license path is set to license.key for srl kind. topology : kinds : srl : type : ixr6 image : srlinux license : license.key That means that containerlab will look for this file by the ${PWD}/license.key path. Before deploying our lab, we need to copy the file in the ~/clab-quickstart directory to make it available by the specified path.","title":"License files"},{"location":"quickstart/#deploying-a-lab","text":"Now when we know what a basic topology file consists of, sorted out the container image name and license file, we can deploy this lab. To keep things easy and guessable, the command to deploy a lab is called deploy . # checking that topology and license files are present in ~/clab-quickstart \u276f ls license.key srl02.yml # checking that srlinux(:latest) image is available \u276f docker image ls srlinux:latest REPOSITORY TAG IMAGE ID CREATED SIZE srlinux latest 79019d14cfc7 3 months ago 1 .32GB # start the lab deployment by referencing the topology file containerlab deploy --topo srl02.yml After a couple of seconds you will see the summary of the deployed nodes: +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srl02-srl1 | dd5c5a8dc51a | srlinux | srl | | running | 172.20.20.5/24 | 2001:172:20:20::5/80 | | 2 | clab-srl02-srl2 | b623b3957f8f | srlinux | srl | | running | 172.20.20.4/24 | 2001:172:20:20::4/80 | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ The node name presented in the summary table is the fully qualified node name, it is built using the following pattern: clab-{{lab_name}}-{{node_name}} .","title":"Deploying a lab"},{"location":"quickstart/#connecting-to-the-nodes","text":"Since the topology nodes are regular containers, you can connect to them just like to any other container. docker exec -it clab-srl02-srl1 bash For containerized network OSes like Nokia SR Linux or Arista cEOS you can connect with SSH by either using the management address assigned to the container: \u276f ssh admin@172.20.20.3 admin@172.20.20.3's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:srl1# or by using node's fully qualified names, for which containerlab creates /etc/hosts entries: ssh admin@clab-srl02-srl1 The following tab view aggregates the ways to open the NOS CLI per supported device: Nokia SR Linux # access CLI docker exec -it <name> sr_cli # access bash docker exec -it <name> bash Arista cEOS # access CLI docker exec -it <name> Cli","title":"Connecting to the nodes"},{"location":"quickstart/#destroying-a-lab","text":"To remove the lab, use the destroy command that takes a topology file as an argument: containerlab destroy --topo srl02.yml","title":"Destroying a lab"},{"location":"quickstart/#what-next","text":"To get a broader view on the containerlab features and components, refer to the User manual section. Do not forget to check out the Lab examples section where we provide complete and ready-to-run topology definition files. This is a great starting point to explore containerlab by doing. Make sure to satisfy lab host pre-requisites \u21a9","title":"What next?"},{"location":"cmd/deploy/","text":"deploy command # Description # The deploy command spins up a lab using the topology expressed via topology definition file . Usage # containerlab [global-flags] deploy [local-flags] aliases: dep Flags # topology # With the global --topo | -t flag a user sets the path to the topology definition file that will be used to spin up a lab. name # With the global --name | -n flag a user sets a lab name. This value will override the lab name value passed in the topology definition file. reconfigure # The local --reconfigure flag instructs containerlab to first destroy the lab and all its directories and then start the deployment process. That will result in a clean (re)deployment where every configuration artefact will be generated (TLS, node config) from scratch. Without this flag present, containerlab will reuse the available configuration artifacts found in the lab directory. Refer to the configuration artifacts page to get more information on the lab directory contents. max-workers # With --max-workers flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create. Examples # # deploy a lab from mylab.yml file located in the same dir containerlab deploy -t mylab.yml # deploy a lab from mylab.yml file and regenerate all configuration artifacts containerlab deploy -t mylab.yml --reconfigure","title":"deploy"},{"location":"cmd/deploy/#deploy-command","text":"","title":"deploy command"},{"location":"cmd/deploy/#description","text":"The deploy command spins up a lab using the topology expressed via topology definition file .","title":"Description"},{"location":"cmd/deploy/#usage","text":"containerlab [global-flags] deploy [local-flags] aliases: dep","title":"Usage"},{"location":"cmd/deploy/#flags","text":"","title":"Flags"},{"location":"cmd/deploy/#topology","text":"With the global --topo | -t flag a user sets the path to the topology definition file that will be used to spin up a lab.","title":"topology"},{"location":"cmd/deploy/#name","text":"With the global --name | -n flag a user sets a lab name. This value will override the lab name value passed in the topology definition file.","title":"name"},{"location":"cmd/deploy/#reconfigure","text":"The local --reconfigure flag instructs containerlab to first destroy the lab and all its directories and then start the deployment process. That will result in a clean (re)deployment where every configuration artefact will be generated (TLS, node config) from scratch. Without this flag present, containerlab will reuse the available configuration artifacts found in the lab directory. Refer to the configuration artifacts page to get more information on the lab directory contents.","title":"reconfigure"},{"location":"cmd/deploy/#max-workers","text":"With --max-workers flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create.","title":"max-workers"},{"location":"cmd/deploy/#examples","text":"# deploy a lab from mylab.yml file located in the same dir containerlab deploy -t mylab.yml # deploy a lab from mylab.yml file and regenerate all configuration artifacts containerlab deploy -t mylab.yml --reconfigure","title":"Examples"},{"location":"cmd/destroy/","text":"destroy command # Description # The destroy command destroys a lab referenced by its topology definition file . Usage # containerlab [global-flags] destroy [local-flags] aliases: des Flags # topology # With the global --topo | -t flag a user sets the path to the topology definition file that will be used get the elements of a lab that will be destroyed. cleanup # The local --cleanup flag instructs containerlab to remove the lab directory and all its content. Without this flag present, containerlab will keep the lab directory and all files inside of it. Refer to the configuration artifacts page to get more information on the lab directory contents. graceful # To make containerlab attempt a graceful shutdown of the running containers, add the --graceful flag to destroy cmd. Without it, containers will be removed forcefully without even attempting to stop them. all # Destroy command provided with --all | -a flag will perform the deletion of all the labs running on the container host. It will not touch containers launched manually. Examples # # destroy a lab based on mylab.yml topology file located in the same dir containerlab destroy -t mylab.yml # destroy a lab and also remove the Lab Directory containerlab destroy -t mylab.yml --cleanup # destroy all labs deployed with containerlab # using shortcut names clab des -a","title":"destroy"},{"location":"cmd/destroy/#destroy-command","text":"","title":"destroy command"},{"location":"cmd/destroy/#description","text":"The destroy command destroys a lab referenced by its topology definition file .","title":"Description"},{"location":"cmd/destroy/#usage","text":"containerlab [global-flags] destroy [local-flags] aliases: des","title":"Usage"},{"location":"cmd/destroy/#flags","text":"","title":"Flags"},{"location":"cmd/destroy/#topology","text":"With the global --topo | -t flag a user sets the path to the topology definition file that will be used get the elements of a lab that will be destroyed.","title":"topology"},{"location":"cmd/destroy/#cleanup","text":"The local --cleanup flag instructs containerlab to remove the lab directory and all its content. Without this flag present, containerlab will keep the lab directory and all files inside of it. Refer to the configuration artifacts page to get more information on the lab directory contents.","title":"cleanup"},{"location":"cmd/destroy/#graceful","text":"To make containerlab attempt a graceful shutdown of the running containers, add the --graceful flag to destroy cmd. Without it, containers will be removed forcefully without even attempting to stop them.","title":"graceful"},{"location":"cmd/destroy/#all","text":"Destroy command provided with --all | -a flag will perform the deletion of all the labs running on the container host. It will not touch containers launched manually.","title":"all"},{"location":"cmd/destroy/#examples","text":"# destroy a lab based on mylab.yml topology file located in the same dir containerlab destroy -t mylab.yml # destroy a lab and also remove the Lab Directory containerlab destroy -t mylab.yml --cleanup # destroy all labs deployed with containerlab # using shortcut names clab des -a","title":"Examples"},{"location":"cmd/generate/","text":"generate command # Description # The generate command generates the topology definition file based on the user input provided via CLI flags. With this command it is possible to generate definition file for a CLOS fabric by just providing the number of nodes on each tier. The generated topology can be saved in a file or immediately scheduled for deployment. It is assumed, that the interconnection between the tiers is done in a full-mesh fashion. Such as tier1 nodes are fully meshed with tier2, tier2 is meshed with tier3 and so on. Usage # containerlab [global-flags] generate [local-flags] aliases: gen Flags # name # With the global --name | -n flag a user sets the name of the lab that will be generated. nodes # The user configures the CLOS fabric topology by using the --nodes flag. The flag value is a comma separated list of CLOS tiers where each tier is defined by the number of nodes, its kind and type. Multiple --node flags can be specified. For example, the following flag value will define a 2-tier CLOS fabric with tier1 (leafs) consists of 4x SR Linux containers of IXR6 type and the 2x Arista cEOS spines: 4:srl:ixr6,2:ceos Note, that the default kind is srl , so you can omit the kind for SR Linux node. The same nodes value can be expressed like that: 4:ixr6,2:ceos kind # With --kind flag it is possible to set the default kind that will be set for the nodes which do not have a kind specified in the --nodes flag. For example the following value will generate a 3-tier CLOS fabric of cEOS nodes: # cEOS fabric containerlab gen -n 3tier --kind ceos --nodes 4 ,2,1 # since SR Linux kind is assumed by default # SRL fabric command is even shorter containerlab gen -n 3tier --nodes 4 ,2,1 image # Use --image flag to specify the container image that should be used by a given kind. The value of this flag follows the kind=image pattern. For example, to set the container image ceos:4.21.F for the ceos kind the flag will be: --image ceos=ceos:4.21.F . To set images for multiple kinds repeat the flag: --image srl=srlinux:latest --image ceos=ceos:4.21.F or use the comma separated form: --image srl=srlinux:latest,ceos=ceos:latest If the kind information is not provided in the image flag, the kind value will be taken from the --kind flag. license # With --license flag it is possible to set the license path that should be used by a given kind. The value of this flag follows the kind=path pattern. For example, to set the license path for the srl kind: --license srl=/tmp/license.key . To set license for multiple kinds repeat the flag: --license <kind1>=/path1 --image <kind2>=/path2 or use the comma separated form: --license <kind1>=/path1,<kind2>=/path2 deploy # When --deploy flag is present, the lab deployment process starts using the generated topology definition file. The generated definition file is first saved by the path set with --file or, if file path is not set, by the default path of <lab-name>.yml . Then the equivalent of the deploy -t <file> --reconfigure command is executed. max-workers # With --max-workers flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create. If during the deployment of a large scaled lab you see errors about max number of opened files reached, limit the max workers with this flag. file # With --file flag its possible to save the generated topology definition in a file by a given path. node-prefix # With --node-prefix flag a user sets the name prefix of every node in a lab. Nodes will be named by the following template: <node-prefix>-<tier>-<node-number> . So a node named node1-3 means this is the third node in a first tier of a topology. Default prefix: node . group-prefix # With --group-prefix it is possible to change the Group value of a node. Group information is used in the topology graph rendering. network # With --network flag a user sets the name of the management network that will be created by container orchestration system such as docker. Default: clab . ipv4-subnet | ipv6-subnet # With --ipv4-subnet and ipv6-subnet its possible to change the address ranges of the management network. Nodes will receive IP addresses from these ranges if they are configured with DHCP. Examples # # generate and deploy a lab topology for 3-tier CLOS network # with 8 leafs, 4 spines and 2 superspines # all using Nokia SR Linux nodes with license and image provided. # Note that `srl` kind in the image and license flags might be omitted, # as it is implied by default) containerlab generate --name 3tier --image srl = srlinux:latest \\ --license srl = license.key \\ --nodes 8 ,4,2 --deploy","title":"generate"},{"location":"cmd/generate/#generate-command","text":"","title":"generate command"},{"location":"cmd/generate/#description","text":"The generate command generates the topology definition file based on the user input provided via CLI flags. With this command it is possible to generate definition file for a CLOS fabric by just providing the number of nodes on each tier. The generated topology can be saved in a file or immediately scheduled for deployment. It is assumed, that the interconnection between the tiers is done in a full-mesh fashion. Such as tier1 nodes are fully meshed with tier2, tier2 is meshed with tier3 and so on.","title":"Description"},{"location":"cmd/generate/#usage","text":"containerlab [global-flags] generate [local-flags] aliases: gen","title":"Usage"},{"location":"cmd/generate/#flags","text":"","title":"Flags"},{"location":"cmd/generate/#name","text":"With the global --name | -n flag a user sets the name of the lab that will be generated.","title":"name"},{"location":"cmd/generate/#nodes","text":"The user configures the CLOS fabric topology by using the --nodes flag. The flag value is a comma separated list of CLOS tiers where each tier is defined by the number of nodes, its kind and type. Multiple --node flags can be specified. For example, the following flag value will define a 2-tier CLOS fabric with tier1 (leafs) consists of 4x SR Linux containers of IXR6 type and the 2x Arista cEOS spines: 4:srl:ixr6,2:ceos Note, that the default kind is srl , so you can omit the kind for SR Linux node. The same nodes value can be expressed like that: 4:ixr6,2:ceos","title":"nodes"},{"location":"cmd/generate/#kind","text":"With --kind flag it is possible to set the default kind that will be set for the nodes which do not have a kind specified in the --nodes flag. For example the following value will generate a 3-tier CLOS fabric of cEOS nodes: # cEOS fabric containerlab gen -n 3tier --kind ceos --nodes 4 ,2,1 # since SR Linux kind is assumed by default # SRL fabric command is even shorter containerlab gen -n 3tier --nodes 4 ,2,1","title":"kind"},{"location":"cmd/generate/#image","text":"Use --image flag to specify the container image that should be used by a given kind. The value of this flag follows the kind=image pattern. For example, to set the container image ceos:4.21.F for the ceos kind the flag will be: --image ceos=ceos:4.21.F . To set images for multiple kinds repeat the flag: --image srl=srlinux:latest --image ceos=ceos:4.21.F or use the comma separated form: --image srl=srlinux:latest,ceos=ceos:latest If the kind information is not provided in the image flag, the kind value will be taken from the --kind flag.","title":"image"},{"location":"cmd/generate/#license","text":"With --license flag it is possible to set the license path that should be used by a given kind. The value of this flag follows the kind=path pattern. For example, to set the license path for the srl kind: --license srl=/tmp/license.key . To set license for multiple kinds repeat the flag: --license <kind1>=/path1 --image <kind2>=/path2 or use the comma separated form: --license <kind1>=/path1,<kind2>=/path2","title":"license"},{"location":"cmd/generate/#deploy","text":"When --deploy flag is present, the lab deployment process starts using the generated topology definition file. The generated definition file is first saved by the path set with --file or, if file path is not set, by the default path of <lab-name>.yml . Then the equivalent of the deploy -t <file> --reconfigure command is executed.","title":"deploy"},{"location":"cmd/generate/#max-workers","text":"With --max-workers flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create. If during the deployment of a large scaled lab you see errors about max number of opened files reached, limit the max workers with this flag.","title":"max-workers"},{"location":"cmd/generate/#file","text":"With --file flag its possible to save the generated topology definition in a file by a given path.","title":"file"},{"location":"cmd/generate/#node-prefix","text":"With --node-prefix flag a user sets the name prefix of every node in a lab. Nodes will be named by the following template: <node-prefix>-<tier>-<node-number> . So a node named node1-3 means this is the third node in a first tier of a topology. Default prefix: node .","title":"node-prefix"},{"location":"cmd/generate/#group-prefix","text":"With --group-prefix it is possible to change the Group value of a node. Group information is used in the topology graph rendering.","title":"group-prefix"},{"location":"cmd/generate/#network","text":"With --network flag a user sets the name of the management network that will be created by container orchestration system such as docker. Default: clab .","title":"network"},{"location":"cmd/generate/#ipv4-subnet-ipv6-subnet","text":"With --ipv4-subnet and ipv6-subnet its possible to change the address ranges of the management network. Nodes will receive IP addresses from these ranges if they are configured with DHCP.","title":"ipv4-subnet | ipv6-subnet"},{"location":"cmd/generate/#examples","text":"# generate and deploy a lab topology for 3-tier CLOS network # with 8 leafs, 4 spines and 2 superspines # all using Nokia SR Linux nodes with license and image provided. # Note that `srl` kind in the image and license flags might be omitted, # as it is implied by default) containerlab generate --name 3tier --image srl = srlinux:latest \\ --license srl = license.key \\ --nodes 8 ,4,2 --deploy","title":"Examples"},{"location":"cmd/graph/","text":"graph command # Description # The graph command generates graphical representations of the topology. Two graphing options are available: an HTML page with embedded graphics generated by containerlab based on a Go HTML template a graph description file in dot format that can be rendered using Graphviz or viewed online . HTML # The HTML based graph is the default graphing option. The topology will be graphed and served online using the embedded web server. The graph is created by rendering a Go HTML template against a data structure containing the topology name as well as a json string where 2 lists are present: nodes and links . nodes contains data about the lab nodes, such as name, kind, type, image, state, IP addresses,... links contains the list of links defined by source node and target node, as well as the endpoint names example of the json string { \"nodes\" : [ { \"name\" : \"node1-1\" , \"image\" : \"srlinux:20.6.1-286\" , \"kind\" : \"srl\" , \"group\" : \"tier-1\" , \"state\" : \"running/Up 21 seconds\" , \"ipv4_address\" : \"172.23.23.3/24\" , \"ipv6_address\" : \"2001:172:23:23::3/80\" }, // omi tte d res t o f n odes ], \"links\" : [ { \"source\" : \"node1-2\" , \"source_endpoint\" : \"e1-1\" , \"target\" : \"node2-1\" , \"target_endpoint\" : \"e1-2\" }, { \"source\" : \"node2-1\" , \"source_endpoint\" : \"e1-4\" , \"target\" : \"node3-1\" , \"target_endpoint\" : \"e1-1\" }, // t he res t is omi tte d ] } Within the template, Javascript libraries such as d3js directed force graph or vis.js network can be used to generate custom topology graphs. containerlab comes with a (minimalistic) default template using d3js . After the graph generation, it's possible to move the nodes to a desired position and export the graph in PNG format. Graphviz # When graph command is called without the --srv flag, containerlab will generate a graph description file in dot format . The dot file can be used to view the graphical representation of the topology either by rendering the dot file into a PNG file or using online dot viewer . Online vs offline graphing # When HTML graph option is used, containerlab will try to build the topology graph by inspecting the running containers which are part of the lab. This essentially means, that the lab must be running. Although this method provides some additional details (like IP addresses), it is not always convenient to run a lab to see its graph. The other option is to use the topology file solely to build the graph. This is done by adding --offline flag. If --offline flag was not provided and no containers were found matching the lab name, containerlab will use the topo file only (as if offline mode was set). Usage # containerlab [global-flags] graph [local-flags] Flags # topology # With the global --topo | -t flag a user sets the path to the topology file that will be used to get the . srv # The --srv flag allows a user to customize the HTTP address and port for the web server. Default value is :50080 . A single path / is served, where the graph is generated based on either a default template or on the template supplied using --template . template # The --template flag allows to customize the HTML based graph by supplying a user defined template that will be rendered and exposed on the address specified by --srv . dot # With --dot flag provided containerlab will generate the dot file instead of serving the topology with embedded HTTP server. Examples # # render a graph from running lab or topo file if lab is not running# # using HTML graph option with default server address :50080 containerlab graph --topo /path/to/topo1.yaml # start an http server on :3002 where topo1 graph will be rendered using a custom template my_template.html containerlab graph --topo /path/to/topo1.yaml --srv \":3002\" --template my_template.html","title":"graph"},{"location":"cmd/graph/#graph-command","text":"","title":"graph command"},{"location":"cmd/graph/#description","text":"The graph command generates graphical representations of the topology. Two graphing options are available: an HTML page with embedded graphics generated by containerlab based on a Go HTML template a graph description file in dot format that can be rendered using Graphviz or viewed online .","title":"Description"},{"location":"cmd/graph/#html","text":"The HTML based graph is the default graphing option. The topology will be graphed and served online using the embedded web server. The graph is created by rendering a Go HTML template against a data structure containing the topology name as well as a json string where 2 lists are present: nodes and links . nodes contains data about the lab nodes, such as name, kind, type, image, state, IP addresses,... links contains the list of links defined by source node and target node, as well as the endpoint names example of the json string { \"nodes\" : [ { \"name\" : \"node1-1\" , \"image\" : \"srlinux:20.6.1-286\" , \"kind\" : \"srl\" , \"group\" : \"tier-1\" , \"state\" : \"running/Up 21 seconds\" , \"ipv4_address\" : \"172.23.23.3/24\" , \"ipv6_address\" : \"2001:172:23:23::3/80\" }, // omi tte d res t o f n odes ], \"links\" : [ { \"source\" : \"node1-2\" , \"source_endpoint\" : \"e1-1\" , \"target\" : \"node2-1\" , \"target_endpoint\" : \"e1-2\" }, { \"source\" : \"node2-1\" , \"source_endpoint\" : \"e1-4\" , \"target\" : \"node3-1\" , \"target_endpoint\" : \"e1-1\" }, // t he res t is omi tte d ] } Within the template, Javascript libraries such as d3js directed force graph or vis.js network can be used to generate custom topology graphs. containerlab comes with a (minimalistic) default template using d3js . After the graph generation, it's possible to move the nodes to a desired position and export the graph in PNG format.","title":"HTML"},{"location":"cmd/graph/#graphviz","text":"When graph command is called without the --srv flag, containerlab will generate a graph description file in dot format . The dot file can be used to view the graphical representation of the topology either by rendering the dot file into a PNG file or using online dot viewer .","title":"Graphviz"},{"location":"cmd/graph/#online-vs-offline-graphing","text":"When HTML graph option is used, containerlab will try to build the topology graph by inspecting the running containers which are part of the lab. This essentially means, that the lab must be running. Although this method provides some additional details (like IP addresses), it is not always convenient to run a lab to see its graph. The other option is to use the topology file solely to build the graph. This is done by adding --offline flag. If --offline flag was not provided and no containers were found matching the lab name, containerlab will use the topo file only (as if offline mode was set).","title":"Online vs offline graphing"},{"location":"cmd/graph/#usage","text":"containerlab [global-flags] graph [local-flags]","title":"Usage"},{"location":"cmd/graph/#flags","text":"","title":"Flags"},{"location":"cmd/graph/#topology","text":"With the global --topo | -t flag a user sets the path to the topology file that will be used to get the .","title":"topology"},{"location":"cmd/graph/#srv","text":"The --srv flag allows a user to customize the HTTP address and port for the web server. Default value is :50080 . A single path / is served, where the graph is generated based on either a default template or on the template supplied using --template .","title":"srv"},{"location":"cmd/graph/#template","text":"The --template flag allows to customize the HTML based graph by supplying a user defined template that will be rendered and exposed on the address specified by --srv .","title":"template"},{"location":"cmd/graph/#dot","text":"With --dot flag provided containerlab will generate the dot file instead of serving the topology with embedded HTTP server.","title":"dot"},{"location":"cmd/graph/#examples","text":"# render a graph from running lab or topo file if lab is not running# # using HTML graph option with default server address :50080 containerlab graph --topo /path/to/topo1.yaml # start an http server on :3002 where topo1 graph will be rendered using a custom template my_template.html containerlab graph --topo /path/to/topo1.yaml --srv \":3002\" --template my_template.html","title":"Examples"},{"location":"cmd/inspect/","text":"inspect command # Description # The inspect command provides the information about the deployed labs. Usage # containerlab [global-flags] inspect [local-flags] Flags # all # With the local --all flag its possible to list all deployed labs in a single table. The output will also show the relative path to the topology file that was used to spawn this lab. The lab name and path values will be set for the first node of such lab, to reduce the clutter. Refer to the examples section for more details. topology | name # With the global --topo | -t or --name | -n flag a user specifies which particular lab they want to get the information about. format # The local --format flag enables different output stylings. By default the table view will be used. Currently, the only other format option is json that will produce the output in the JSON format. details # The inspect command produces a brief summary about the running lab components. It is also possible to get a full view on the running containers by adding --details flag. With this flag inspect command will output every bit of information about the running containers. This is what docker inspect command provides. Examples # # list all running labs on the host containerlab inspect --all +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | # | Topo Path | Lab Name | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | 1 | newlab.yml | newlab | clab-newlab-n1 | 3c8262034088 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.4/24 | 2001 :172:20:20::4/80 | | 2 | | | clab-newlab-n2 | 79c562b71997 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.5/24 | 2001 :172:20:20::5/80 | | 3 | srl02.yml | srl01 | clab-srl01-srl | 13c9e7543771 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | | 4 | | | clab-srl01-srl2 | 8cfca93b7b6f | srlinux:20.6.3-145 | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ # provide information about the running lab named srl02 containerlab inspect --name srlceos01 +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlceos01-ceos | 90bebb1e2c5f | ceos | ceos | | running | 172 .20.20.4/24 | 2001 :172:20:20::4/80 | | 2 | clab-srlceos01-srl | 82e9aa3c7e6b | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ # now in json format containerlab inspect --name srlceos01 -f json [ { \"lab_name\" : \"srlceos01\" , \"name\" : \"clab-srlceos01-srl\" , \"container_id\" : \"82e9aa3c7e6b\" , \"image\" : \"srlinux\" , \"kind\" : \"srl\" , \"state\" : \"running\" , \"ipv4_address\" : \"172.20.20.3/24\" , \"ipv6_address\" : \"2001:172:20:20::3/80\" } , { \"lab_name\" : \"srlceos01\" , \"name\" : \"clab-srlceos01-ceos\" , \"container_id\" : \"90bebb1e2c5f\" , \"image\" : \"ceos\" , \"kind\" : \"ceos\" , \"state\" : \"running\" , \"ipv4_address\" : \"172.20.20.4/24\" , \"ipv6_address\" : \"2001:172:20:20::4/80\" } ]","title":"inspect"},{"location":"cmd/inspect/#inspect-command","text":"","title":"inspect command"},{"location":"cmd/inspect/#description","text":"The inspect command provides the information about the deployed labs.","title":"Description"},{"location":"cmd/inspect/#usage","text":"containerlab [global-flags] inspect [local-flags]","title":"Usage"},{"location":"cmd/inspect/#flags","text":"","title":"Flags"},{"location":"cmd/inspect/#all","text":"With the local --all flag its possible to list all deployed labs in a single table. The output will also show the relative path to the topology file that was used to spawn this lab. The lab name and path values will be set for the first node of such lab, to reduce the clutter. Refer to the examples section for more details.","title":"all"},{"location":"cmd/inspect/#topology-name","text":"With the global --topo | -t or --name | -n flag a user specifies which particular lab they want to get the information about.","title":"topology | name"},{"location":"cmd/inspect/#format","text":"The local --format flag enables different output stylings. By default the table view will be used. Currently, the only other format option is json that will produce the output in the JSON format.","title":"format"},{"location":"cmd/inspect/#details","text":"The inspect command produces a brief summary about the running lab components. It is also possible to get a full view on the running containers by adding --details flag. With this flag inspect command will output every bit of information about the running containers. This is what docker inspect command provides.","title":"details"},{"location":"cmd/inspect/#examples","text":"# list all running labs on the host containerlab inspect --all +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | # | Topo Path | Lab Name | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | 1 | newlab.yml | newlab | clab-newlab-n1 | 3c8262034088 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.4/24 | 2001 :172:20:20::4/80 | | 2 | | | clab-newlab-n2 | 79c562b71997 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.5/24 | 2001 :172:20:20::5/80 | | 3 | srl02.yml | srl01 | clab-srl01-srl | 13c9e7543771 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | | 4 | | | clab-srl01-srl2 | 8cfca93b7b6f | srlinux:20.6.3-145 | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ # provide information about the running lab named srl02 containerlab inspect --name srlceos01 +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlceos01-ceos | 90bebb1e2c5f | ceos | ceos | | running | 172 .20.20.4/24 | 2001 :172:20:20::4/80 | | 2 | clab-srlceos01-srl | 82e9aa3c7e6b | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ # now in json format containerlab inspect --name srlceos01 -f json [ { \"lab_name\" : \"srlceos01\" , \"name\" : \"clab-srlceos01-srl\" , \"container_id\" : \"82e9aa3c7e6b\" , \"image\" : \"srlinux\" , \"kind\" : \"srl\" , \"state\" : \"running\" , \"ipv4_address\" : \"172.20.20.3/24\" , \"ipv6_address\" : \"2001:172:20:20::3/80\" } , { \"lab_name\" : \"srlceos01\" , \"name\" : \"clab-srlceos01-ceos\" , \"container_id\" : \"90bebb1e2c5f\" , \"image\" : \"ceos\" , \"kind\" : \"ceos\" , \"state\" : \"running\" , \"ipv4_address\" : \"172.20.20.4/24\" , \"ipv6_address\" : \"2001:172:20:20::4/80\" } ]","title":"Examples"},{"location":"cmd/save/","text":"save command # Description # The save command perform configuration save for all the containers running in a lab. The exact command that performs configuration save depends on a given kind. The below table explains the method used for each kind: Kind Command Notes Nokia SR Linux sr_cli -d tools system configuration generate-checkpoint configuration is saved in a checkpoint file Arista cEOS not yet implemented Usage # containerlab [global-flags] save [local-flags] Flags # topology | name # With the global --topo | -t or --name | -n flag a user specifies from which lab to take the containers and perform the save configuration task. Examples # # save the configuration of the containers running in lab named srl02 \u276f container-lab save -n srl02 INFO [ 0001 ] clab-srl02-srl1: stdout: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:54.998Z' and comment '' INFO [ 0002 ] clab-srl02-srl2: stdout: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:56.444Z' and comment ''","title":"save"},{"location":"cmd/save/#save-command","text":"","title":"save command"},{"location":"cmd/save/#description","text":"The save command perform configuration save for all the containers running in a lab. The exact command that performs configuration save depends on a given kind. The below table explains the method used for each kind: Kind Command Notes Nokia SR Linux sr_cli -d tools system configuration generate-checkpoint configuration is saved in a checkpoint file Arista cEOS not yet implemented","title":"Description"},{"location":"cmd/save/#usage","text":"containerlab [global-flags] save [local-flags]","title":"Usage"},{"location":"cmd/save/#flags","text":"","title":"Flags"},{"location":"cmd/save/#topology-name","text":"With the global --topo | -t or --name | -n flag a user specifies from which lab to take the containers and perform the save configuration task.","title":"topology | name"},{"location":"cmd/save/#examples","text":"# save the configuration of the containers running in lab named srl02 \u276f container-lab save -n srl02 INFO [ 0001 ] clab-srl02-srl1: stdout: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:54.998Z' and comment '' INFO [ 0002 ] clab-srl02-srl2: stdout: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:56.444Z' and comment ''","title":"Examples"},{"location":"cmd/tools/disable-tx-offload/","text":"disable-tx-offload command # Description # The disable-tx-offload command under the tools command disables tx checksum offload for eth0 interface of a container referenced by its name. The need for disable-tx-offload might arise when you launch a container outside of containerlab or restart a container. Some nodes, like SR Linux, will require to have correct checksums in TCP packets, thus its needed to disable checksum offload on those containers for them to do checksum calculations instead of offloading it. Usage # containerlab tools disable-tx-offload [local-flags] Flags # container # With the local mandatory --container | -c flag a user specifies which container to remove tx offload in. Examples # # disable tx checksum on gnmic container \u276f clab tools disable-checksum -c clab-st-gnmic INFO [ 0000 ] getting container 'clab-st-gnmic' information INFO [ 0000 ] Tx checksum offload disabled for eth0 interface of clab-st-gnmic container","title":"disable-tx-offload"},{"location":"cmd/tools/disable-tx-offload/#disable-tx-offload-command","text":"","title":"disable-tx-offload command"},{"location":"cmd/tools/disable-tx-offload/#description","text":"The disable-tx-offload command under the tools command disables tx checksum offload for eth0 interface of a container referenced by its name. The need for disable-tx-offload might arise when you launch a container outside of containerlab or restart a container. Some nodes, like SR Linux, will require to have correct checksums in TCP packets, thus its needed to disable checksum offload on those containers for them to do checksum calculations instead of offloading it.","title":"Description"},{"location":"cmd/tools/disable-tx-offload/#usage","text":"containerlab tools disable-tx-offload [local-flags]","title":"Usage"},{"location":"cmd/tools/disable-tx-offload/#flags","text":"","title":"Flags"},{"location":"cmd/tools/disable-tx-offload/#container","text":"With the local mandatory --container | -c flag a user specifies which container to remove tx offload in.","title":"container"},{"location":"cmd/tools/disable-tx-offload/#examples","text":"# disable tx checksum on gnmic container \u276f clab tools disable-checksum -c clab-st-gnmic INFO [ 0000 ] getting container 'clab-st-gnmic' information INFO [ 0000 ] Tx checksum offload disabled for eth0 interface of clab-st-gnmic container","title":"Examples"},{"location":"lab-examples/bgp-vpls-nok-jun/","text":"Description BGP VPLS between Nokia SR OS and Juniper vMX Components Nokia SR OS, Juniper vMX Resource requirements 1 2 7-10 GB Lab location hellt/bgp-vpls-lab Topology file vpls.yml Version information 2 containerlab:0.10.1 , vr-sros:20.10.R1 , vr-vmx:20.4R1.12 , docker-ce:19.03.13 , vrnetlab 3 Description # This lab demonstrates how containerlab can be used in a classical networking labs where the prime focus is not on the containerized NOS, but on a classic VM-based routers. The topology created in this lab matches the network used in the BGP VPLS Deep Dive article: It allows readers to follow through the article with the author and create BGP VPLS service between the Nokia and Juniper routers using configuration snippets provided within the lab repository. As the article was done before Nokia introduced MD-CLI, the configuration snippets for SR OS were translated to MD-CLI. Quickstart # Ensure that your host supports virtualization and/or nested virtualization in case of a VM. Install 4 containerlab. Build if needed, vrnetlab container images for the routers used in the lab. Clone lab repository . Deploy the lab topology clab dep -t vpls.yml Resource requirements are provisional. Consult with the installation guides for additional information. Memory deduplication techniques like UKMS might help with RAM consumption. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9 Router images are built with vrnetlab aebe377 . To reproduce the image, checkout to this commit and build the relevant images. Note, that you might need to use containerlab of the version that is stated in the description. \u21a9 If installing the latest containerlab, make sure to use the latest hellt/vrnetlab project as well, as there might have been changes with the integration. If unsure, install the containerlab version that is specified in the lab description. \u21a9","title":"BGP VPLS between Nokia and Juniper"},{"location":"lab-examples/bgp-vpls-nok-jun/#description","text":"This lab demonstrates how containerlab can be used in a classical networking labs where the prime focus is not on the containerized NOS, but on a classic VM-based routers. The topology created in this lab matches the network used in the BGP VPLS Deep Dive article: It allows readers to follow through the article with the author and create BGP VPLS service between the Nokia and Juniper routers using configuration snippets provided within the lab repository. As the article was done before Nokia introduced MD-CLI, the configuration snippets for SR OS were translated to MD-CLI.","title":"Description"},{"location":"lab-examples/bgp-vpls-nok-jun/#quickstart","text":"Ensure that your host supports virtualization and/or nested virtualization in case of a VM. Install 4 containerlab. Build if needed, vrnetlab container images for the routers used in the lab. Clone lab repository . Deploy the lab topology clab dep -t vpls.yml Resource requirements are provisional. Consult with the installation guides for additional information. Memory deduplication techniques like UKMS might help with RAM consumption. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9 Router images are built with vrnetlab aebe377 . To reproduce the image, checkout to this commit and build the relevant images. Note, that you might need to use containerlab of the version that is stated in the description. \u21a9 If installing the latest containerlab, make sure to use the latest hellt/vrnetlab project as well, as there might have been changes with the integration. If unsure, install the containerlab version that is specified in the lab description. \u21a9","title":"Quickstart"},{"location":"lab-examples/ext-bridge/","text":"Description Connecting nodes via linux bridges Components Nokia SR Linux Resource requirements 1 1 2 GB Topology file br01.yml Name br01 Description # This lab consists of three Nokia SR Linux nodes connected to a linux bridge. Note containerlab will not create/remove the bridge interface on your behalf. bridge element must be part of the lab nodes. Consult with the topology file to see how to reference a bridge. Use cases # By introducing a link of bridge type to the containerlab topology, we are opening ourselves to some additional scenarios: interconnect nodes via a broadcast domain connect multiple fabrics together connect containerlab nodes to the applications/nodes running outside of the lab host Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"External bridge capability"},{"location":"lab-examples/ext-bridge/#description","text":"This lab consists of three Nokia SR Linux nodes connected to a linux bridge. Note containerlab will not create/remove the bridge interface on your behalf. bridge element must be part of the lab nodes. Consult with the topology file to see how to reference a bridge.","title":"Description"},{"location":"lab-examples/ext-bridge/#use-cases","text":"By introducing a link of bridge type to the containerlab topology, we are opening ourselves to some additional scenarios: interconnect nodes via a broadcast domain connect multiple fabrics together connect containerlab nodes to the applications/nodes running outside of the lab host Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"Use cases"},{"location":"lab-examples/lab-examples/","text":"About lab examples # containerlab aims to provide a simple, intuitive and yet customizable way to run container based labs. To help our users to have a running and functional lab as quickly as possible, we ship some essential lab topologies within the containerlab package. These lab examples are meant to be used as-is or as a base layer to a more customized or elaborated lab scenarios. Once containerlab is installed, you will find the lab examples directories by the /etc/containerlab/lab-examples path. Copy those directories over to your working directory to start using the provided labs. Container images versions Some lab examples may use the images without a tag, i.e. image: srlinux . This means that the image with a latest tag must exist. A user needs to tag the image themselves if the latest tag is missing. For example: docker tag srlinux:20.6.1-286 srlinux:latest The source code of the lab examples is contained within the containerlab repo unless mentioned otherwise; any questions, issues or contributions related to the provided examples can be addressed via Github issues . Each lab comes with a definitive description that can be found in this documentation section. How to deploy a lab from the lab catalog? # Running the labs from the catalog is easy. Copy lab catalog # First, you need to copy the lab catalog to your working directory, to ensure that the changes you might make to the lab files will not be overwritten once you upgrade containerlab. To copy the entire catalog into your working directory: # copy over the srl02 lab files cp -a /etc/containerlab/lab-examples/* . as a result of this command you will get several directories copied to the current working directory. Note Big labs or community provided labs are typically stored in a separate git repository. To fetch those labs you will need to clone the lab' repo instead of copying the directories from /etc/containerlab/lab-examples . Get the lab name # Every lab in the catalog has a unique short name. For example this lab states in the summary table its name is srl02 . You will find a folder matching this name in your working directory, change into it: cd srl02 Check images and licenses # Within the lab directory you will find the files that are used in the lab. Usually its only the topology definition files and, sometimes, config files. If you check the topology file you will see if any license files are required and what images are specified for each node/kind. Either change the topology file to point to the right image/license or change the image/license to match the topo definition file values. Deploy the lab # You are ready to deploy! containerlab deploy -t <lab_name>","title":"About"},{"location":"lab-examples/lab-examples/#about-lab-examples","text":"containerlab aims to provide a simple, intuitive and yet customizable way to run container based labs. To help our users to have a running and functional lab as quickly as possible, we ship some essential lab topologies within the containerlab package. These lab examples are meant to be used as-is or as a base layer to a more customized or elaborated lab scenarios. Once containerlab is installed, you will find the lab examples directories by the /etc/containerlab/lab-examples path. Copy those directories over to your working directory to start using the provided labs. Container images versions Some lab examples may use the images without a tag, i.e. image: srlinux . This means that the image with a latest tag must exist. A user needs to tag the image themselves if the latest tag is missing. For example: docker tag srlinux:20.6.1-286 srlinux:latest The source code of the lab examples is contained within the containerlab repo unless mentioned otherwise; any questions, issues or contributions related to the provided examples can be addressed via Github issues . Each lab comes with a definitive description that can be found in this documentation section.","title":"About lab examples"},{"location":"lab-examples/lab-examples/#how-to-deploy-a-lab-from-the-lab-catalog","text":"Running the labs from the catalog is easy.","title":"How to deploy a lab from the lab catalog?"},{"location":"lab-examples/lab-examples/#copy-lab-catalog","text":"First, you need to copy the lab catalog to your working directory, to ensure that the changes you might make to the lab files will not be overwritten once you upgrade containerlab. To copy the entire catalog into your working directory: # copy over the srl02 lab files cp -a /etc/containerlab/lab-examples/* . as a result of this command you will get several directories copied to the current working directory. Note Big labs or community provided labs are typically stored in a separate git repository. To fetch those labs you will need to clone the lab' repo instead of copying the directories from /etc/containerlab/lab-examples .","title":"Copy lab catalog"},{"location":"lab-examples/lab-examples/#get-the-lab-name","text":"Every lab in the catalog has a unique short name. For example this lab states in the summary table its name is srl02 . You will find a folder matching this name in your working directory, change into it: cd srl02","title":"Get the lab name"},{"location":"lab-examples/lab-examples/#check-images-and-licenses","text":"Within the lab directory you will find the files that are used in the lab. Usually its only the topology definition files and, sometimes, config files. If you check the topology file you will see if any license files are required and what images are specified for each node/kind. Either change the topology file to point to the right image/license or change the image/license to match the topo definition file values.","title":"Check images and licenses"},{"location":"lab-examples/lab-examples/#deploy-the-lab","text":"You are ready to deploy! containerlab deploy -t <lab_name>","title":"Deploy the lab"},{"location":"lab-examples/min-5clos/","text":"Description A 5-stage CLOS topology based on Nokia SR Linux Components Nokia SR Linux Resource requirements 1 4 8 GB Topology file clos02.yml Name clos02 Description # This labs provides a lightweight folded 5-stage CLOS fabric with Super Spine level bridging two PODs. The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation. Use cases # With this lightweight CLOS topology a user can exhibit the following scenarios: perform configuration tasks applied to the 5-stage CLOS fabric demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"5-stage CLOS"},{"location":"lab-examples/min-5clos/#description","text":"This labs provides a lightweight folded 5-stage CLOS fabric with Super Spine level bridging two PODs. The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation.","title":"Description"},{"location":"lab-examples/min-5clos/#use-cases","text":"With this lightweight CLOS topology a user can exhibit the following scenarios: perform configuration tasks applied to the 5-stage CLOS fabric demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"Use cases"},{"location":"lab-examples/min-clos/","text":"Description A minimal CLOS topology with two leafs and a spine Components Nokia SR Linux Resource requirements 1 1.5 3 GB Topology file clos01.yml Name clos01 Description # This labs provides a lightweight folded CLOS fabric topology using a minimal set of nodes: two leaves and a single spine. The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation. Use cases # With this lightweight CLOS topology a user can exhibit the following scenarios: perform configuration tasks applied to the 3-stage CLOS fabric demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"3-nodes CLOS"},{"location":"lab-examples/min-clos/#description","text":"This labs provides a lightweight folded CLOS fabric topology using a minimal set of nodes: two leaves and a single spine. The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation.","title":"Description"},{"location":"lab-examples/min-clos/#use-cases","text":"With this lightweight CLOS topology a user can exhibit the following scenarios: perform configuration tasks applied to the 3-stage CLOS fabric demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"Use cases"},{"location":"lab-examples/single-srl/","text":"Description a single Nokia SR Linux node Components Nokia SR Linux Resource requirements 1 0.5 1 GB Topology file srl01.yml Name srl01 Description # A lab consists of a single SR Linux container equipped with a single interface - its management interface. No other network/data interfaces are created. The SR Linux's mgmt interface is connected to the containerlab docker network that is created as part of the lab deployment process. The mgmt interface of SRL will get IPv4/6 address information via DHCP service provided by docker daemon. Use cases # This lightweight lab enables the users to perform the following exercises: get familiar with SR Linux architecture explore SR Linux extensible CLI navigate the SR Linux YANG tree play with gNMI 2 and JSON-RPC programmable interfaces write/debug/manage custom apps built for SR Linux NDK Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9 Check out gnmic gNMI client to interact with SR Linux gNMI server. \u21a9","title":"Single SR Linux node"},{"location":"lab-examples/single-srl/#description","text":"A lab consists of a single SR Linux container equipped with a single interface - its management interface. No other network/data interfaces are created. The SR Linux's mgmt interface is connected to the containerlab docker network that is created as part of the lab deployment process. The mgmt interface of SRL will get IPv4/6 address information via DHCP service provided by docker daemon.","title":"Description"},{"location":"lab-examples/single-srl/#use-cases","text":"This lightweight lab enables the users to perform the following exercises: get familiar with SR Linux architecture explore SR Linux extensible CLI navigate the SR Linux YANG tree play with gNMI 2 and JSON-RPC programmable interfaces write/debug/manage custom apps built for SR Linux NDK Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9 Check out gnmic gNMI client to interact with SR Linux gNMI server. \u21a9","title":"Use cases"},{"location":"lab-examples/srl-ceos/","text":"Description A Nokia SR Linux connected back-to-back with Arista cEOS Components Nokia SR Linux , Arista cEOS Resource requirements 1 1 2 GB Topology file srlceos01.yml Name srlceos01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , ceos:4.25.0F , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Arista cEOS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the containerlab docker network. Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Arista cEOS operating systems. BGP # This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and Arista cEOS. Both nodes exchange NLRI with their loopback prefix making it reachable. Configuration # Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlceos01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 # configure BGP set / network-instance default protocols bgp admin-state enable set / network-instance default protocols bgp router-id 10 .10.10.1 set / network-instance default protocols bgp autonomous-system 65001 set / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable set / network-instance default protocols bgp group ibgp export-policy export-lo set / network-instance default protocols bgp neighbor 192 .168.1.2 admin-state enable set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-group ibgp set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-as 65001 # create export policy set / routing-policy policy export-lo statement 10 match protocol local set / routing-policy policy export-lo statement 10 action accept # commit config commit now ceos Get into cEOS CLI with docker exec -it clab-srlceos01-ceos Cli and start configuration # enter configuration mode configure ip routing # configure loopback and data interfaces interface Ethernet1 no switchport ip address 192 .168.1.2/24 exit interface Loopback0 ip address 10 .10.10.2/32 exit # configure BGP router bgp 65001 router-id 10 .10.10.2 neighbor 192 .168.1.1 remote-as 65001 network 10 .10.10.2/32 exit Verification # Once BGP peering is established, the routes can be seen in GRT of both nodes: srl A:srl# show network-instance default route-table ipv4-unicast summary | grep bgp | 10 .10.10.2/32 | 0 | true | bgp | 0 | 170 | 192 .168.1.2 ( indirect ) | None | ceos ceos>show ip route VRF: default Codes: C - connected, S - static, K - kernel, O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1 , E2 - OSPF external type 2 , N1 - OSPF NSSA external type 1 , N2 - OSPF NSSA external type2, B - BGP, B I - iBGP, B E - eBGP, R - RIP, I L1 - IS-IS level 1 , I L2 - IS-IS level 2 , O3 - OSPFv3, A B - BGP Aggregate, A O - OSPF Summary, NG - Nexthop Group Static Route, V - VXLAN Control Service, DH - DHCP client installed default route, M - Martian, DP - Dynamic Policy Route, L - VRF Leaked, RC - Route Cache Route Gateway of last resort: K 0 .0.0.0/0 [ 40 /0 ] via 172 .20.20.1, Management0 B I 10 .10.10.1/32 [ 200 /0 ] via 192 .168.1.1, Ethernet1 C 10 .10.10.2/32 is directly connected, Loopback0 C 172 .20.20.0/24 is directly connected, Management0 C 192 .168.1.0/24 is directly connected, Ethernet1 Data plane confirms that routes have been programmed to FIB: A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=3.47 ms Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Arista cEOS"},{"location":"lab-examples/srl-ceos/#description","text":"A lab consists of an SR Linux node connected with Arista cEOS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the containerlab docker network.","title":"Description"},{"location":"lab-examples/srl-ceos/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Arista cEOS operating systems.","title":"Use cases"},{"location":"lab-examples/srl-ceos/#bgp","text":"This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and Arista cEOS. Both nodes exchange NLRI with their loopback prefix making it reachable.","title":"BGP"},{"location":"lab-examples/srl-ceos/#configuration","text":"Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlceos01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 # configure BGP set / network-instance default protocols bgp admin-state enable set / network-instance default protocols bgp router-id 10 .10.10.1 set / network-instance default protocols bgp autonomous-system 65001 set / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable set / network-instance default protocols bgp group ibgp export-policy export-lo set / network-instance default protocols bgp neighbor 192 .168.1.2 admin-state enable set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-group ibgp set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-as 65001 # create export policy set / routing-policy policy export-lo statement 10 match protocol local set / routing-policy policy export-lo statement 10 action accept # commit config commit now ceos Get into cEOS CLI with docker exec -it clab-srlceos01-ceos Cli and start configuration # enter configuration mode configure ip routing # configure loopback and data interfaces interface Ethernet1 no switchport ip address 192 .168.1.2/24 exit interface Loopback0 ip address 10 .10.10.2/32 exit # configure BGP router bgp 65001 router-id 10 .10.10.2 neighbor 192 .168.1.1 remote-as 65001 network 10 .10.10.2/32 exit","title":"Configuration"},{"location":"lab-examples/srl-ceos/#verification","text":"Once BGP peering is established, the routes can be seen in GRT of both nodes: srl A:srl# show network-instance default route-table ipv4-unicast summary | grep bgp | 10 .10.10.2/32 | 0 | true | bgp | 0 | 170 | 192 .168.1.2 ( indirect ) | None | ceos ceos>show ip route VRF: default Codes: C - connected, S - static, K - kernel, O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1 , E2 - OSPF external type 2 , N1 - OSPF NSSA external type 1 , N2 - OSPF NSSA external type2, B - BGP, B I - iBGP, B E - eBGP, R - RIP, I L1 - IS-IS level 1 , I L2 - IS-IS level 2 , O3 - OSPFv3, A B - BGP Aggregate, A O - OSPF Summary, NG - Nexthop Group Static Route, V - VXLAN Control Service, DH - DHCP client installed default route, M - Martian, DP - Dynamic Policy Route, L - VRF Leaked, RC - Route Cache Route Gateway of last resort: K 0 .0.0.0/0 [ 40 /0 ] via 172 .20.20.1, Management0 B I 10 .10.10.1/32 [ 200 /0 ] via 192 .168.1.1, Ethernet1 C 10 .10.10.2/32 is directly connected, Loopback0 C 172 .20.20.0/24 is directly connected, Management0 C 192 .168.1.0/24 is directly connected, Ethernet1 Data plane confirms that routes have been programmed to FIB: A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=3.47 ms Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Verification"},{"location":"lab-examples/srl-crpd/","text":"Description A Nokia SR Linux connected back-to-back with Juniper cRPD Components Nokia SR Linux , Juniper cRPD Resource requirements 1 1 2 GB Topology file srlcrpd01.yml Name srlcrpd01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , crpd:20.2R1.10 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Juniper cRPD via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper cRPD network operating systems. OSPF # Configuration # Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable OSPF on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlcrpd01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 # configure OSPF set / network-instance default router-id 10 .10.10.1 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 set / network-instance default protocols ospf instance main admin-state enable set / network-instance default protocols ospf instance main version ospf-v2 set / network-instance default protocols ospf instance main area 0 .0.0.0 interface ethernet-1/1.0 interface-type point-to-point set / network-instance default protocols ospf instance main area 0 .0.0.0 interface ethernet-1/1.0 # commit config commit now crpd cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process bash shell and configure interfaces: docker exec -it clab-srlcrpd01-crpd bash # configure linux interfaces ip addr add 192 .168.1.2/24 dev eth1 ip addr add 10 .10.10.2/32 dev lo Then launch the CLI and continue configuration docker exec -it clab-srlcrpd01-crpd cli : # enter configuration mode configure set routing-options router-id 10 .10.10.2 set protocols ospf area 0 .0.0.0 interface eth1 interface-type p2p set protocols ospf area 0 .0.0.0 interface lo.0 interface-type nbma # commit configuration commit Verificaton # After the configuration is done on both nodes, verify the control plane by checking the route tables on both ends and ensuring dataplane was programmed as well by pinging the remote loopback srl # control plane verification A:srl# / show network-instance default route-table ipv4-unicast summary | grep ospf | 10 .10.10.2/32 | 0 | true | ospfv2 | 1 | 10 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | # data plane verification A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms crpd # control plane verification root@crpd> show route | match OSPF 10 .10.10.1/32 * [ OSPF/10 ] 00 :01:24, metric 1 224 .0.0.5/32 * [ OSPF/10 ] 00 :05:49, metric 1 IS-IS # Configuration # Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable IS-IS on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlcrpd01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 # configure IS-IS set / network-instance default router-id 10 .10.10.1 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 set / network-instance default protocols isis instance main admin-state enable set / network-instance default protocols isis instance main net [ 49 .0001.0100.1001.0001.00 ] set / network-instance default protocols isis instance main interface ethernet-1/1.0 admin-state enable set / network-instance default protocols isis instance main interface ethernet-1/1.0 circuit-type point-to-point set / network-instance default protocols isis instance main interface lo0.0 # commit config commit now crpd cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process bash shell and configure interfaces: docker exec -it clab-srlcrpd01-crpd bash # configure linux interfaces ip addr add 192 .168.1.2/24 dev eth1 ip addr add 10 .10.10.2/32 dev lo Then launch the CLI and continue configuration docker exec -it clab-srlcrpd01-crpd cli : # enter configuration mode configure set interfaces lo0 unit 0 family iso address 49 .0001.0100.1001.0002.00 set routing-options router-id 10 .10.10.2 set protocols isis interface all point-to-point set protocols isis interface lo0.0 set protocols isis level 1 wide-metrics-only set protocols isis level 2 wide-metrics-only set protocols isis reference-bandwidth 100g # commit configuration commit srl # control plane verification A:srl# / show network-instance default route-table ipv4-unicast summary | grep isis | 10 .10.10.2/32 | 0 | true | isis | 10 | 18 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | | 172 .20.20.0/24 | 0 | true | isis | 110 | 18 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | # data plane verification A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms crpd # control plane verification root@crpd> show route table inet.0 | match IS-IS 10 .10.10.1/32 * [ IS-IS/18 ] 00 :00:13, metric 100 Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Juniper cRPD"},{"location":"lab-examples/srl-crpd/#description","text":"A lab consists of an SR Linux node connected with Juniper cRPD via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network.","title":"Description"},{"location":"lab-examples/srl-crpd/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper cRPD network operating systems.","title":"Use cases"},{"location":"lab-examples/srl-crpd/#ospf","text":"","title":"OSPF"},{"location":"lab-examples/srl-crpd/#configuration","text":"Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable OSPF on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlcrpd01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 # configure OSPF set / network-instance default router-id 10 .10.10.1 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 set / network-instance default protocols ospf instance main admin-state enable set / network-instance default protocols ospf instance main version ospf-v2 set / network-instance default protocols ospf instance main area 0 .0.0.0 interface ethernet-1/1.0 interface-type point-to-point set / network-instance default protocols ospf instance main area 0 .0.0.0 interface ethernet-1/1.0 # commit config commit now crpd cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process bash shell and configure interfaces: docker exec -it clab-srlcrpd01-crpd bash # configure linux interfaces ip addr add 192 .168.1.2/24 dev eth1 ip addr add 10 .10.10.2/32 dev lo Then launch the CLI and continue configuration docker exec -it clab-srlcrpd01-crpd cli : # enter configuration mode configure set routing-options router-id 10 .10.10.2 set protocols ospf area 0 .0.0.0 interface eth1 interface-type p2p set protocols ospf area 0 .0.0.0 interface lo.0 interface-type nbma # commit configuration commit","title":"Configuration"},{"location":"lab-examples/srl-crpd/#verificaton","text":"After the configuration is done on both nodes, verify the control plane by checking the route tables on both ends and ensuring dataplane was programmed as well by pinging the remote loopback srl # control plane verification A:srl# / show network-instance default route-table ipv4-unicast summary | grep ospf | 10 .10.10.2/32 | 0 | true | ospfv2 | 1 | 10 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | # data plane verification A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms crpd # control plane verification root@crpd> show route | match OSPF 10 .10.10.1/32 * [ OSPF/10 ] 00 :01:24, metric 1 224 .0.0.5/32 * [ OSPF/10 ] 00 :05:49, metric 1","title":"Verificaton"},{"location":"lab-examples/srl-crpd/#is-is","text":"","title":"IS-IS"},{"location":"lab-examples/srl-crpd/#configuration_1","text":"Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable IS-IS on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlcrpd01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 # configure IS-IS set / network-instance default router-id 10 .10.10.1 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 set / network-instance default protocols isis instance main admin-state enable set / network-instance default protocols isis instance main net [ 49 .0001.0100.1001.0001.00 ] set / network-instance default protocols isis instance main interface ethernet-1/1.0 admin-state enable set / network-instance default protocols isis instance main interface ethernet-1/1.0 circuit-type point-to-point set / network-instance default protocols isis instance main interface lo0.0 # commit config commit now crpd cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process bash shell and configure interfaces: docker exec -it clab-srlcrpd01-crpd bash # configure linux interfaces ip addr add 192 .168.1.2/24 dev eth1 ip addr add 10 .10.10.2/32 dev lo Then launch the CLI and continue configuration docker exec -it clab-srlcrpd01-crpd cli : # enter configuration mode configure set interfaces lo0 unit 0 family iso address 49 .0001.0100.1001.0002.00 set routing-options router-id 10 .10.10.2 set protocols isis interface all point-to-point set protocols isis interface lo0.0 set protocols isis level 1 wide-metrics-only set protocols isis level 2 wide-metrics-only set protocols isis reference-bandwidth 100g # commit configuration commit srl # control plane verification A:srl# / show network-instance default route-table ipv4-unicast summary | grep isis | 10 .10.10.2/32 | 0 | true | isis | 10 | 18 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | | 172 .20.20.0/24 | 0 | true | isis | 110 | 18 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | # data plane verification A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms crpd # control plane verification root@crpd> show route table inet.0 | match IS-IS 10 .10.10.1/32 * [ IS-IS/18 ] 00 :00:13, metric 100 Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Configuration"},{"location":"lab-examples/srl-frr/","text":"Description A Nokia SR Linux connected back-to-back FRR router Components Nokia SR Linux , FRR Resource requirements 1 1 2 GB Topology file srlfrr01.yml Name srlfrr01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , frrouting/frr:v7.5.0 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with FRR router via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Use cases # This lab allows users to launch basic control plane interoperability scenarios between Nokia SR Linux and FRR network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. There you will find the config files to demonstrate a classic iBGP peering use case: daemons : frr daemons config that is bind mounted to the frr container to trigger the start of the relevant FRR services frr.cfg : vtysh config lines to configure a basic iBGP peering srl.cfg : sr_cli config lines to configure a basic iBGP peering Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and FRR"},{"location":"lab-examples/srl-frr/#description","text":"A lab consists of an SR Linux node connected with FRR router via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network.","title":"Description"},{"location":"lab-examples/srl-frr/#use-cases","text":"This lab allows users to launch basic control plane interoperability scenarios between Nokia SR Linux and FRR network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. There you will find the config files to demonstrate a classic iBGP peering use case: daemons : frr daemons config that is bind mounted to the frr container to trigger the start of the relevant FRR services frr.cfg : vtysh config lines to configure a basic iBGP peering srl.cfg : sr_cli config lines to configure a basic iBGP peering Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/srl-sonic/","text":"Description A Nokia SR Linux connected back-to-back with SONiC-VS Components Nokia SR Linux , SONiC Resource requirements 1 1 2 GB Topology file sonic01.yml Name sonic01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , docker-sonic-vs:202012 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Azure SONiC via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the containerlab docker network. Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and SONiC operating systems. BGP # This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and SONiC. Both nodes exchange NLRI with their loopback prefix making it reachable. Configuration # Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlsonic01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 # configure BGP set / network-instance default protocols bgp admin-state enable set / network-instance default protocols bgp router-id 10 .10.10.1 set / network-instance default protocols bgp autonomous-system 65001 set / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable set / network-instance default protocols bgp group ibgp export-policy export-lo set / network-instance default protocols bgp neighbor 192 .168.1.2 admin-state enable set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-group ibgp set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-as 65001 # create export policy set / routing-policy policy export-lo statement 10 match protocol local set / routing-policy policy export-lo statement 10 action accept # commit config commit now sonic Get into sonic container shell with docker exec -it clab-srlsonic01-sonic bash and configure the so-called front-panel ports. Since we defined only one data interface for our sonic/srl nodes, we need to confgure a single port: config interface ip add Ethernet0 192 .168.1.2/24 config interface startup Ethernet0 Now when data interface has been configured, enter in the FRR shell to configure BGP by typing vtysh command inside the sonic container. # enter configuration mode configure # configure BGP router bgp 65001 bgp router-id 10 .10.10.2 neighbor 192 .168.1.1 remote-as 65001 address-family ipv4 unicast network 10 .10.10.2/32 exit-address-family exit access-list all seq 5 permit any Verification # Once BGP peering is established, the routes can be seen in GRT of both nodes: srl A:srl# / show network-instance default route-table ipv4-unicast summary | grep bgp | 10 .10.10.2/32 | 0 | true | bgp | 0 | 170 | 192 .168.1.2 ( indirect ) | None | sonic sonic# sh ip route Codes: K - kernel route, C - connected, S - static, R - RIP, O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP, T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP, F - PBR, f - OpenFabric, > - selected route, * - FIB route, q - queued route, r - rejected route K>* 0 .0.0.0/0 [ 0 /0 ] via 172 .20.20.1, eth0, 00 :20:55 B>* 10 .10.10.1/32 [ 200 /0 ] via 192 .168.1.1, Ethernet0, 00 :01:51 C>* 172 .20.20.0/24 is directly connected, eth0, 00 :20:55 B 192 .168.1.0/24 [ 200 /0 ] via 192 .168.1.0 inactive, 00 :01:51 C>* 192 .168.1.0/24 is directly connected, Ethernet0, 00 :03:50 Data plane confirms that routes have been programmed to FIB: sonic# ping 10.10.10.1 PING 10.10.10.1 (10.10.10.1) 56(84) bytes of data. 64 bytes from 10.10.10.1: icmp_seq=1 ttl=64 time=2.28 ms 64 bytes from 10.10.10.1: icmp_seq=2 ttl=64 time=2.84 ms Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and SONiC"},{"location":"lab-examples/srl-sonic/#description","text":"A lab consists of an SR Linux node connected with Azure SONiC via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the containerlab docker network.","title":"Description"},{"location":"lab-examples/srl-sonic/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and SONiC operating systems.","title":"Use cases"},{"location":"lab-examples/srl-sonic/#bgp","text":"This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and SONiC. Both nodes exchange NLRI with their loopback prefix making it reachable.","title":"BGP"},{"location":"lab-examples/srl-sonic/#configuration","text":"Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlsonic01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 # configure BGP set / network-instance default protocols bgp admin-state enable set / network-instance default protocols bgp router-id 10 .10.10.1 set / network-instance default protocols bgp autonomous-system 65001 set / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable set / network-instance default protocols bgp group ibgp export-policy export-lo set / network-instance default protocols bgp neighbor 192 .168.1.2 admin-state enable set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-group ibgp set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-as 65001 # create export policy set / routing-policy policy export-lo statement 10 match protocol local set / routing-policy policy export-lo statement 10 action accept # commit config commit now sonic Get into sonic container shell with docker exec -it clab-srlsonic01-sonic bash and configure the so-called front-panel ports. Since we defined only one data interface for our sonic/srl nodes, we need to confgure a single port: config interface ip add Ethernet0 192 .168.1.2/24 config interface startup Ethernet0 Now when data interface has been configured, enter in the FRR shell to configure BGP by typing vtysh command inside the sonic container. # enter configuration mode configure # configure BGP router bgp 65001 bgp router-id 10 .10.10.2 neighbor 192 .168.1.1 remote-as 65001 address-family ipv4 unicast network 10 .10.10.2/32 exit-address-family exit access-list all seq 5 permit any","title":"Configuration"},{"location":"lab-examples/srl-sonic/#verification","text":"Once BGP peering is established, the routes can be seen in GRT of both nodes: srl A:srl# / show network-instance default route-table ipv4-unicast summary | grep bgp | 10 .10.10.2/32 | 0 | true | bgp | 0 | 170 | 192 .168.1.2 ( indirect ) | None | sonic sonic# sh ip route Codes: K - kernel route, C - connected, S - static, R - RIP, O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP, T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP, F - PBR, f - OpenFabric, > - selected route, * - FIB route, q - queued route, r - rejected route K>* 0 .0.0.0/0 [ 0 /0 ] via 172 .20.20.1, eth0, 00 :20:55 B>* 10 .10.10.1/32 [ 200 /0 ] via 192 .168.1.1, Ethernet0, 00 :01:51 C>* 172 .20.20.0/24 is directly connected, eth0, 00 :20:55 B 192 .168.1.0/24 [ 200 /0 ] via 192 .168.1.0 inactive, 00 :01:51 C>* 192 .168.1.0/24 is directly connected, Ethernet0, 00 :03:50 Data plane confirms that routes have been programmed to FIB: sonic# ping 10.10.10.1 PING 10.10.10.1 (10.10.10.1) 56(84) bytes of data. 64 bytes from 10.10.10.1: icmp_seq=1 ttl=64 time=2.28 ms 64 bytes from 10.10.10.1: icmp_seq=2 ttl=64 time=2.84 ms Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Verification"},{"location":"lab-examples/two-srls/","text":"Description Two Nokia SR Linux nodes Components Nokia SR Linux Resource requirements 1 1 2 GB Topology file srl02.yml Name srl02 Validated versions 2 containerlab v0.8.2 , srlinux:20.6.2-332 Description # A lab consists of two SR Linux nodes connected with each other via a point-to-point link over e1-1 interfaces. Both nodes are also connected with their management interfaces to the clab docker network. Configuration # The nodes of this lab have been provided with a startup configuration by means of config directive in the topo definition file. The startup configuration adds loopback and interfaces addressing as per the diagram above. Once the lab is started, the nodes will be able to ping each other via configured interfaces: A:srl1# ping 192.168.0.1 network-instance default Using network instance default PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data. 64 bytes from 192.168.0.1: icmp_seq=1 ttl=64 time=5.17 ms Use cases # This lab, besides having the same objectives as srl01 lab, also enables the following scenarios: get to know protocols and services configuration verify basic control plane and data plane operations explore SR Linux state datastore for the paths which reflect control plane operation metrics or dataplane counters Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9 versions of respective container images or software that was used to create the lab. \u21a9","title":"Two SR Linux nodes"},{"location":"lab-examples/two-srls/#description","text":"A lab consists of two SR Linux nodes connected with each other via a point-to-point link over e1-1 interfaces. Both nodes are also connected with their management interfaces to the clab docker network.","title":"Description"},{"location":"lab-examples/two-srls/#configuration","text":"The nodes of this lab have been provided with a startup configuration by means of config directive in the topo definition file. The startup configuration adds loopback and interfaces addressing as per the diagram above. Once the lab is started, the nodes will be able to ping each other via configured interfaces: A:srl1# ping 192.168.0.1 network-instance default Using network instance default PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data. 64 bytes from 192.168.0.1: icmp_seq=1 ttl=64 time=5.17 ms","title":"Configuration"},{"location":"lab-examples/two-srls/#use-cases","text":"This lab, besides having the same objectives as srl01 lab, also enables the following scenarios: get to know protocols and services configuration verify basic control plane and data plane operations explore SR Linux state datastore for the paths which reflect control plane operation metrics or dataplane counters Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9 versions of respective container images or software that was used to create the lab. \u21a9","title":"Use cases"},{"location":"lab-examples/vr-sros/","text":"Description A Nokia SR Linux connected back-to-back with Nokia SR OS Components Nokia SR Linux , Nokia SR OS Resource requirements 1 2 5 GB Topology file vr01.yml Name vr01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , vr-sros:20.10.R1 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Nokia SR OS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Nokia SR OS VM is launched as a container, using vrnetlab integration . Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Nokia SR OS network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Nokia SR OS"},{"location":"lab-examples/vr-sros/#description","text":"A lab consists of an SR Linux node connected with Nokia SR OS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Nokia SR OS VM is launched as a container, using vrnetlab integration .","title":"Description"},{"location":"lab-examples/vr-sros/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Nokia SR OS network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/vr-vmx/","text":"Description A Nokia SR Linux connected back-to-back with Juniper vMX Components Nokia SR Linux , Juniper vMX Resource requirements 1 2 8 GB Topology file vr02.yml Name vr02 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , vr-vmx:20.2R1.10 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Juniper vMX via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Juniper vMX VM is launched as a container, using vrnetlab integration . Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper vMX network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Juniper vMX"},{"location":"lab-examples/vr-vmx/#description","text":"A lab consists of an SR Linux node connected with Juniper vMX via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Juniper vMX VM is launched as a container, using vrnetlab integration .","title":"Description"},{"location":"lab-examples/vr-vmx/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper vMX network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/vr-xrv/","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRv Components Nokia SR Linux , Cisco XRv Resource requirements 1 1 3 GB Topology file vr03.yml Name vr03 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , vr-xrv:6.1.2 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Cisco XRv via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Cisco XRv VM is launched as a container, using vrnetlab integration . Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Cisco XRv"},{"location":"lab-examples/vr-xrv/#description","text":"A lab consists of an SR Linux node connected with Cisco XRv via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Cisco XRv VM is launched as a container, using vrnetlab integration .","title":"Description"},{"location":"lab-examples/vr-xrv/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/vr-xrv9k/","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRv9k Components Nokia SR Linux , Cisco XRv9k Resource requirements 1 2 12 GB Topology file vr04.yml Name vr04 Version information 2 containerlab:0.9.5 , srlinux:20.6.3-145 , vr-xrv9k:7.2.1 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Cisco XRv9k via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Cisco XRv9k VM is launched as a container, using vrnetlab integration . Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv9k network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Cisco XRv9k"},{"location":"lab-examples/vr-xrv9k/#description","text":"A lab consists of an SR Linux node connected with Cisco XRv9k via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Cisco XRv9k VM is launched as a container, using vrnetlab integration .","title":"Description"},{"location":"lab-examples/vr-xrv9k/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv9k network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/wan/","text":"Description WAN emulating topology Components Nokia SR Linux Resource requirements 1 1 3 GB Topology file srl03.yml Name srl03 Description # Nokia SR Linux while focusing on the data center deployments in the first releases, will also be suitable for WAN deployments. In this lab users presented with a small WAN topology of four interconnected SR Linux nodes with multiple p2p interfaces between them. Use cases # The WAN-centric scenarios can be tested with this lab: Link aggregation WAN protocols and features Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"WAN topology"},{"location":"lab-examples/wan/#description","text":"Nokia SR Linux while focusing on the data center deployments in the first releases, will also be suitable for WAN deployments. In this lab users presented with a small WAN topology of four interconnected SR Linux nodes with multiple p2p interfaces between them.","title":"Description"},{"location":"lab-examples/wan/#use-cases","text":"The WAN-centric scenarios can be tested with this lab: Link aggregation WAN protocols and features Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"Use cases"},{"location":"manual/conf-artifacts/","text":"When containerlab deploys a lab it creates a Lab Directory in the current working directory . This directory is used to keep all the necessary files that are needed to run/configure the nodes. We call these files configuration artifacts . Things like: Root CA certificate and per-node TLS certificate and private keys per-node config file node-specific files and directories that are required to launch the container license files all these artifacts will be generated under a Lab Directory. Identifying a lab directory # The lab directory name will be constructed by the following template clab-<lab_name> . Thus if the name of your lab is srl02 you will find the clab-srl02 directory created after the lab deployment process is finished. \u276f ls -lah clab-srl02 total 4.0K drwxr-xr-x 5 root root 40 Dec 1 22:11 . drwxr-xr-x 23 root root 4.0K Dec 1 22:11 .. drwxr-xr-x 5 root root 42 Dec 1 22:11 ca drwxr-xr-x 3 root root 79 Dec 1 22:11 srl1 drwxr-xr-x 3 root root 79 Dec 1 22:11 srl2 The contents of this directory will contain kind-specific files and directories. Containerlab will name directories after the node names and will only created those if they are needed. For instance, by default any node of kind linux will not have it's own directory. Persistance of a lab directory # When a user first deploy a lab, the Lab Directory gets created. Depending on node kind, this directory might act as a persistent storage area for a node. A common case is having the configuration file saved when the changes are made to the node via management interfaces. Below is an example of the srl1 node directory contents. It keeps a directory that is mounted to containers configuration path, as well as stores additional files needed to launch and configure the node. ~/clab/clab-srl02 \u276f ls -lah srl1 drwxrwxrwx+ 6 1002 1002 87 Dec 1 22:11 config -rw-r--r-- 1 root root 2.8K Dec 1 22:11 license.key -rw-r--r-- 1 root root 4.4K Dec 1 22:11 srlinux.conf -rw-r--r-- 1 root root 233 Dec 1 22:11 topology.yml When a user destroys a lab without providing --cleanup flag to the destroy command, the Lab Directory does not get deleted. This means that every configuration artefact will be kept on disk. Moreover, when the user will deploy the same lab, containerlab will reuse the configuration artifacts if possible, which will, for example, start the nodes with the config files saved from the previous lab run. To be able to deploy a lab without reusing existing configuration artefact use the --reconfigure flag with deploy command. With that setting, containerlab will first delete the Lab Directory and then will start the deployment process.","title":"Configuration artifacts"},{"location":"manual/conf-artifacts/#identifying-a-lab-directory","text":"The lab directory name will be constructed by the following template clab-<lab_name> . Thus if the name of your lab is srl02 you will find the clab-srl02 directory created after the lab deployment process is finished. \u276f ls -lah clab-srl02 total 4.0K drwxr-xr-x 5 root root 40 Dec 1 22:11 . drwxr-xr-x 23 root root 4.0K Dec 1 22:11 .. drwxr-xr-x 5 root root 42 Dec 1 22:11 ca drwxr-xr-x 3 root root 79 Dec 1 22:11 srl1 drwxr-xr-x 3 root root 79 Dec 1 22:11 srl2 The contents of this directory will contain kind-specific files and directories. Containerlab will name directories after the node names and will only created those if they are needed. For instance, by default any node of kind linux will not have it's own directory.","title":"Identifying a lab directory"},{"location":"manual/conf-artifacts/#persistance-of-a-lab-directory","text":"When a user first deploy a lab, the Lab Directory gets created. Depending on node kind, this directory might act as a persistent storage area for a node. A common case is having the configuration file saved when the changes are made to the node via management interfaces. Below is an example of the srl1 node directory contents. It keeps a directory that is mounted to containers configuration path, as well as stores additional files needed to launch and configure the node. ~/clab/clab-srl02 \u276f ls -lah srl1 drwxrwxrwx+ 6 1002 1002 87 Dec 1 22:11 config -rw-r--r-- 1 root root 2.8K Dec 1 22:11 license.key -rw-r--r-- 1 root root 4.4K Dec 1 22:11 srlinux.conf -rw-r--r-- 1 root root 233 Dec 1 22:11 topology.yml When a user destroys a lab without providing --cleanup flag to the destroy command, the Lab Directory does not get deleted. This means that every configuration artefact will be kept on disk. Moreover, when the user will deploy the same lab, containerlab will reuse the configuration artifacts if possible, which will, for example, start the nodes with the config files saved from the previous lab run. To be able to deploy a lab without reusing existing configuration artefact use the --reconfigure flag with deploy command. With that setting, containerlab will first delete the Lab Directory and then will start the deployment process.","title":"Persistance of a lab directory"},{"location":"manual/network/","text":"One of the most important tasks in the process of building container based labs is to create a virtual wiring between the containers and the host. That is one of the problems that containerlab was designed to solve. In this document we will discuss the networking concepts that containerlab employs to provide the following connectivity scenarios: Make containers available from the lab host Interconnect containers to create network topologies of users choice Management network # As governed by the well-established container networking principles containers are able to get network connectivity using various drivers/methods. The most common networking driver that is enabled by default for docker-managed containers is the bridge driver . The bridge driver connects containers to a linux bridge interface named docker0 on most linux operating systems. The containers are then able to communicate with each other and the host via this virtual switch (bridge interface). In containerlab we follow a similar approach: containers launched by containerlab will be attached with their interface to a containerlab-managed docker network. It's best to be explained by an example which we will base on a two nodes lab from our catalog: name : srl02 topology : kinds : srl : type : ixr6 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] As seen from the topology definition file, the lab consists of the two SR Linux nodes which are interconnected via a single point-to-point link. The diagram above shows that these two nodes are not only interconnected between themselves, but also connected to a bridge interface on the lab host. This is driven by the containerlab default management network settings. default settings # When no information about the management network is provided within the topo definition file, containerlab will do the following create, if not already created, a docker network named clab configure the IPv4/6 addressing pertaining to this docker network Info We often refer to clab docker network simply as management network since its the network to which management interfaces of the containerized NOS'es are connected. The addressing information that containerlab will use on this network: IPv4: subnet 172.20.20.0/24, gateway 172.20.20.1 IPv6: subnet 2001:172:20:20::/80, gateway 2001:172:20:20::1 This management network will be configured with 1450 MTU. This option is configurable. With these defaults in place, the two containers from this lab will get connected to that management network and will be able to communicate using the IP addresses allocated by docker daemon. The addresses that docker carves out for each container are presented to a user once the lab deployment finishes or can be queried any time after: # addressing information is available once the lab deployment completes \u276f containerlab deploy -t srl02.yml # deployment log omitted for brevity +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | | 2 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ # addresses can also be fetched afterwards with `inspect` command \u276f containerlab inspect -a +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Lab Name | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | srl02 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | | 2 | srl02 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ The output above shows that srl1 container has been assigned 172.20.20.3/24 / 2001:172:20:20::3/80 IPv4/6 address. We can ensure this by querying the srl1 management interfaces address info: \u276f docker exec clab-srl02-srl1 ip address show dummy-mgmt0 6 : dummy-mgmt0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 2a:66:2b:09:2e:4d brd ff:ff:ff:ff:ff:ff inet 172 .20.20.3/24 brd 172 .20.20.255 scope global dummy-mgmt0 valid_lft forever preferred_lft forever inet6 2001 :172:20:20::3/80 scope global valid_lft forever preferred_lft forever Now it's possible to reach the assigned IP address from the lab host as well as from other containers connected to this management network. # ping srl1 management interface from srl2 \u276f docker exec -it clab-srl02-srl2 sr_cli \"ping 172.20.20.3 network-instance mgmt\" x -> $176 x48 Using network instance mgmt PING 172 .20.20.3 ( 172 .20.20.3 ) 56 ( 84 ) bytes of data. 64 bytes from 172 .20.20.3: icmp_seq = 1 ttl = 64 time = 2 .43 ms Note If you run multiple labs without changing the default management settings, the containers of those labs will end up connecting to the same management network with their management interface. configuring management network # Most of the time there is no need to change the defaults for management network configuration, but sometimes it is needed. For example, it might be that the default network ranges are overlapping with existing addressing scheme on the lab host or it might be desirable to have predefined management IP addresses. For such cases the users need to add the mgmt container at the top level of their topology definition file: name : srl02 mgmt : network : custom_mgmt # management network name ipv4_subnet : 172.100.100.0/24 # ipv4 range ipv6_subnet : 2001:172:100:100::/80 # ipv6 range (optional) topology : # the rest of the file is omitted for brevity With this settings in place container will get their IP addresses from the specified ranges accordingly. user-defined addresses # By default container runtime will assign the management IP addresses for the containers. But sometimes it's useful to have a user-defined addressing in the management network. For such cases users can define the desired IPv4/6 addresses on a per-node basis: mgmt : network : fixedips ipv4_subnet : 172.100.100.0/24 ipv6_subnet : 2001:172:100:100::/80 topology : nodes : n1 : kind : srl mgmt_ipv4 : 172.100.100.11 # set ipv4 address on management network mgmt_ipv6 : 2001:172:100:100::11 # set ipv6 address on management network Users can specify either IPv4 or IPv6 or both addresses, if one of the addresses is omitted, it will be assigned by container runtime in an arbitrary fashion. Note If user-defined IP addresses are needed, they must be provided for all containers attached to a given network to avoid address collision. IPv4/6 addresses set on a node level must be from the management network range. MTU # The MTU of the management network can be set to user defined value: mgmt : network : clab_mgmt mtu : 2100 # set mtu of the management network to 2100 This will result in every interface connected to that network to inherit this MTU value. connection details # When containerlab needs to create the management network it asks the docker daemon to do this. Docker will fullfil the request and will create a network with the underlying linux bridge interface backing it. The bridge interface name is generated by the docker daemon, but it is easy to find it: # list existing docker networks # notice the presence of the `clab` network with a `bridge` driver \u276f docker network ls NETWORK ID NAME DRIVER SCOPE 5d60b6ec8420 bridge bridge local d2169a14e334 clab bridge local 58ec5037122a host host local 4c1491a09a1a none null local # the underlying linux bridge interface name follows the `br-<first_12_chars_of_docker_network_id> pattern # to find the network ID use: \u276f docker network inspect clab -f {{ .ID }} | head -c 12 d2169a14e334 # now the name is known and its easy to show bridge state \u276f brctl show br-d2169a14e334 bridge name bridge id STP enabled interfaces br-d2169a14e334 8000 .0242fe382b74 no vetha57b950 vethe9da10a As explained in the beginning of this article, containers will connect to this docker network. This connection is carried out by the veth devices created and attached with one end to bridge interface in the lab host and the other end in the container namespace. This is illustrated by the bridge output above and the diagram at the beginning the of the article. Point-to-point links # Management network is used to provide management access to the NOS containers, it does not carry control or dataplane traffic. In containerlab we create additional point-to-point links between the containers to provide the datapath between the lab nodes. The above diagram shows how links are created in the topology definition file. In this example, the datapath consists of the two virtual point-to-point wires between SR Linux and cEOS containers. These links are created on-demand by containerlab itself. The p2p links are provided by the veth device pairs where each end of the veth pair is attached to a respective container.","title":"Network wiring concepts"},{"location":"manual/network/#management-network","text":"As governed by the well-established container networking principles containers are able to get network connectivity using various drivers/methods. The most common networking driver that is enabled by default for docker-managed containers is the bridge driver . The bridge driver connects containers to a linux bridge interface named docker0 on most linux operating systems. The containers are then able to communicate with each other and the host via this virtual switch (bridge interface). In containerlab we follow a similar approach: containers launched by containerlab will be attached with their interface to a containerlab-managed docker network. It's best to be explained by an example which we will base on a two nodes lab from our catalog: name : srl02 topology : kinds : srl : type : ixr6 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] As seen from the topology definition file, the lab consists of the two SR Linux nodes which are interconnected via a single point-to-point link. The diagram above shows that these two nodes are not only interconnected between themselves, but also connected to a bridge interface on the lab host. This is driven by the containerlab default management network settings.","title":"Management network"},{"location":"manual/network/#default-settings","text":"When no information about the management network is provided within the topo definition file, containerlab will do the following create, if not already created, a docker network named clab configure the IPv4/6 addressing pertaining to this docker network Info We often refer to clab docker network simply as management network since its the network to which management interfaces of the containerized NOS'es are connected. The addressing information that containerlab will use on this network: IPv4: subnet 172.20.20.0/24, gateway 172.20.20.1 IPv6: subnet 2001:172:20:20::/80, gateway 2001:172:20:20::1 This management network will be configured with 1450 MTU. This option is configurable. With these defaults in place, the two containers from this lab will get connected to that management network and will be able to communicate using the IP addresses allocated by docker daemon. The addresses that docker carves out for each container are presented to a user once the lab deployment finishes or can be queried any time after: # addressing information is available once the lab deployment completes \u276f containerlab deploy -t srl02.yml # deployment log omitted for brevity +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | | 2 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ # addresses can also be fetched afterwards with `inspect` command \u276f containerlab inspect -a +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Lab Name | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | srl02 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | | 2 | srl02 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ The output above shows that srl1 container has been assigned 172.20.20.3/24 / 2001:172:20:20::3/80 IPv4/6 address. We can ensure this by querying the srl1 management interfaces address info: \u276f docker exec clab-srl02-srl1 ip address show dummy-mgmt0 6 : dummy-mgmt0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 2a:66:2b:09:2e:4d brd ff:ff:ff:ff:ff:ff inet 172 .20.20.3/24 brd 172 .20.20.255 scope global dummy-mgmt0 valid_lft forever preferred_lft forever inet6 2001 :172:20:20::3/80 scope global valid_lft forever preferred_lft forever Now it's possible to reach the assigned IP address from the lab host as well as from other containers connected to this management network. # ping srl1 management interface from srl2 \u276f docker exec -it clab-srl02-srl2 sr_cli \"ping 172.20.20.3 network-instance mgmt\" x -> $176 x48 Using network instance mgmt PING 172 .20.20.3 ( 172 .20.20.3 ) 56 ( 84 ) bytes of data. 64 bytes from 172 .20.20.3: icmp_seq = 1 ttl = 64 time = 2 .43 ms Note If you run multiple labs without changing the default management settings, the containers of those labs will end up connecting to the same management network with their management interface.","title":"default settings"},{"location":"manual/network/#configuring-management-network","text":"Most of the time there is no need to change the defaults for management network configuration, but sometimes it is needed. For example, it might be that the default network ranges are overlapping with existing addressing scheme on the lab host or it might be desirable to have predefined management IP addresses. For such cases the users need to add the mgmt container at the top level of their topology definition file: name : srl02 mgmt : network : custom_mgmt # management network name ipv4_subnet : 172.100.100.0/24 # ipv4 range ipv6_subnet : 2001:172:100:100::/80 # ipv6 range (optional) topology : # the rest of the file is omitted for brevity With this settings in place container will get their IP addresses from the specified ranges accordingly.","title":"configuring management network"},{"location":"manual/network/#user-defined-addresses","text":"By default container runtime will assign the management IP addresses for the containers. But sometimes it's useful to have a user-defined addressing in the management network. For such cases users can define the desired IPv4/6 addresses on a per-node basis: mgmt : network : fixedips ipv4_subnet : 172.100.100.0/24 ipv6_subnet : 2001:172:100:100::/80 topology : nodes : n1 : kind : srl mgmt_ipv4 : 172.100.100.11 # set ipv4 address on management network mgmt_ipv6 : 2001:172:100:100::11 # set ipv6 address on management network Users can specify either IPv4 or IPv6 or both addresses, if one of the addresses is omitted, it will be assigned by container runtime in an arbitrary fashion. Note If user-defined IP addresses are needed, they must be provided for all containers attached to a given network to avoid address collision. IPv4/6 addresses set on a node level must be from the management network range.","title":"user-defined addresses"},{"location":"manual/network/#mtu","text":"The MTU of the management network can be set to user defined value: mgmt : network : clab_mgmt mtu : 2100 # set mtu of the management network to 2100 This will result in every interface connected to that network to inherit this MTU value.","title":"MTU"},{"location":"manual/network/#connection-details","text":"When containerlab needs to create the management network it asks the docker daemon to do this. Docker will fullfil the request and will create a network with the underlying linux bridge interface backing it. The bridge interface name is generated by the docker daemon, but it is easy to find it: # list existing docker networks # notice the presence of the `clab` network with a `bridge` driver \u276f docker network ls NETWORK ID NAME DRIVER SCOPE 5d60b6ec8420 bridge bridge local d2169a14e334 clab bridge local 58ec5037122a host host local 4c1491a09a1a none null local # the underlying linux bridge interface name follows the `br-<first_12_chars_of_docker_network_id> pattern # to find the network ID use: \u276f docker network inspect clab -f {{ .ID }} | head -c 12 d2169a14e334 # now the name is known and its easy to show bridge state \u276f brctl show br-d2169a14e334 bridge name bridge id STP enabled interfaces br-d2169a14e334 8000 .0242fe382b74 no vetha57b950 vethe9da10a As explained in the beginning of this article, containers will connect to this docker network. This connection is carried out by the veth devices created and attached with one end to bridge interface in the lab host and the other end in the container namespace. This is illustrated by the bridge output above and the diagram at the beginning the of the article.","title":"connection details"},{"location":"manual/network/#point-to-point-links","text":"Management network is used to provide management access to the NOS containers, it does not carry control or dataplane traffic. In containerlab we create additional point-to-point links between the containers to provide the datapath between the lab nodes. The above diagram shows how links are created in the topology definition file. In this example, the datapath consists of the two virtual point-to-point wires between SR Linux and cEOS containers. These links are created on-demand by containerlab itself. The p2p links are provided by the veth device pairs where each end of the veth pair is attached to a respective container.","title":"Point-to-point links"},{"location":"manual/nodes/","text":"Node object is one of the pillars of containerlab. Essentially, it is nodes and links what constitute the lab topology. To let users build flexible and customizable labs the nodes are meant to be configurable. The node configuration is part of the topology definition file and may consist of the following fields that we explain in details below. # part of topology definition file topology : nodes : node1 : # node name kind : srl type : ixrd2 image : srlinux license : license.key config : /root/mylab/node1.cfg binds : - /usr/local/bin/gobgp:/root/gobgp - /root/files:/root/files:ro ports : - 80:8080 - 55555:43555/udp - 55554:43554/tcp user : test env : ENV1 : VAL1 cmd : /bin/bash script.sh kind # The kind property selects which kind this node is of. Kinds are essentially a way of telling containerlab how to treat the nodes properties considering the specific flavor of the node. We dedicated a separate section to discuss kinds in details. Note Kind must be defined either by setting the kind for a node specifically (as in the example above), or by setting the default kind: topology : defaults : kind : srl nodes : node1 : # kind value of `srl` is inherited from defaults section type # With type the user sets a type of the node. Types work in combination with the kinds, such as the type value of ixrd2 sets the chassis type for SR Linux node, thus this value only makes sense to nodes of kind srl . Other nodes might treat type field differently, that will depend on the kind of the node. The type values and effects defined in the documentation for a specific kind. image # The common image attribute sets the container image name that will be used to start the node. The image name should be provided in a well-known format of repository(:tag) . We use <repository> image name throughout the docs articles. This means that the image with <repository>:latest name will be looked up. A user will need to add the latest tag if they want to use the same loose-tag naming: # tagging srlinux:20.6.1-286 as srlinux:latest # after this change its possible to use `srlinux:latest` or `srlinux` image name docker tag srlinux:20.6.1-286 srlinux:latest license # Some containerized NOSes require a license to operate. With license property a user sets a path to a license file that a node will use. The license file will then be mounted to the container by the path that is defined by the kind/type of the node. config # For the specific kinds its possible to pass a path to a config template file that a node will use. The template engine is Go template . The srlconfig.tpl template is used by default for Nokia SR Linux nodes and can be used to create configuration templates for SR Linux nodes. Supported for: Nokia SR Linux. binds # In order to expose host files to the containerized nodes a user can leverage the bind mount capability. Provide a list of binds instructions under the binds container of the node configuration. The string format of those binding instructions follow the same rules as the --volume parameter of the docker/podman CLI. binds : # mount a file from a host to a container (implicit RW mode) - /usr/local/bin/gobgp:/root/gobgp # mount a directory from a host to a container in RO mode - /root/files:/root/files:ro ports # To bind the ports between the lab host and the containers the users can populate the ports object inside the node: ports : - 80:8080 # tcp port 80 of the host is mapped to port 8080 of the container - 55555:43555/udp - 55554:43554/tcp The list of port bindings consists of strings in the same format that is acceptable by docker run command's -p/--export flag . This option is only configurable under the node level. env # To add environment variables to a node use the env container that can be added at defaults , kind and node levels. The variables values are merged when the same vars are defined on multiple levels with nodes level being the most specific. topology : defaults : env : ENV1 : 3 # ENV1=3 will be set if its not set on kind or node level ENV2 : glob # ENV2=glob will be set for all nodes kinds : srl : env : ENV1 : 2 # ENV1=2 will be set to if its not set on node level ENV3 : kind # ENV3=kind will be set for all nodes of srl kind nodes : node1 : env : ENV1 : 1 # ENV1=1 will be set for node1 user # To set a user which will be used to run a containerized process use the user configuration option. Can be defined at node , kind and global levels. topology : defaults : user : alice # alice user will be used for all nodes unless set on kind or node levels kinds : srl : user : bob # bob user will be used for nodes of kind srl unless it is set on node level nodes : node1 : user : clab # clab user will be used for node1 cmd # It is possible to set/override the command of the container image with cmd configuration option. It accepts the \"shell\" form and can be set on all levels. topology : defaults : cmd : bash cmd.sh kinds : srl : cmd : bash cmd2.sh nodes : node1 : cmd : bash cmd3.sh share # Container lab integrates with mysocket.io service to allow for private, Internet-reachable tunnels created for sockets of containerlab nodes. This enables effortless access sharing with cusomters/partners/colleagues. This integration is extensively described on Share lab access page. name : demo topology : nodes : r1 : kind : srl share : - tcp/22 # tcp port 22 will be exposed - tcp/57400 # tcp port 57400 will be exposed","title":"Nodes"},{"location":"manual/nodes/#kind","text":"The kind property selects which kind this node is of. Kinds are essentially a way of telling containerlab how to treat the nodes properties considering the specific flavor of the node. We dedicated a separate section to discuss kinds in details. Note Kind must be defined either by setting the kind for a node specifically (as in the example above), or by setting the default kind: topology : defaults : kind : srl nodes : node1 : # kind value of `srl` is inherited from defaults section","title":"kind"},{"location":"manual/nodes/#type","text":"With type the user sets a type of the node. Types work in combination with the kinds, such as the type value of ixrd2 sets the chassis type for SR Linux node, thus this value only makes sense to nodes of kind srl . Other nodes might treat type field differently, that will depend on the kind of the node. The type values and effects defined in the documentation for a specific kind.","title":"type"},{"location":"manual/nodes/#image","text":"The common image attribute sets the container image name that will be used to start the node. The image name should be provided in a well-known format of repository(:tag) . We use <repository> image name throughout the docs articles. This means that the image with <repository>:latest name will be looked up. A user will need to add the latest tag if they want to use the same loose-tag naming: # tagging srlinux:20.6.1-286 as srlinux:latest # after this change its possible to use `srlinux:latest` or `srlinux` image name docker tag srlinux:20.6.1-286 srlinux:latest","title":"image"},{"location":"manual/nodes/#license","text":"Some containerized NOSes require a license to operate. With license property a user sets a path to a license file that a node will use. The license file will then be mounted to the container by the path that is defined by the kind/type of the node.","title":"license"},{"location":"manual/nodes/#config","text":"For the specific kinds its possible to pass a path to a config template file that a node will use. The template engine is Go template . The srlconfig.tpl template is used by default for Nokia SR Linux nodes and can be used to create configuration templates for SR Linux nodes. Supported for: Nokia SR Linux.","title":"config"},{"location":"manual/nodes/#binds","text":"In order to expose host files to the containerized nodes a user can leverage the bind mount capability. Provide a list of binds instructions under the binds container of the node configuration. The string format of those binding instructions follow the same rules as the --volume parameter of the docker/podman CLI. binds : # mount a file from a host to a container (implicit RW mode) - /usr/local/bin/gobgp:/root/gobgp # mount a directory from a host to a container in RO mode - /root/files:/root/files:ro","title":"binds"},{"location":"manual/nodes/#ports","text":"To bind the ports between the lab host and the containers the users can populate the ports object inside the node: ports : - 80:8080 # tcp port 80 of the host is mapped to port 8080 of the container - 55555:43555/udp - 55554:43554/tcp The list of port bindings consists of strings in the same format that is acceptable by docker run command's -p/--export flag . This option is only configurable under the node level.","title":"ports"},{"location":"manual/nodes/#env","text":"To add environment variables to a node use the env container that can be added at defaults , kind and node levels. The variables values are merged when the same vars are defined on multiple levels with nodes level being the most specific. topology : defaults : env : ENV1 : 3 # ENV1=3 will be set if its not set on kind or node level ENV2 : glob # ENV2=glob will be set for all nodes kinds : srl : env : ENV1 : 2 # ENV1=2 will be set to if its not set on node level ENV3 : kind # ENV3=kind will be set for all nodes of srl kind nodes : node1 : env : ENV1 : 1 # ENV1=1 will be set for node1","title":"env"},{"location":"manual/nodes/#user","text":"To set a user which will be used to run a containerized process use the user configuration option. Can be defined at node , kind and global levels. topology : defaults : user : alice # alice user will be used for all nodes unless set on kind or node levels kinds : srl : user : bob # bob user will be used for nodes of kind srl unless it is set on node level nodes : node1 : user : clab # clab user will be used for node1","title":"user"},{"location":"manual/nodes/#cmd","text":"It is possible to set/override the command of the container image with cmd configuration option. It accepts the \"shell\" form and can be set on all levels. topology : defaults : cmd : bash cmd.sh kinds : srl : cmd : bash cmd2.sh nodes : node1 : cmd : bash cmd3.sh","title":"cmd"},{"location":"manual/nodes/#share","text":"Container lab integrates with mysocket.io service to allow for private, Internet-reachable tunnels created for sockets of containerlab nodes. This enables effortless access sharing with cusomters/partners/colleagues. This integration is extensively described on Share lab access page. name : demo topology : nodes : r1 : kind : srl share : - tcp/22 # tcp port 22 will be exposed - tcp/57400 # tcp port 57400 will be exposed","title":"share"},{"location":"manual/published-ports/","text":"Containerlab labs are typically deployed in the isolated environments, such as company's internal network, cloud instance or even a laptop. The nodes deployed in a lab can happily talk to each other and, if needed, can reach Internet in the outbound direction. But sometimes it is really needed to let your lab nodes be reachable over Internet securely and privately in the incoming direction. There are many use cases that warrant such publishing , some of the most notable are: create a lab in your environment and share it with a customer/colleague on-demand in no time make an interactive demo/training where certain nodes' are shared with an audience for hand-on experience share a private lab with someone to collaborate expose management interfaces (gNMI, NETCONF, SNMP) to test integration with collectors deployed outside of your lab environment Containerlab made all of these use cases possible by integrating with mysocket.io service. Mysocket.io provides personal tunnels for https/https/tls/tcp ports over global anycast 1 network spanning US, Europe and Asia. To make a certain port of a certain node available via mysocket.io tunnel a single line in the topology definition file is all that's needed: name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be published - tcp/57400 # tcp port 57400 will be published - http/10200 # http service running over 10200 will be published Registration # Tunnels set up by mysocket.io are associated with a user who set them, thus users are required to register within the service. Luckily, the registration is trivial, all you need to provide is an email and a public SSH key that will be used to set up tunnels. For convenience, containerlab comes with a script to create mysocket.io account in one go: # create mysocket.io account # docker needs to be installed # usage: mysocket-user.sh <email> <password> <abs path to public ssh key> /etc/containerlab/tools/mysocket-user myemail@gmail.com mypassword /root/.ssh/mykey.pub A confirmation email will arrive shortly to finish account setup procedure. Acquiring a token # To authenticate with mysocket.io service a user needs to acquire/refresh the token. Containerlab users can leverage another convenience script that eases this step: # get/refresh mysocketio token # the script will save the token under the $(pwd)/mysocket_token file # usage: mysocket-token.sh <email> <password> /etc/containerlab/tools/mysocket-token.sh myemail@gmail.com mypassword The script will get the token and save it in the current directory under mysocket_token name. Info The token is valid for 5 hours, once the token expires, the already established tunnels will continue to work, but to establish new tunnels a new token must be provided. Specify what to share # To indicate which ports to publish the users need to add publish section under node/kind or default level of the topology definition file . In the example below, we decide to publish SSH and gNMI services of r1 node: name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be exposed - tcp/57400 # tcp port 57400 will be exposed The publish section holds a list of <type>/<port-number> strings, where type must be one of the supported mysocket.io socket type 2 - http/https/tls/tcp. Every type/port combination will be exposed via its own private tunnel. Note For a single account the following maximum number of tunnels is set: * tcp based tunnels - 5 * http based tunnels - 10 If >5 tcp tunnels is required users should launch a VM in a lab, expose it's SSH service and use this VM as a jumpbox for other TCP services. Add mysocketio node # Containerlab integrates with mysocket.io service by leveraging it's client application packaged in a container format. In order for the sockets indicated in the publish block to be exposed, a user needs to add a node of mysocketio kind to the topology. Augmenting the topology we used above, the full topology file will look like: name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be exposed - tcp/57400 # tcp port 57400 will be exposed # adding mysocketio linux container mysocketio : kind : mysocketio image : ghcr.io/hellt/mysocketctl:0.1.0 binds : - ~/.ssh/privkey:/root/.ssh/id_rsa # bind mount your private key - mysocketio_token:/root/.mysocketio_token # bind mount API token The mysocketio node is a simple linux container with mysocketctl client installed. Containerlab uses this node to create the sockets and start tunnels as defined in the publish block. Pay specific attention to binds defined for mysocketio node. With this section we provide the two crucial artifacts: * path to the private key, that matches the public key used during the registration * path to the API token that we acquired before launching the lab And that is all that is needed to expose the sockets in an automated way. Explore published ports # When a user launches a lab with published ports it will be presented with a summary table after the lab deployment process finishes: +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ | 1 | clab-sock-r1 | 9cefd6cdb239 | srlinux:20.6.3-145 | srl | | running | 172.20.20.2/24 | 2001:172:20:20::2/80 | | 2 | clab-sock-mysocketctl | 8f5385beb97e | ghcr.io/hellt/mysocketctl:0.1.0 | mysocketio | | running | 172.20.20.3/24 | 2001:172:20:20::3/80 | +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ Published ports: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 SOCKET ID \u2502 DNS NAME \u2502 PORT(S) \u2502 TYPE \u2502 CLOUD AUTH \u2502 NAME \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 2b2808ae-b05d-4103-9b95-ab473a82658c \u2502 nameless-bird-8969.edge.mysocket.io \u2502 16086 \u2502 tcp \u2502 false \u2502 clab-tcp-22-r1 \u2502 \u2502 8455571c-deea-4b09-bc1d-7a56f41e8c52 \u2502 restless-night-8051.edge.mysocket.io \u2502 11107 \u2502 tcp \u2502 false \u2502 clab-tcp-57400-r1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The Published ports table lists the ports and their corresponding DNS names. Looking at the NAME column users can quickly discover which tunnel corresponds to which node and its published port. The socket name follows the clab-<type>-<port>-<node-name> pattern. To access the published port, users need to combine the DNS name and the Port to derive the full address. For the exposed SSH port, for example, the ssh client can use the following command to access remote SSH service: ssh user@nameless-bird-8969.edge.mysocket.io -p 16086 Troubleshooting # To check the health status of the established tunnels execute the following command to check the logs created on mysocketio container: docker exec -it <mysocketio-node-name> /bin/sh -c \"cat socket*\" This command will display all the logs for the published ports. If something is not right, you will see the erros in the log. https://mysocket.readthedocs.io/en/latest/about/about.html#build-on-a-global-anycast-network \u21a9 https://mysocket.readthedocs.io/en/latest/about/about.html#features \u21a9","title":"Publish ports"},{"location":"manual/published-ports/#registration","text":"Tunnels set up by mysocket.io are associated with a user who set them, thus users are required to register within the service. Luckily, the registration is trivial, all you need to provide is an email and a public SSH key that will be used to set up tunnels. For convenience, containerlab comes with a script to create mysocket.io account in one go: # create mysocket.io account # docker needs to be installed # usage: mysocket-user.sh <email> <password> <abs path to public ssh key> /etc/containerlab/tools/mysocket-user myemail@gmail.com mypassword /root/.ssh/mykey.pub A confirmation email will arrive shortly to finish account setup procedure.","title":"Registration"},{"location":"manual/published-ports/#acquiring-a-token","text":"To authenticate with mysocket.io service a user needs to acquire/refresh the token. Containerlab users can leverage another convenience script that eases this step: # get/refresh mysocketio token # the script will save the token under the $(pwd)/mysocket_token file # usage: mysocket-token.sh <email> <password> /etc/containerlab/tools/mysocket-token.sh myemail@gmail.com mypassword The script will get the token and save it in the current directory under mysocket_token name. Info The token is valid for 5 hours, once the token expires, the already established tunnels will continue to work, but to establish new tunnels a new token must be provided.","title":"Acquiring a token"},{"location":"manual/published-ports/#specify-what-to-share","text":"To indicate which ports to publish the users need to add publish section under node/kind or default level of the topology definition file . In the example below, we decide to publish SSH and gNMI services of r1 node: name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be exposed - tcp/57400 # tcp port 57400 will be exposed The publish section holds a list of <type>/<port-number> strings, where type must be one of the supported mysocket.io socket type 2 - http/https/tls/tcp. Every type/port combination will be exposed via its own private tunnel. Note For a single account the following maximum number of tunnels is set: * tcp based tunnels - 5 * http based tunnels - 10 If >5 tcp tunnels is required users should launch a VM in a lab, expose it's SSH service and use this VM as a jumpbox for other TCP services.","title":"Specify what to share"},{"location":"manual/published-ports/#add-mysocketio-node","text":"Containerlab integrates with mysocket.io service by leveraging it's client application packaged in a container format. In order for the sockets indicated in the publish block to be exposed, a user needs to add a node of mysocketio kind to the topology. Augmenting the topology we used above, the full topology file will look like: name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be exposed - tcp/57400 # tcp port 57400 will be exposed # adding mysocketio linux container mysocketio : kind : mysocketio image : ghcr.io/hellt/mysocketctl:0.1.0 binds : - ~/.ssh/privkey:/root/.ssh/id_rsa # bind mount your private key - mysocketio_token:/root/.mysocketio_token # bind mount API token The mysocketio node is a simple linux container with mysocketctl client installed. Containerlab uses this node to create the sockets and start tunnels as defined in the publish block. Pay specific attention to binds defined for mysocketio node. With this section we provide the two crucial artifacts: * path to the private key, that matches the public key used during the registration * path to the API token that we acquired before launching the lab And that is all that is needed to expose the sockets in an automated way.","title":"Add mysocketio node"},{"location":"manual/published-ports/#explore-published-ports","text":"When a user launches a lab with published ports it will be presented with a summary table after the lab deployment process finishes: +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ | 1 | clab-sock-r1 | 9cefd6cdb239 | srlinux:20.6.3-145 | srl | | running | 172.20.20.2/24 | 2001:172:20:20::2/80 | | 2 | clab-sock-mysocketctl | 8f5385beb97e | ghcr.io/hellt/mysocketctl:0.1.0 | mysocketio | | running | 172.20.20.3/24 | 2001:172:20:20::3/80 | +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ Published ports: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 SOCKET ID \u2502 DNS NAME \u2502 PORT(S) \u2502 TYPE \u2502 CLOUD AUTH \u2502 NAME \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 2b2808ae-b05d-4103-9b95-ab473a82658c \u2502 nameless-bird-8969.edge.mysocket.io \u2502 16086 \u2502 tcp \u2502 false \u2502 clab-tcp-22-r1 \u2502 \u2502 8455571c-deea-4b09-bc1d-7a56f41e8c52 \u2502 restless-night-8051.edge.mysocket.io \u2502 11107 \u2502 tcp \u2502 false \u2502 clab-tcp-57400-r1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The Published ports table lists the ports and their corresponding DNS names. Looking at the NAME column users can quickly discover which tunnel corresponds to which node and its published port. The socket name follows the clab-<type>-<port>-<node-name> pattern. To access the published port, users need to combine the DNS name and the Port to derive the full address. For the exposed SSH port, for example, the ssh client can use the following command to access remote SSH service: ssh user@nameless-bird-8969.edge.mysocket.io -p 16086","title":"Explore published ports"},{"location":"manual/published-ports/#troubleshooting","text":"To check the health status of the established tunnels execute the following command to check the logs created on mysocketio container: docker exec -it <mysocketio-node-name> /bin/sh -c \"cat socket*\" This command will display all the logs for the published ports. If something is not right, you will see the erros in the log. https://mysocket.readthedocs.io/en/latest/about/about.html#build-on-a-global-anycast-network \u21a9 https://mysocket.readthedocs.io/en/latest/about/about.html#features \u21a9","title":"Troubleshooting"},{"location":"manual/topo-def-file/","text":"Containerlab builds labs based on the topology information that users pass to it. This topology information is expressed as a code contained in the topology definition file which structure is the prime focus of this document. Topology definition components # The topology definition file is a configuration file expressed in YAML. In this document we take a pre-packaged Nokia SR Linux and Arista cEOS lab and explain the topology definition structure using its definition file srlceos01.yml which is pasted below: name : srlceos01 topology : nodes : srl : kind : srl type : ixrd2 image : srlinux license : license.key ceos : kind : ceos image : ceos links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] This topology results in the two nodes being started up and interconnected with each other using a single point-po-point interface: Let's touch on the key components of the topology definition file used in this example. Name # The topology must have a name associated with it. The name is used to distinct one topology from another, to allow multiple topologies to be deployed on the host without their names clashed. name : srlceos01 Its user's responsibility to give labs unique names if they plan to run multiple labs. The name is a free-formed string, though its recommended not to use dashes ( - ) as they are used to separate lab names from node names. When containerlab starts the containers, their names will be generated using the following pattern: clab-{{lab_name}}-{{node_name}} . The lab name here is used to make the container's names unique between two different labs even if the nodes are named the same. Topology # The topology object inside the topology definition is the core element of the file. Under the topology element you will find all the core objects such as nodes , links , kinds and defaults . Nodes # As with every other topology the nodes are in the center of things. With nodes we tell which lab elements we want to run, in what configuration and flavor. Let's zoom into the two nodes we have defined in our topology: topology : nodes : srl : # this is a name of the 1st node kind : srl type : ixrd2 image : srlinux license : license.key ceos : # this is a name of the 2nd node kind : ceos image : ceos We defined individual nodes under the topology.nodes container. The name of the node is the key under which it is defined. Following the example, our two nodes will be named srl and ceos respectively. Each node can be defined with a set of properties. Such as the srl node is defined with the following node-specific properties: srl : kind : srl type : ixrd2 image : srlinux license : license.key Refer to the node configuration document to understand which options are available for nodes and what is their meaning. Links # Although its totally fine to define the node without any links (like in this lab ) most of the time we interconnect the nodes with links. One of containerlab purposes is to make the interconnection of nodes simple. We define the links under the topology.links container in the following manner: # nodes configuration omitted for clarity topology : nodes : srl : ceos : links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] - endpoints : [ \"srl:e1-2\" , \"ceos:eth2\" ] As you see, the topology.links container consists of links. The link itself is expressed as list of two endpoints . This might sound complicated, lets use a graphical explanation: As demonstrated on a diagram above, the links between the containers are the point-to-point links which are defined by a pair of interfaces. The link defined as: endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] translates to an intent of creation a p2p link between the node named srl and its e1-1 interface and the node named ceos and its eth1 interface. The p2p link is realized with a veth pair. Kinds # Kinds define the flavor of the node, it says if the node is a specific containerized Network OS or something else. We go into details of kinds in its own document section , but for the sake of the topology container, we must discuss what happens when kinds section appears in the topology definition: topology : kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl srl3 : kind : srl In the example above the topology.kinds container has the srl kind referenced. With this, we set some values for the properties of the srl kind. With a configuration like that, we say that nodes that have srl kind associated will also inherit its properties (type, image, license). Essentially, what kinds section allows us to do is to shorten the lab definition in cases when we have a number of nodes of a same kind. All the nodes ( srl1 , srl2 , srl3 ) will have the same values for type , image and license . Consider how the topology would have looked like without setting the kinds object: topology : nodes : srl1 : kind : srl type : ixrd2 image : srlinux license : license.key srl2 : kind : srl type : ixrd2 image : srlinux license : license.key srl3 : kind : srl type : ixrd2 image : srlinux license : license.key A lot of unnecessary repetition which is eliminated when setting kinds . Defaults # Since kinds set the values for the properties of a specific kind, we also introduced the defaults container, that can set values globally. With defaults you can, for example, set the default kind for all the nodes like that: topology : defaults : kind : srl kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : srl2 : srl3 : Now every node without a kind specified under it, will inherit the global default value of srl .","title":"Topology definition"},{"location":"manual/topo-def-file/#topology-definition-components","text":"The topology definition file is a configuration file expressed in YAML. In this document we take a pre-packaged Nokia SR Linux and Arista cEOS lab and explain the topology definition structure using its definition file srlceos01.yml which is pasted below: name : srlceos01 topology : nodes : srl : kind : srl type : ixrd2 image : srlinux license : license.key ceos : kind : ceos image : ceos links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] This topology results in the two nodes being started up and interconnected with each other using a single point-po-point interface: Let's touch on the key components of the topology definition file used in this example.","title":"Topology definition components"},{"location":"manual/topo-def-file/#name","text":"The topology must have a name associated with it. The name is used to distinct one topology from another, to allow multiple topologies to be deployed on the host without their names clashed. name : srlceos01 Its user's responsibility to give labs unique names if they plan to run multiple labs. The name is a free-formed string, though its recommended not to use dashes ( - ) as they are used to separate lab names from node names. When containerlab starts the containers, their names will be generated using the following pattern: clab-{{lab_name}}-{{node_name}} . The lab name here is used to make the container's names unique between two different labs even if the nodes are named the same.","title":"Name"},{"location":"manual/topo-def-file/#topology","text":"The topology object inside the topology definition is the core element of the file. Under the topology element you will find all the core objects such as nodes , links , kinds and defaults .","title":"Topology"},{"location":"manual/topo-def-file/#nodes","text":"As with every other topology the nodes are in the center of things. With nodes we tell which lab elements we want to run, in what configuration and flavor. Let's zoom into the two nodes we have defined in our topology: topology : nodes : srl : # this is a name of the 1st node kind : srl type : ixrd2 image : srlinux license : license.key ceos : # this is a name of the 2nd node kind : ceos image : ceos We defined individual nodes under the topology.nodes container. The name of the node is the key under which it is defined. Following the example, our two nodes will be named srl and ceos respectively. Each node can be defined with a set of properties. Such as the srl node is defined with the following node-specific properties: srl : kind : srl type : ixrd2 image : srlinux license : license.key Refer to the node configuration document to understand which options are available for nodes and what is their meaning.","title":"Nodes"},{"location":"manual/topo-def-file/#links","text":"Although its totally fine to define the node without any links (like in this lab ) most of the time we interconnect the nodes with links. One of containerlab purposes is to make the interconnection of nodes simple. We define the links under the topology.links container in the following manner: # nodes configuration omitted for clarity topology : nodes : srl : ceos : links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] - endpoints : [ \"srl:e1-2\" , \"ceos:eth2\" ] As you see, the topology.links container consists of links. The link itself is expressed as list of two endpoints . This might sound complicated, lets use a graphical explanation: As demonstrated on a diagram above, the links between the containers are the point-to-point links which are defined by a pair of interfaces. The link defined as: endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] translates to an intent of creation a p2p link between the node named srl and its e1-1 interface and the node named ceos and its eth1 interface. The p2p link is realized with a veth pair.","title":"Links"},{"location":"manual/topo-def-file/#kinds","text":"Kinds define the flavor of the node, it says if the node is a specific containerized Network OS or something else. We go into details of kinds in its own document section , but for the sake of the topology container, we must discuss what happens when kinds section appears in the topology definition: topology : kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl srl3 : kind : srl In the example above the topology.kinds container has the srl kind referenced. With this, we set some values for the properties of the srl kind. With a configuration like that, we say that nodes that have srl kind associated will also inherit its properties (type, image, license). Essentially, what kinds section allows us to do is to shorten the lab definition in cases when we have a number of nodes of a same kind. All the nodes ( srl1 , srl2 , srl3 ) will have the same values for type , image and license . Consider how the topology would have looked like without setting the kinds object: topology : nodes : srl1 : kind : srl type : ixrd2 image : srlinux license : license.key srl2 : kind : srl type : ixrd2 image : srlinux license : license.key srl3 : kind : srl type : ixrd2 image : srlinux license : license.key A lot of unnecessary repetition which is eliminated when setting kinds .","title":"Kinds"},{"location":"manual/topo-def-file/#defaults","text":"Since kinds set the values for the properties of a specific kind, we also introduced the defaults container, that can set values globally. With defaults you can, for example, set the default kind for all the nodes like that: topology : defaults : kind : srl kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : srl2 : srl3 : Now every node without a kind specified under it, will inherit the global default value of srl .","title":"Defaults"},{"location":"manual/vrnetlab/","text":"Containerlab focuses on containers, but there are way more routing products which are only shipped in a virtual machine packaging. Leaving containerlab users without ability to create topologies with both containerized and VM-based routing systems would have been a shame. Keeping this requirement in mind from the very beginning, we added a kind bridge , that allows to, ehm, bridge your containerized topology with other resources available via a bridged network. For example a VM based router. Although this approach has many pros, it doesn't allow users to define the VM based nodes in the same topology file. But not anymore, with vrnetlab integration containerlab became capable of launching topologies with VM-based routers. Vrnetlab # Vrnetlab essentially allows to package a regular VM inside a container and makes it runnable and accessible as if it was a container image. To make this work, vrnetlab provides a set of scripts that will build the container image taking a user provided qcow file as an input. This enables containerlab to build topologies which consist both of native containerized NOSes and the VMs: Info Although multiple vendors are supported in vrnetlab, to make these images work with container-based networking, we needed to fork the project and provide the necessary improvements. Thus, the VM based products will appear in the supported list gradually. Make sure, that the VM that containerlab runs on have Nested virtualization enabled to support vrnetlab based containers. Supported VM products # Nokia SR OS # Nokia's virtualized SR OS, aka VSR/VSim has been added to containerlab supported kinds under the vr-sros kind. A demo lab explains the way this kind can be used. To build a container image with SR OS inside users should follow the provided build instructions and using the code of the forked version of a vrnetlab project. Warning When building SR OS vrnetlab image for use with containerlab, do not provide the license during the image build process. The license shall be provided in the containerlab topology definition file 1 . Juniper vMX # Juniper's virtualized MX router - vMX - has been added to containerlab supported kinds under the vr-vmx kind. A demo lab explains the way this kind can be used. To build a container image with vMX inside users should follow the instructions provided and using the code of the forked version of a vrnetlab project. Cisco XRv # Cisco's virtualized XR router (demo) - XRv - has been added to containerlab supported kinds under the vr-xrv9k and vr-xrv kinds. The xr-xrv kind is added for XRv images which are supreceded by XRv9k images. The reason we keep vr-xrv is that it is much more lightweight and can be used for basic control plane interops on a resource constrained hosts. The demo lab for xrv9k and demo lab for xrv explain the way this kinds can be used. To build a container image with XRv9k/XRv inside users should follow the instructions provided in the relevant folders and using the code of the forked version of a vrnetlab project. Connection modes # Containerlab offers several ways VM based routers can be connected with the rest of the docker workloads. By default, vrnetlab integrated routers will use tc backend 2 which doesn't require any additional packages to be installed on the containerhost and supoprts transparent passage of LACP frames. Any other datapaths? Althout tc based datapath should cover all the needed connectivity requirements, if other, bridge-like, datapaths are needed, Containerlab offers OpenvSwitch and Linux bridge modes. Users can plug in those datapaths by specifying CONNECTION_MODE env variable: # the env variable can also be set in the defaults section name : myTopo topology : nodes : sr1 : kind : vr-sros image : vrnetlab/vr-sros:20.10.R1 env : CONNECTION_MODE : bridge # use `ovs` for openvswitch datapath see this example lab with a license path provided in the topology definition file \u21a9 pros and cons of different datapaths were examined here \u21a9","title":"VM based routers integration"},{"location":"manual/vrnetlab/#vrnetlab","text":"Vrnetlab essentially allows to package a regular VM inside a container and makes it runnable and accessible as if it was a container image. To make this work, vrnetlab provides a set of scripts that will build the container image taking a user provided qcow file as an input. This enables containerlab to build topologies which consist both of native containerized NOSes and the VMs: Info Although multiple vendors are supported in vrnetlab, to make these images work with container-based networking, we needed to fork the project and provide the necessary improvements. Thus, the VM based products will appear in the supported list gradually. Make sure, that the VM that containerlab runs on have Nested virtualization enabled to support vrnetlab based containers.","title":"Vrnetlab"},{"location":"manual/vrnetlab/#supported-vm-products","text":"","title":"Supported VM products"},{"location":"manual/vrnetlab/#nokia-sr-os","text":"Nokia's virtualized SR OS, aka VSR/VSim has been added to containerlab supported kinds under the vr-sros kind. A demo lab explains the way this kind can be used. To build a container image with SR OS inside users should follow the provided build instructions and using the code of the forked version of a vrnetlab project. Warning When building SR OS vrnetlab image for use with containerlab, do not provide the license during the image build process. The license shall be provided in the containerlab topology definition file 1 .","title":"Nokia SR OS"},{"location":"manual/vrnetlab/#juniper-vmx","text":"Juniper's virtualized MX router - vMX - has been added to containerlab supported kinds under the vr-vmx kind. A demo lab explains the way this kind can be used. To build a container image with vMX inside users should follow the instructions provided and using the code of the forked version of a vrnetlab project.","title":"Juniper vMX"},{"location":"manual/vrnetlab/#cisco-xrv","text":"Cisco's virtualized XR router (demo) - XRv - has been added to containerlab supported kinds under the vr-xrv9k and vr-xrv kinds. The xr-xrv kind is added for XRv images which are supreceded by XRv9k images. The reason we keep vr-xrv is that it is much more lightweight and can be used for basic control plane interops on a resource constrained hosts. The demo lab for xrv9k and demo lab for xrv explain the way this kinds can be used. To build a container image with XRv9k/XRv inside users should follow the instructions provided in the relevant folders and using the code of the forked version of a vrnetlab project.","title":"Cisco XRv"},{"location":"manual/vrnetlab/#connection-modes","text":"Containerlab offers several ways VM based routers can be connected with the rest of the docker workloads. By default, vrnetlab integrated routers will use tc backend 2 which doesn't require any additional packages to be installed on the containerhost and supoprts transparent passage of LACP frames. Any other datapaths? Althout tc based datapath should cover all the needed connectivity requirements, if other, bridge-like, datapaths are needed, Containerlab offers OpenvSwitch and Linux bridge modes. Users can plug in those datapaths by specifying CONNECTION_MODE env variable: # the env variable can also be set in the defaults section name : myTopo topology : nodes : sr1 : kind : vr-sros image : vrnetlab/vr-sros:20.10.R1 env : CONNECTION_MODE : bridge # use `ovs` for openvswitch datapath see this example lab with a license path provided in the topology definition file \u21a9 pros and cons of different datapaths were examined here \u21a9","title":"Connection modes"},{"location":"manual/wireshark/","text":"Every lab must have a packet capturing abilities, without it data plane verification becomes unnecessary complicated. Warning Pcap or it didn't happen Containerlab is no exception and capturing packets is something you can and should do with the labs launched by containerlab. Consider the following lab topology which highlights the typical points of packet capture. Since containerlab leverages linux network devices, users are free to use whatever tool of choice to sniff from any of them. This article will provide examples for tcpdump and wireshark tools. Packet capture, namespaces and interfaces # Capturing the packets from an interface requires having that interface name and it's network namespace (netns). And that's it. Keep in mind, that containers employ network isolation by the means of network namespaces. As depicted above, each container has its own network namespace which is named exactly the same. This makes it trivial to pinpoint which namespace to use. If containerlab at the end of a lab deploy reports that it created the containers with the names clab-lab1-srl clab-lab1-ceos clab-lab1-linux then the namespaces for each of those containers will be named the same (clab-lab1-srl, etc). To list the interfaces (links) of a given container leverage the ip utility: # where $netns_name is the container name of a node ip netns exec $netns_name ip link Capturing with tcpdump/wireshark # Now when it is clear which netns names corresponds to which container and which interfaces are available inside the given lab node, its extremely easy to start capturing traffic. local capture # From the containerlab host to capture from any interface inside a container simply use: # where $lab_node_name is the name of the container, which is also the name of the network namespace # and $if_name is the interface name inside the container netns ip netns exec $lab_node_name tcpdump -nni $if_name remote capture # If you want to start capture from a remote machine, then add ssh command to the mix: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -nni $if_name \" Capturing remotely with tcpdump makes little sense, but it makes all the difference when wireshark is concerned. Wireshark normally is not installed on the containerlab host, but it more often than not installed on the users machine/laptop. Thus it is possible to use remote capture capability to let wireshark receive the traffic from the remote containerlab node: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | wireshark -k -i - This will start the capture from a given interface and redirect the received flow to the wireshark input. Note Windows users should use WSL and invoke the command similar to the following: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | /mnt/c/Program \\ Files/Wireshark/wireshark.exe -k -i - Examples # Lets take the first diagram of this article and see which commands are used to sniff from the highlighted interfaces. In the examples below the wireshark will be used as a sniffing tool and the following naming simplifications and conventions used: $clab_host - address of the containerlab host clab-pcap-srl , clab-pcap-ceos , clab-pcap-linux - container names of the SRL, cEOS and Linux nodes accordingly. SR Linux [1], [4] SR Linux linecard interfaces are named as e<linecard_num>-<port_num> which translates to ethernet-<linecard_num>/<port_num> name inside the NOS itself. So to capture from ethernet-1/1 interface the following command should be used: ssh $clab_host \"ip netns exec $clab -pcap-srl tcpdump -U -nni e1-1 -w -\" | wireshark -k -i - The management interface on the SR Linux container is named mgmt0 , so the relevant command will look like: ssh $clab_host \"ip netns exec $clab -pcap-srl tcpdump -U -nni mgmt0 -w -\" | wireshark -k -i - cEOS [2] Similarly to SR Linux example, to capture the data interface of cEOS is no different. Just pick the right interface: ssh $clab_host \"ip netns exec $clab -pcap-ceos tcpdump -U -nni eth1 -w -\" | wireshark -k -i - Linux container [3] A bare linux container is no different, its interfaces are named ethX where eth0 is the interface connected to the containerlab management network. So to capture from the first data link we will use eth1 interface: ssh $clab_host \"ip netns exec $clab -pcap-linux tcpdump -U -nni eth1 -w -\" | wireshark -k -i - management bridge [5] It is also possible to listen for all management traffic that traverses the containerlab's management network. To do that you firstly need to find out the name of the linux bridge and then capture from it: ssh $clab_host \"tcpdump -U -nni brXXXXXX -w -\" | wireshark -k -i - Note that in this case you do not need to drill into the network namespace, since management bridge is in the default netns. To simplify wireshark remote capturing process users can create a tiny bash script that will save some typing: #!/bin/sh # call this script as `bash script_name.sh <container-name> <interface-name>` ssh <containerlab_address> \"ip netns exec $1 tcpdump -U -nni $2 -w -\" | wireshark -k -i -","title":"Packet capture & Wireshark"},{"location":"manual/wireshark/#packet-capture-namespaces-and-interfaces","text":"Capturing the packets from an interface requires having that interface name and it's network namespace (netns). And that's it. Keep in mind, that containers employ network isolation by the means of network namespaces. As depicted above, each container has its own network namespace which is named exactly the same. This makes it trivial to pinpoint which namespace to use. If containerlab at the end of a lab deploy reports that it created the containers with the names clab-lab1-srl clab-lab1-ceos clab-lab1-linux then the namespaces for each of those containers will be named the same (clab-lab1-srl, etc). To list the interfaces (links) of a given container leverage the ip utility: # where $netns_name is the container name of a node ip netns exec $netns_name ip link","title":"Packet capture, namespaces and interfaces"},{"location":"manual/wireshark/#capturing-with-tcpdumpwireshark","text":"Now when it is clear which netns names corresponds to which container and which interfaces are available inside the given lab node, its extremely easy to start capturing traffic.","title":"Capturing with tcpdump/wireshark"},{"location":"manual/wireshark/#local-capture","text":"From the containerlab host to capture from any interface inside a container simply use: # where $lab_node_name is the name of the container, which is also the name of the network namespace # and $if_name is the interface name inside the container netns ip netns exec $lab_node_name tcpdump -nni $if_name","title":"local capture"},{"location":"manual/wireshark/#remote-capture","text":"If you want to start capture from a remote machine, then add ssh command to the mix: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -nni $if_name \" Capturing remotely with tcpdump makes little sense, but it makes all the difference when wireshark is concerned. Wireshark normally is not installed on the containerlab host, but it more often than not installed on the users machine/laptop. Thus it is possible to use remote capture capability to let wireshark receive the traffic from the remote containerlab node: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | wireshark -k -i - This will start the capture from a given interface and redirect the received flow to the wireshark input. Note Windows users should use WSL and invoke the command similar to the following: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | /mnt/c/Program \\ Files/Wireshark/wireshark.exe -k -i -","title":"remote capture"},{"location":"manual/wireshark/#examples","text":"Lets take the first diagram of this article and see which commands are used to sniff from the highlighted interfaces. In the examples below the wireshark will be used as a sniffing tool and the following naming simplifications and conventions used: $clab_host - address of the containerlab host clab-pcap-srl , clab-pcap-ceos , clab-pcap-linux - container names of the SRL, cEOS and Linux nodes accordingly. SR Linux [1], [4] SR Linux linecard interfaces are named as e<linecard_num>-<port_num> which translates to ethernet-<linecard_num>/<port_num> name inside the NOS itself. So to capture from ethernet-1/1 interface the following command should be used: ssh $clab_host \"ip netns exec $clab -pcap-srl tcpdump -U -nni e1-1 -w -\" | wireshark -k -i - The management interface on the SR Linux container is named mgmt0 , so the relevant command will look like: ssh $clab_host \"ip netns exec $clab -pcap-srl tcpdump -U -nni mgmt0 -w -\" | wireshark -k -i - cEOS [2] Similarly to SR Linux example, to capture the data interface of cEOS is no different. Just pick the right interface: ssh $clab_host \"ip netns exec $clab -pcap-ceos tcpdump -U -nni eth1 -w -\" | wireshark -k -i - Linux container [3] A bare linux container is no different, its interfaces are named ethX where eth0 is the interface connected to the containerlab management network. So to capture from the first data link we will use eth1 interface: ssh $clab_host \"ip netns exec $clab -pcap-linux tcpdump -U -nni eth1 -w -\" | wireshark -k -i - management bridge [5] It is also possible to listen for all management traffic that traverses the containerlab's management network. To do that you firstly need to find out the name of the linux bridge and then capture from it: ssh $clab_host \"tcpdump -U -nni brXXXXXX -w -\" | wireshark -k -i - Note that in this case you do not need to drill into the network namespace, since management bridge is in the default netns. To simplify wireshark remote capturing process users can create a tiny bash script that will save some typing: #!/bin/sh # call this script as `bash script_name.sh <container-name> <interface-name>` ssh <containerlab_address> \"ip netns exec $1 tcpdump -U -nni $2 -w -\" | wireshark -k -i -","title":"Examples"},{"location":"manual/kinds/bridge/","text":"Linux bridge # Containerlab can connect its nodes to a Linux bridge instead of interconnecting the nodes directly. This connectivity option is enabled with bridge kind and opens a variety of integrations that containerlab labs can have with workloads of other types. For example, by connecting a lab node to a bridge we can: allow a node to talk to any workloads (VMs, containers, baremetals) which are connected to that bridge let a node to reach networks which are available via that bridge scale out containerlab labs by running separate labs in different hosts and get network reachibility between them wiring nodes' data interfaces via a broadcast domain (linux bridge) and use vlans to making dynamic connections Using bridge kind # Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file , the bridge needs to be created first. Once the bridge is created, it needs to be referenced as a node inside the topology file: # topology documentation: http://containerlab.srlinux.dev/lab-examples/ext-bridge/ name : br01 topology : kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl srl3 : kind : srl # note, that the bridge br-clab must be created manually br-clab : kind : bridge links : - endpoints : [ \"srl1:e1-1\" , \"br-clab:eth1\" ] - endpoints : [ \"srl2:e1-1\" , \"br-clab:eth2\" ] - endpoints : [ \"srl3:e1-1\" , \"br-clab:eth3\" ] In the example above, node br-clab of kind bridge tells containerlab to identify it as a linux bridge and look for a bridge named br-clab . When connecting other nodes to a bridge, the bridge endpoint must be present in the links section. It doesn't really matter how you name the interfaces of this bridge. In the example above we named them eth1 , eth2 , eth3 accordingly. Chec out \"External bridge\" lab for a ready-made example on how to use bridges.","title":"bridge - Linux bridge"},{"location":"manual/kinds/bridge/#linux-bridge","text":"Containerlab can connect its nodes to a Linux bridge instead of interconnecting the nodes directly. This connectivity option is enabled with bridge kind and opens a variety of integrations that containerlab labs can have with workloads of other types. For example, by connecting a lab node to a bridge we can: allow a node to talk to any workloads (VMs, containers, baremetals) which are connected to that bridge let a node to reach networks which are available via that bridge scale out containerlab labs by running separate labs in different hosts and get network reachibility between them wiring nodes' data interfaces via a broadcast domain (linux bridge) and use vlans to making dynamic connections","title":"Linux bridge"},{"location":"manual/kinds/bridge/#using-bridge-kind","text":"Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file , the bridge needs to be created first. Once the bridge is created, it needs to be referenced as a node inside the topology file: # topology documentation: http://containerlab.srlinux.dev/lab-examples/ext-bridge/ name : br01 topology : kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl srl3 : kind : srl # note, that the bridge br-clab must be created manually br-clab : kind : bridge links : - endpoints : [ \"srl1:e1-1\" , \"br-clab:eth1\" ] - endpoints : [ \"srl2:e1-1\" , \"br-clab:eth2\" ] - endpoints : [ \"srl3:e1-1\" , \"br-clab:eth3\" ] In the example above, node br-clab of kind bridge tells containerlab to identify it as a linux bridge and look for a bridge named br-clab . When connecting other nodes to a bridge, the bridge endpoint must be present in the links section. It doesn't really matter how you name the interfaces of this bridge. In the example above we named them eth1 , eth2 , eth3 accordingly. Chec out \"External bridge\" lab for a ready-made example on how to use bridges.","title":"Using bridge kind"},{"location":"manual/kinds/ceos/","text":"Arista cEOS # Arista cEOS is identified with ceos kind in the topology file . The ceos kind defines a supported feature set and a startup procedure of a ceos node. cEOS nodes launched with containerlab comes up with their management interface eth0 configured with IPv4/6 addresses as assigned by docker hostname assigned to the node name gNMI, Netconf and eAPI services enabled admin user created with password admin Managing ceos nodes # Arista cEOS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running ceos container: docker exec -it <container-name/id> bash CLI to connect to the ceos CLI docker exec -it <container-name/id> Cli NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf gNMI gNMI server is running over port 6030 in non-secure mode using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address>:6030 --insecure \\ -u admin -p admin \\ capabilities Info Default user credentials: admin:admin Interfaces mapping # ceos container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface When containerlab launches ceos node, it will set IPv4/6 addresses as assigned by docker to the eth0 interface and ceos node will boot with that addresses configure. Data interfaces eth1+ need to be configured with IP addressing manually. ceos interfaces output This output demonstrates the IP addressing of the linux interfaces of ceos node. bash-4.2# ip address 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/24 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever <SNIP> 5877: eth0@if5878: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.2/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2001:172:20:20::2/80 scope global valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe14:1402/64 scope link valid_lft forever preferred_lft forever This output shows how the linux interfaces are mapped into the ceos OS. ceos>sh ip int br Address Interface IP Address Status Protocol MTU Owner ----------------- -------------------- ------------ -------------- ---------- ------- Management0 172.20.20.2/24 up up 1500 ceos>sh ipv6 int br Interface Status MTU IPv6 Address Addr State Addr Source --------------- ------------ ---------- -------------------------------- ---------------- ----------- Ma0 up 1500 fe80::42:acff:fe14:1402/64 up link local 2001:172:20:20::2/80 up config As you see, the management interface Ma0 inherits the IP address that docker assigned to ceos container management interface. Features and options # Node configuration # cEOS nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of ceos kind with a basic config or to provide a custom config file that will be used as a startup config instead. Default node configuration # When a node is defined without config statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : ceos topology : nodes : ceos : kind : ceos The generated config will be saved by the path clab-<lab_name>/<node-name>/flash/startup-config . Using the example topology presented above, the exact path to the config will be clab-ceos/ceos/flash/startup-config . User defined config # It is possible to make ceos nodes to boot up with a user-defined config instead of a built-in one. With a config property a user sets the path to the config file that will be mounted to a container and used as a startup config: name : ceos_lab topology : nodes : ceos : kind : ceos config : myconfig.conf With such topology file containerlab is instructed to take a file myconfig.conf from the current working directory, copy it to the lab directory for that specific node under the /flash/startup-config name and mount that dir to the container. This will result in this config to act as a startup config for the node. Saving configuration # With containerlab save command it's possible to save running cEOS configuration into a file. The configuration will be saved by conf-saved.conf path in the relevant node directory. Container configuration # To start an Arista cEOS node containerlab uses the configuration instructions described in Arista Forums 1 . The exact parameters are outlined below. Startup command /sbin/init systemd.setenv=INTFTYPE=eth systemd.setenv=ETBA=4 systemd.setenv=SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT=1 systemd.setenv=CEOS=1 systemd.setenv=EOS_PLATFORM=ceoslab systemd.setenv=container=docker systemd.setenv=MAPETH0=1 systemd.setenv=MGMT_INTF=eth0 Environment variables CEOS:1 EOS_PLATFORM\":ceoslab container:docker ETBA:1 SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT:1 INTFTYPE:eth MAPETH0:1 MGMT_INTF:eth0 File mounts # When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For ceos kind containerlab creates flash directory for each ceos node and mounts these folders by /mnt/flash paths. \u276f tree clab-srlceos01/ceos clab-srlceos01/ceos \u2514\u2500\u2500 flash \u251c\u2500\u2500 AsuFastPktTransmit.log \u251c\u2500\u2500 debug \u2502 \u2514\u2500\u2500 proc \u2502 \u2514\u2500\u2500 modules \u251c\u2500\u2500 fastpkttx.backup \u251c\u2500\u2500 Fossil \u251c\u2500\u2500 kickstart-config \u251c\u2500\u2500 persist \u2502 \u251c\u2500\u2500 local \u2502 \u251c\u2500\u2500 messages \u2502 \u251c\u2500\u2500 persistentRestartLog \u2502 \u251c\u2500\u2500 secure \u2502 \u2514\u2500\u2500 sys \u251c\u2500\u2500 schedule \u2502 \u2514\u2500\u2500 tech-support \u2502 \u2514\u2500\u2500 ceos_tech-support_2021-01-14.0907.log.gz \u251c\u2500\u2500 SsuRestoreLegacy.log \u251c\u2500\u2500 SsuRestore.log \u2514\u2500\u2500 startup-config 9 directories, 11 files Lab examples # The following labs feature cEOS node: SR Linux and cEOS https://eos.arista.com/ceos-lab-topo/ \u21a9","title":"ceos - Arista cEOS"},{"location":"manual/kinds/ceos/#arista-ceos","text":"Arista cEOS is identified with ceos kind in the topology file . The ceos kind defines a supported feature set and a startup procedure of a ceos node. cEOS nodes launched with containerlab comes up with their management interface eth0 configured with IPv4/6 addresses as assigned by docker hostname assigned to the node name gNMI, Netconf and eAPI services enabled admin user created with password admin","title":"Arista cEOS"},{"location":"manual/kinds/ceos/#managing-ceos-nodes","text":"Arista cEOS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running ceos container: docker exec -it <container-name/id> bash CLI to connect to the ceos CLI docker exec -it <container-name/id> Cli NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf gNMI gNMI server is running over port 6030 in non-secure mode using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address>:6030 --insecure \\ -u admin -p admin \\ capabilities Info Default user credentials: admin:admin","title":"Managing ceos nodes"},{"location":"manual/kinds/ceos/#interfaces-mapping","text":"ceos container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface When containerlab launches ceos node, it will set IPv4/6 addresses as assigned by docker to the eth0 interface and ceos node will boot with that addresses configure. Data interfaces eth1+ need to be configured with IP addressing manually. ceos interfaces output This output demonstrates the IP addressing of the linux interfaces of ceos node. bash-4.2# ip address 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/24 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever <SNIP> 5877: eth0@if5878: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.2/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2001:172:20:20::2/80 scope global valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe14:1402/64 scope link valid_lft forever preferred_lft forever This output shows how the linux interfaces are mapped into the ceos OS. ceos>sh ip int br Address Interface IP Address Status Protocol MTU Owner ----------------- -------------------- ------------ -------------- ---------- ------- Management0 172.20.20.2/24 up up 1500 ceos>sh ipv6 int br Interface Status MTU IPv6 Address Addr State Addr Source --------------- ------------ ---------- -------------------------------- ---------------- ----------- Ma0 up 1500 fe80::42:acff:fe14:1402/64 up link local 2001:172:20:20::2/80 up config As you see, the management interface Ma0 inherits the IP address that docker assigned to ceos container management interface.","title":"Interfaces mapping"},{"location":"manual/kinds/ceos/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/ceos/#node-configuration","text":"cEOS nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of ceos kind with a basic config or to provide a custom config file that will be used as a startup config instead.","title":"Node configuration"},{"location":"manual/kinds/ceos/#default-node-configuration","text":"When a node is defined without config statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : ceos topology : nodes : ceos : kind : ceos The generated config will be saved by the path clab-<lab_name>/<node-name>/flash/startup-config . Using the example topology presented above, the exact path to the config will be clab-ceos/ceos/flash/startup-config .","title":"Default node configuration"},{"location":"manual/kinds/ceos/#user-defined-config","text":"It is possible to make ceos nodes to boot up with a user-defined config instead of a built-in one. With a config property a user sets the path to the config file that will be mounted to a container and used as a startup config: name : ceos_lab topology : nodes : ceos : kind : ceos config : myconfig.conf With such topology file containerlab is instructed to take a file myconfig.conf from the current working directory, copy it to the lab directory for that specific node under the /flash/startup-config name and mount that dir to the container. This will result in this config to act as a startup config for the node.","title":"User defined config"},{"location":"manual/kinds/ceos/#saving-configuration","text":"With containerlab save command it's possible to save running cEOS configuration into a file. The configuration will be saved by conf-saved.conf path in the relevant node directory.","title":"Saving configuration"},{"location":"manual/kinds/ceos/#container-configuration","text":"To start an Arista cEOS node containerlab uses the configuration instructions described in Arista Forums 1 . The exact parameters are outlined below. Startup command /sbin/init systemd.setenv=INTFTYPE=eth systemd.setenv=ETBA=4 systemd.setenv=SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT=1 systemd.setenv=CEOS=1 systemd.setenv=EOS_PLATFORM=ceoslab systemd.setenv=container=docker systemd.setenv=MAPETH0=1 systemd.setenv=MGMT_INTF=eth0 Environment variables CEOS:1 EOS_PLATFORM\":ceoslab container:docker ETBA:1 SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT:1 INTFTYPE:eth MAPETH0:1 MGMT_INTF:eth0","title":"Container configuration"},{"location":"manual/kinds/ceos/#file-mounts","text":"When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For ceos kind containerlab creates flash directory for each ceos node and mounts these folders by /mnt/flash paths. \u276f tree clab-srlceos01/ceos clab-srlceos01/ceos \u2514\u2500\u2500 flash \u251c\u2500\u2500 AsuFastPktTransmit.log \u251c\u2500\u2500 debug \u2502 \u2514\u2500\u2500 proc \u2502 \u2514\u2500\u2500 modules \u251c\u2500\u2500 fastpkttx.backup \u251c\u2500\u2500 Fossil \u251c\u2500\u2500 kickstart-config \u251c\u2500\u2500 persist \u2502 \u251c\u2500\u2500 local \u2502 \u251c\u2500\u2500 messages \u2502 \u251c\u2500\u2500 persistentRestartLog \u2502 \u251c\u2500\u2500 secure \u2502 \u2514\u2500\u2500 sys \u251c\u2500\u2500 schedule \u2502 \u2514\u2500\u2500 tech-support \u2502 \u2514\u2500\u2500 ceos_tech-support_2021-01-14.0907.log.gz \u251c\u2500\u2500 SsuRestoreLegacy.log \u251c\u2500\u2500 SsuRestore.log \u2514\u2500\u2500 startup-config 9 directories, 11 files","title":"File mounts"},{"location":"manual/kinds/ceos/#lab-examples","text":"The following labs feature cEOS node: SR Linux and cEOS https://eos.arista.com/ceos-lab-topo/ \u21a9","title":"Lab examples"},{"location":"manual/kinds/crpd/","text":"Juniper cRPD # Juniper cRPD is identified with crpd kind in the topology file . A kind defines a supported feature set and a startup procedure of a crpd node. cRPD nodes launched with containerlab comes up pre-provisioned with SSH service enabled, root user created and NETCONF enabled. Managing cRPD nodes # Juniper cRPD node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running cRPD container: docker exec -it <container-name/id> bash CLI to connect to the cRPD CLI docker exec -it <container-name/id> cli NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf Info Default user credentials: root:clab123 Interfaces mapping # cRPD container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface When containerlab launches cRPD node, it will assign IPv4/6 address to the eth0 interface. Data interface eth1 needs to be configured with IP addressing manually. cRPD interfaces output This output demonstrates the IP addressing of the linux interfaces of cRPD node. \u276f docker exec -it clab-crpd-crpd bash ===> Containerized Routing Protocols Daemon (CRPD) Copyright (C) 2020, Juniper Networks, Inc. All rights reserved. <=== root@crpd:/# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever <SNIP> 5767: eth0@if5768: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.3/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2001:172:20:20::3/80 scope global nodad valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe14:1403/64 scope link valid_lft forever preferred_lft forever 5770: eth1@if5769: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether b6:d3:63:f1:cb:7b brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::b4d3:63ff:fef1:cb7b/64 scope link valid_lft forever preferred_lft forever This output shows how the linux interfaces are mapped into the cRPD OS. root@crpd> show interfaces routing Interface State Addresses lsi Up tunl0 Up ISO enabled sit0 Up ISO enabled INET6 ::172.20.20.3 INET6 ::127.0.0.1 lo.0 Up ISO enabled INET6 fe80::1 ip6tnl0 Up ISO enabled INET6 fe80::42a:e9ff:fede:a0e3 gretap0 Down ISO enabled gre0 Up ISO enabled eth1 Up ISO enabled INET6 fe80::b4d3:63ff:fef1:cb7b eth0 Up ISO enabled INET 172.20.20.3 INET6 2001:172:20:20::3 INET6 fe80::42:acff:fe14:1403 As you see, the management interface eth0 inherits the IP address that docker assigned to cRPD container. Features and options # Node configuration # cRPD nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of crpd kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead. Default node configuration # When a node is defined without config statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : crpd topology : nodes : crpd : kind : crpd The generated config will be saved by the path clab-<lab_name>/<node-name>/config/juniper.conf . Using the example topology presented above, the exact path to the config will be clab-crpd/crpd/config/juniper.conf . User defined config # It is possible to make cRPD nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container: name : crpd_lab topology : nodes : crpd : kind : crpd config : myconfig.conf With such topology file containerlab is instructed to take a file myconfig.conf from the current working directory, copy it to the lab directory for that specific node under the /config/juniper.conf name and mount that dir to the container. This will result in this config to act as a startup config for the node. Saving configuration # With containerlab save command it's possible to save running cRPD configuration into a file. The configuration will be saved by conf-saved.conf path in the relevant node directory. License # cRPD containers require a license file to have some features to be activated. With a license directive it's possible to provide a path to a license file that will be copied over to the nodes configuration directory by the /config/license.conf path. Container configuration # To launch cRPD, containerlab uses the deployment instructions that are provided in the TechLibrary as well as leveraging some setup steps outlined by Matt Oswalt in this blog post . The SSH service is already enabled for root login, so nothing is needed to be done additionally. The root user is created already with the clab123 password. File mounts # When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For crpd kind containerlab creates config and log directories for each crpd node and mounts these folders by /config and /var/log paths accordingly. \u276f tree clab-crpd/crpd clab-crpd/crpd \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 juniper.conf \u2502 \u251c\u2500\u2500 license \u2502 \u2502 \u2514\u2500\u2500 safenet \u2502 \u2514\u2500\u2500 sshd_config \u2514\u2500\u2500 log \u251c\u2500\u2500 cscript.log \u251c\u2500\u2500 license \u251c\u2500\u2500 messages \u251c\u2500\u2500 mgd-api \u251c\u2500\u2500 na-grpcd \u251c\u2500\u2500 __policy_names_rpdc__ \u2514\u2500\u2500 __policy_names_rpdn__ 4 directories, 9 files Lab examples # The following labs feature cRPD node: SR Linux and cRPD","title":"crpd - Juniper cRPD"},{"location":"manual/kinds/crpd/#juniper-crpd","text":"Juniper cRPD is identified with crpd kind in the topology file . A kind defines a supported feature set and a startup procedure of a crpd node. cRPD nodes launched with containerlab comes up pre-provisioned with SSH service enabled, root user created and NETCONF enabled.","title":"Juniper cRPD"},{"location":"manual/kinds/crpd/#managing-crpd-nodes","text":"Juniper cRPD node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running cRPD container: docker exec -it <container-name/id> bash CLI to connect to the cRPD CLI docker exec -it <container-name/id> cli NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf Info Default user credentials: root:clab123","title":"Managing cRPD nodes"},{"location":"manual/kinds/crpd/#interfaces-mapping","text":"cRPD container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface When containerlab launches cRPD node, it will assign IPv4/6 address to the eth0 interface. Data interface eth1 needs to be configured with IP addressing manually. cRPD interfaces output This output demonstrates the IP addressing of the linux interfaces of cRPD node. \u276f docker exec -it clab-crpd-crpd bash ===> Containerized Routing Protocols Daemon (CRPD) Copyright (C) 2020, Juniper Networks, Inc. All rights reserved. <=== root@crpd:/# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever <SNIP> 5767: eth0@if5768: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.3/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2001:172:20:20::3/80 scope global nodad valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe14:1403/64 scope link valid_lft forever preferred_lft forever 5770: eth1@if5769: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether b6:d3:63:f1:cb:7b brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::b4d3:63ff:fef1:cb7b/64 scope link valid_lft forever preferred_lft forever This output shows how the linux interfaces are mapped into the cRPD OS. root@crpd> show interfaces routing Interface State Addresses lsi Up tunl0 Up ISO enabled sit0 Up ISO enabled INET6 ::172.20.20.3 INET6 ::127.0.0.1 lo.0 Up ISO enabled INET6 fe80::1 ip6tnl0 Up ISO enabled INET6 fe80::42a:e9ff:fede:a0e3 gretap0 Down ISO enabled gre0 Up ISO enabled eth1 Up ISO enabled INET6 fe80::b4d3:63ff:fef1:cb7b eth0 Up ISO enabled INET 172.20.20.3 INET6 2001:172:20:20::3 INET6 fe80::42:acff:fe14:1403 As you see, the management interface eth0 inherits the IP address that docker assigned to cRPD container.","title":"Interfaces mapping"},{"location":"manual/kinds/crpd/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/crpd/#node-configuration","text":"cRPD nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of crpd kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead.","title":"Node configuration"},{"location":"manual/kinds/crpd/#default-node-configuration","text":"When a node is defined without config statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : crpd topology : nodes : crpd : kind : crpd The generated config will be saved by the path clab-<lab_name>/<node-name>/config/juniper.conf . Using the example topology presented above, the exact path to the config will be clab-crpd/crpd/config/juniper.conf .","title":"Default node configuration"},{"location":"manual/kinds/crpd/#user-defined-config","text":"It is possible to make cRPD nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container: name : crpd_lab topology : nodes : crpd : kind : crpd config : myconfig.conf With such topology file containerlab is instructed to take a file myconfig.conf from the current working directory, copy it to the lab directory for that specific node under the /config/juniper.conf name and mount that dir to the container. This will result in this config to act as a startup config for the node.","title":"User defined config"},{"location":"manual/kinds/crpd/#saving-configuration","text":"With containerlab save command it's possible to save running cRPD configuration into a file. The configuration will be saved by conf-saved.conf path in the relevant node directory.","title":"Saving configuration"},{"location":"manual/kinds/crpd/#license","text":"cRPD containers require a license file to have some features to be activated. With a license directive it's possible to provide a path to a license file that will be copied over to the nodes configuration directory by the /config/license.conf path.","title":"License"},{"location":"manual/kinds/crpd/#container-configuration","text":"To launch cRPD, containerlab uses the deployment instructions that are provided in the TechLibrary as well as leveraging some setup steps outlined by Matt Oswalt in this blog post . The SSH service is already enabled for root login, so nothing is needed to be done additionally. The root user is created already with the clab123 password.","title":"Container configuration"},{"location":"manual/kinds/crpd/#file-mounts","text":"When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For crpd kind containerlab creates config and log directories for each crpd node and mounts these folders by /config and /var/log paths accordingly. \u276f tree clab-crpd/crpd clab-crpd/crpd \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 juniper.conf \u2502 \u251c\u2500\u2500 license \u2502 \u2502 \u2514\u2500\u2500 safenet \u2502 \u2514\u2500\u2500 sshd_config \u2514\u2500\u2500 log \u251c\u2500\u2500 cscript.log \u251c\u2500\u2500 license \u251c\u2500\u2500 messages \u251c\u2500\u2500 mgd-api \u251c\u2500\u2500 na-grpcd \u251c\u2500\u2500 __policy_names_rpdc__ \u2514\u2500\u2500 __policy_names_rpdn__ 4 directories, 9 files","title":"File mounts"},{"location":"manual/kinds/crpd/#lab-examples","text":"The following labs feature cRPD node: SR Linux and cRPD","title":"Lab examples"},{"location":"manual/kinds/kinds/","text":"Kinds # Containerlab launches, wires up and manages container based labs. The steps required to launch a debian or centos container image aren't at all different. On the other hand, Nokia SR Linux launching procedure is nothing like the one for Arista cEOS. Things like required syscalls, mounted directories, entrypoints and commands to execute are all different for the containerized NOS'es. To let containerlab to understand which launching sequence to use the notion of a kind was introduced. Essentially we remove the need to understand certain setup peculiarities of different NOS'es by abstracting them with kinds . Given the following topology definition file , containerlab is able to know how to launch node1 as SR Linux container and node2 as a cEOS one because they are associated with the kinds: name : srlceos01 topology : nodes : node1 : kind : srl # node1 is of srl kind type : ixrd2 image : srlinux license : license.key node2 : kind : ceos # node2 is of ceos kind image : ceos links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] Containerlab supports a fixed number of kinds. Within each predefined kind we store the necessary information that is used to launch the container successfully. The following kinds are supported or in the roadmap of containerlab: Name Kind Status Nokia SR Linux srl supported Juniper cRPD crpd supported Arista cEOS ceos supported SONiC sonic supported Nokia SR OS vr-sros supported Juniper vMX vr-vmx supported Cisco XRv9k vr-xrv9k supported Cisco XRv vr-xrv supported Linux container linux supported Linux bridge bridge supported Refer to a specific kind documentation article to see the details about it.","title":"About"},{"location":"manual/kinds/kinds/#kinds","text":"Containerlab launches, wires up and manages container based labs. The steps required to launch a debian or centos container image aren't at all different. On the other hand, Nokia SR Linux launching procedure is nothing like the one for Arista cEOS. Things like required syscalls, mounted directories, entrypoints and commands to execute are all different for the containerized NOS'es. To let containerlab to understand which launching sequence to use the notion of a kind was introduced. Essentially we remove the need to understand certain setup peculiarities of different NOS'es by abstracting them with kinds . Given the following topology definition file , containerlab is able to know how to launch node1 as SR Linux container and node2 as a cEOS one because they are associated with the kinds: name : srlceos01 topology : nodes : node1 : kind : srl # node1 is of srl kind type : ixrd2 image : srlinux license : license.key node2 : kind : ceos # node2 is of ceos kind image : ceos links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] Containerlab supports a fixed number of kinds. Within each predefined kind we store the necessary information that is used to launch the container successfully. The following kinds are supported or in the roadmap of containerlab: Name Kind Status Nokia SR Linux srl supported Juniper cRPD crpd supported Arista cEOS ceos supported SONiC sonic supported Nokia SR OS vr-sros supported Juniper vMX vr-vmx supported Cisco XRv9k vr-xrv9k supported Cisco XRv vr-xrv supported Linux container linux supported Linux bridge bridge supported Refer to a specific kind documentation article to see the details about it.","title":"Kinds"},{"location":"manual/kinds/linux/","text":"Linux container # Labs deployed with containerlab are endlessly flexible, mostly because containerlab can spin up and wire regular containers as part of the lab topology. Nowadays more and more workloads are packaged into containers, and containerlab users can nicely integrate them in their labs following a familiar docker' compose-like syntax. As long as the networking domain is considered, the most common use case for bare linux containers is to introduce \"clients\" or traffic generators which are connected to the network nodes or host telemetry/monitoring stacks. But, of course, you are free to choose which container to add into your lab, there is not restriction to that! Using linux containers # As with any other node, the linux container is a node of a specific kind, linux in this case. # a simple topo of two alpine containers connected with each other name : demo topology : n1 : kind : linux image : alpine:latest n2 : kind : linux image : alpine:latest links : - endpoints : [ \"n1:eth1\" , \"n2:eth1\" ] With a topology file like that, the nodes will start and both containers will have eth1 link available. Containerlab tries to deliver the same level of flexibility in container configuration as docker-compose has. With linux containers it is possible to use the following node configuration parameters: image - to set an image source for the container binds - to mount files from the host to a container ports - to expose services running in the container to a host env - to set environment variables user - to set a user that will be used inside the container system cmd - to provide a command that will be executed when the container is started share - to provide expose container' service via myscoket.io integration","title":"linux - Linux container"},{"location":"manual/kinds/linux/#linux-container","text":"Labs deployed with containerlab are endlessly flexible, mostly because containerlab can spin up and wire regular containers as part of the lab topology. Nowadays more and more workloads are packaged into containers, and containerlab users can nicely integrate them in their labs following a familiar docker' compose-like syntax. As long as the networking domain is considered, the most common use case for bare linux containers is to introduce \"clients\" or traffic generators which are connected to the network nodes or host telemetry/monitoring stacks. But, of course, you are free to choose which container to add into your lab, there is not restriction to that!","title":"Linux container"},{"location":"manual/kinds/linux/#using-linux-containers","text":"As with any other node, the linux container is a node of a specific kind, linux in this case. # a simple topo of two alpine containers connected with each other name : demo topology : n1 : kind : linux image : alpine:latest n2 : kind : linux image : alpine:latest links : - endpoints : [ \"n1:eth1\" , \"n2:eth1\" ] With a topology file like that, the nodes will start and both containers will have eth1 link available. Containerlab tries to deliver the same level of flexibility in container configuration as docker-compose has. With linux containers it is possible to use the following node configuration parameters: image - to set an image source for the container binds - to mount files from the host to a container ports - to expose services running in the container to a host env - to set environment variables user - to set a user that will be used inside the container system cmd - to provide a command that will be executed when the container is started share - to provide expose container' service via myscoket.io integration","title":"Using linux containers"},{"location":"manual/kinds/sonic-vs/","text":"SONiC # SONiC is identified with sonic-vs kind in the topology file . A kind defines a supported feature set and a startup procedure of a sonic-vs node. Info vs in the name of a kind refers to a SONiC platform type. sonic-vs nodes launched with containerlab comes without any additional configuration. Managing sonic-vs nodes # SONiC node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running sonic-vs container: docker exec -it <container-name/id> bash CLI to connect to the sonic-vs CLI (vtysh) docker exec -it <container-name/id> vtysh Interfaces mapping # sonic-vs container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data (front-panel port) interface When containerlab launches sonic-vs node, it will assign IPv4/6 address to the eth0 interface. Data interface eth1 which is mapped to Ethernet0 port needs to be configured with IP addressing manually. See Lab examples for exact configurations. Lab examples # The following labs feature sonic-vs node: SR Linux and sonic-vs","title":"sonic-vs - SONiC"},{"location":"manual/kinds/sonic-vs/#sonic","text":"SONiC is identified with sonic-vs kind in the topology file . A kind defines a supported feature set and a startup procedure of a sonic-vs node. Info vs in the name of a kind refers to a SONiC platform type. sonic-vs nodes launched with containerlab comes without any additional configuration.","title":"SONiC"},{"location":"manual/kinds/sonic-vs/#managing-sonic-vs-nodes","text":"SONiC node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running sonic-vs container: docker exec -it <container-name/id> bash CLI to connect to the sonic-vs CLI (vtysh) docker exec -it <container-name/id> vtysh","title":"Managing sonic-vs nodes"},{"location":"manual/kinds/sonic-vs/#interfaces-mapping","text":"sonic-vs container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data (front-panel port) interface When containerlab launches sonic-vs node, it will assign IPv4/6 address to the eth0 interface. Data interface eth1 which is mapped to Ethernet0 port needs to be configured with IP addressing manually. See Lab examples for exact configurations.","title":"Interfaces mapping"},{"location":"manual/kinds/sonic-vs/#lab-examples","text":"The following labs feature sonic-vs node: SR Linux and sonic-vs","title":"Lab examples"},{"location":"manual/kinds/srl/","text":"Nokia SR Linux # Nokia SR Linux NOS is identified with srl kind in the topology file . A kind defines a supported feature set and a startup procedure of a node. Managing SR Linux nodes # There are many ways to manage SR Linux nodes, ranging from classic CLI management all the way up to the gNMI programming. Here is a short summary on how to access those interfaces: bash to connect to a bash shell of a running SR Linux container: docker exec -it <container-name/id> bash CLI to connect to the SR Linux CLI docker exec -it <container-name/id> sr_cli gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --skip-verify \\ -u admin -p admin \\ -e json_ietf \\ get --path /system/name/host-name Info Default user credentials: admin:admin Features and options # Types # For SR Linux nodes type defines the hardware variant that this node will emulate. The available type values are: ixr6 , ixr10 , ixrd1 , ixrd2 , ixrd3 which correspond to a hardware variant of Nokia 7250/7220 IXR chassis. By default, ixr6 type will be used by containerlab. Based on the provided type, containerlab will generate the topology file that will be mounted to SR Linux container and make it boot in a chosen HW variant. Node configuration # SR Linux nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of srl kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead. Default node configuration # When a node is defined without config statement present, containerlab will generate an empty config from this template and put it in that directory. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : srl_lab topology : nodes : srl1 : kind : srl type : ixr6 license : lic.key The generated config will be saved by the path clab-<lab_name>/<node-name>/config/config.json . Using the example topology presented above, the exact path to the config will be clab-srl_lab/srl1/config/config.json . User defined config # It is possible to make SR Linux nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container: name : srl_lab topology : nodes : srl1 : kind : srl type : ixr6 license : lic.key config : myconfig.json With such topology file containerlab is instructed to take a file myconfig.json from the current working directory, copy it to the lab directory for that specific node under the config.json name and mount that file to the container. This will result in this config to act as a startup config for the node. Saving configuration # As was explained in the Node configuration section, SR Linux containers can make their config persist because config files are provided to the containers from the host via bind mount. There are two options to make a running configuration to be saved in a file. Rewriting startup configuration # When a user configures SR Linux node via CLI the changes are saved into the running configuration stored in memory. To save the running configuration as a startup configuration the user needs to execute the tools system configuration save CLI command. This will write the config to the config.json file that holds the startup config and is exposed to the host. Generating config checkpoint # If the startup configuration must be left intact, use an alternative method of saving the configuration checkpoint: tools system configuration generate-checkpoint . This command will create a checkpoint-x.json file that you will be able to find in the same config directory. Containerlab allows to perform a bulk configuration-save operation that can be executed with containerlab save -t <path-to-topo-file> command. With this command, every node that supports the \"save\" operation will execute a command to save it's running configuration to a persistent location. For SR Linux nodes the save command will trigger the checkpoint generation: \u276f containerlab save -t srl02.yml INFO[0000] Getting topology information from ../srl02.yml file... INFO[0001] clab-srl02-srl1 output: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-12-03T15:12:46.854Z' and comment '' INFO[0004] clab-srl02-srl2 output: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-12-03T15:12:49.892Z' and comment '' License # SR Linux containers require a license file to be provided. With a license directive it's possible to provide a path to a license file that will be used for srl nodes. Container configuration # To start an SR Linux NOS containerlab uses the configuration that is described in SR Linux Software Installation Guide Startup command sudo bash -c /opt/srlinux/bin/sr_linux Syscalls net.ipv4.ip_forward = \"0\" net.ipv6.conf.all.disable_ipv6 = \"0\" net.ipv6.conf.all.accept_dad = \"0\" net.ipv6.conf.default.accept_dad = \"0\" net.ipv6.conf.all.autoconf = \"0\" net.ipv6.conf.default.autoconf = \"0\" Environment variables SRLINUX=1 File mounts # Config directory # When a user starts a lab, containerlab creates a lab directory for storing configuration artifacts . For srl kind containerlab creates directories for each node of that kind. ~/clab/clab-srl02 \u276f ls -lah srl1 drwxrwxrwx+ 6 1002 1002 87 Dec 1 22:11 config -rw-r--r-- 1 root root 2.8K Dec 1 22:11 license.key -rw-r--r-- 1 root root 4.4K Dec 1 22:11 srlinux.conf -rw-r--r-- 1 root root 233 Dec 1 22:11 topology.yml The config directory is mounted to container's /etc/opt/srlinux/ in rw mode and will effectively contain configuration that SR Linux runs of as well as the files that SR Linux keeps in its /etc/opt/srlinux/ directory: \u276f ls srl1/config banner cli config.json devices tls ztp CLI env config # Another file that SR Linux expects to have is the srlinux.conf file that contains CLI environment config. Containerlab uses a template of this file and mounts it to /home/admin/.srlinux.conf in rw mode. Topology file # The topology file that defines the emulated hardware type is driven by the value of the kinds type parameter. Depending on a specified type the appropriate content will be populated into the topology.yml file that will get mounted to /tmp/topology.yml directory inside the container in ro mode.","title":"srl - Nokia SR Linux"},{"location":"manual/kinds/srl/#nokia-sr-linux","text":"Nokia SR Linux NOS is identified with srl kind in the topology file . A kind defines a supported feature set and a startup procedure of a node.","title":"Nokia SR Linux"},{"location":"manual/kinds/srl/#managing-sr-linux-nodes","text":"There are many ways to manage SR Linux nodes, ranging from classic CLI management all the way up to the gNMI programming. Here is a short summary on how to access those interfaces: bash to connect to a bash shell of a running SR Linux container: docker exec -it <container-name/id> bash CLI to connect to the SR Linux CLI docker exec -it <container-name/id> sr_cli gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --skip-verify \\ -u admin -p admin \\ -e json_ietf \\ get --path /system/name/host-name Info Default user credentials: admin:admin","title":"Managing SR Linux nodes"},{"location":"manual/kinds/srl/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/srl/#types","text":"For SR Linux nodes type defines the hardware variant that this node will emulate. The available type values are: ixr6 , ixr10 , ixrd1 , ixrd2 , ixrd3 which correspond to a hardware variant of Nokia 7250/7220 IXR chassis. By default, ixr6 type will be used by containerlab. Based on the provided type, containerlab will generate the topology file that will be mounted to SR Linux container and make it boot in a chosen HW variant.","title":"Types"},{"location":"manual/kinds/srl/#node-configuration","text":"SR Linux nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of srl kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead.","title":"Node configuration"},{"location":"manual/kinds/srl/#default-node-configuration","text":"When a node is defined without config statement present, containerlab will generate an empty config from this template and put it in that directory. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : srl_lab topology : nodes : srl1 : kind : srl type : ixr6 license : lic.key The generated config will be saved by the path clab-<lab_name>/<node-name>/config/config.json . Using the example topology presented above, the exact path to the config will be clab-srl_lab/srl1/config/config.json .","title":"Default node configuration"},{"location":"manual/kinds/srl/#user-defined-config","text":"It is possible to make SR Linux nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container: name : srl_lab topology : nodes : srl1 : kind : srl type : ixr6 license : lic.key config : myconfig.json With such topology file containerlab is instructed to take a file myconfig.json from the current working directory, copy it to the lab directory for that specific node under the config.json name and mount that file to the container. This will result in this config to act as a startup config for the node.","title":"User defined config"},{"location":"manual/kinds/srl/#saving-configuration","text":"As was explained in the Node configuration section, SR Linux containers can make their config persist because config files are provided to the containers from the host via bind mount. There are two options to make a running configuration to be saved in a file.","title":"Saving configuration"},{"location":"manual/kinds/srl/#rewriting-startup-configuration","text":"When a user configures SR Linux node via CLI the changes are saved into the running configuration stored in memory. To save the running configuration as a startup configuration the user needs to execute the tools system configuration save CLI command. This will write the config to the config.json file that holds the startup config and is exposed to the host.","title":"Rewriting startup configuration"},{"location":"manual/kinds/srl/#generating-config-checkpoint","text":"If the startup configuration must be left intact, use an alternative method of saving the configuration checkpoint: tools system configuration generate-checkpoint . This command will create a checkpoint-x.json file that you will be able to find in the same config directory. Containerlab allows to perform a bulk configuration-save operation that can be executed with containerlab save -t <path-to-topo-file> command. With this command, every node that supports the \"save\" operation will execute a command to save it's running configuration to a persistent location. For SR Linux nodes the save command will trigger the checkpoint generation: \u276f containerlab save -t srl02.yml INFO[0000] Getting topology information from ../srl02.yml file... INFO[0001] clab-srl02-srl1 output: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-12-03T15:12:46.854Z' and comment '' INFO[0004] clab-srl02-srl2 output: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-12-03T15:12:49.892Z' and comment ''","title":"Generating config checkpoint"},{"location":"manual/kinds/srl/#license","text":"SR Linux containers require a license file to be provided. With a license directive it's possible to provide a path to a license file that will be used for srl nodes.","title":"License"},{"location":"manual/kinds/srl/#container-configuration","text":"To start an SR Linux NOS containerlab uses the configuration that is described in SR Linux Software Installation Guide Startup command sudo bash -c /opt/srlinux/bin/sr_linux Syscalls net.ipv4.ip_forward = \"0\" net.ipv6.conf.all.disable_ipv6 = \"0\" net.ipv6.conf.all.accept_dad = \"0\" net.ipv6.conf.default.accept_dad = \"0\" net.ipv6.conf.all.autoconf = \"0\" net.ipv6.conf.default.autoconf = \"0\" Environment variables SRLINUX=1","title":"Container configuration"},{"location":"manual/kinds/srl/#file-mounts","text":"","title":"File mounts"},{"location":"manual/kinds/srl/#config-directory","text":"When a user starts a lab, containerlab creates a lab directory for storing configuration artifacts . For srl kind containerlab creates directories for each node of that kind. ~/clab/clab-srl02 \u276f ls -lah srl1 drwxrwxrwx+ 6 1002 1002 87 Dec 1 22:11 config -rw-r--r-- 1 root root 2.8K Dec 1 22:11 license.key -rw-r--r-- 1 root root 4.4K Dec 1 22:11 srlinux.conf -rw-r--r-- 1 root root 233 Dec 1 22:11 topology.yml The config directory is mounted to container's /etc/opt/srlinux/ in rw mode and will effectively contain configuration that SR Linux runs of as well as the files that SR Linux keeps in its /etc/opt/srlinux/ directory: \u276f ls srl1/config banner cli config.json devices tls ztp","title":"Config directory"},{"location":"manual/kinds/srl/#cli-env-config","text":"Another file that SR Linux expects to have is the srlinux.conf file that contains CLI environment config. Containerlab uses a template of this file and mounts it to /home/admin/.srlinux.conf in rw mode.","title":"CLI env config"},{"location":"manual/kinds/srl/#topology-file","text":"The topology file that defines the emulated hardware type is driven by the value of the kinds type parameter. Depending on a specified type the appropriate content will be populated into the topology.yml file that will get mounted to /tmp/topology.yml directory inside the container in ro mode.","title":"Topology file"},{"location":"manual/kinds/vr-sros/","text":"Nokia SR OS # Nokia SR OS virtualized router is identified with vr-sros kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-sros nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled. Managing vr-sros nodes # Note Containers with SR OS inside will take ~3min to fully boot. You can monitor the progress with watch docker ps waiting till the status will change to healthy . Nokia SR OS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-sros container: docker exec -it <container-name/id> bash CLI via SSH to connect to the SR OS CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u admin -p admin \\ capabilities Info Default user credentials: admin:admin Interfaces mapping # vr-sros container uses the following mapping for its interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of SR OS line card eth2+ - second and subsequent data interface data interfaces need to be defined consequently Note, that data interfaces need to be defined in the topology file in a consequent fashion. Meaning that no gaps are allowed between the interfaces definition: links : - endpoints : [ \"srl:e1-1\" , \"sros:eth1\" ] # this is not allowed by vr-sros kind. # if eth5 is needed, define eth2-4 as well - endpoints : [ \"srl:e1-5\" , \"sros:eth5\" ] When containerlab launches vr-sros node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Variants # Virtual SR OS simulator can be run in multiple HW variants as explained in the vSIM installation guide . vr-sros container images come with pre-packaged SR OS variants as defined in the upstream repo as well as support custom variant definition . The pre-packaged variants are identified by the variant name and come up with cards and mda already configured. Custom variants, on the other hand, give users the full flexibility in emulated hardware configuration, but cards and mdas would need to be configured manually. To make vr-sros to boot in one of the pacakged variants use its name like that: topology : nodes : sros : kind : vr-sros image : vrnetlab/vr-sros:20.10.R1 type : sr-1s # if type omitted, the default sr-1 variant will be used license : license-sros20.txt Custom variant can be defined as simple as that: # for distributed chassis CPM and IOM are indicated with markers cp: and lc: # notice the delimeter string `___` that MUST be present between CPM and IOM portions # max_nics value is provided in `lc` part. # mem is provided in GB # quote the string value type : \"cp: cpu=2 ram=4 chassis=ixr-e slot=A card=cpm-ixr-e ___ lc: cpu=2 ram=4 max_nics=34 chassis=ixr-e slot=1 card=imm24-sfp++8-sfp28+2-qsfp28 mda/1=m24-sfp++8-sfp28+2-qsfp28\" # an integrated custom type definition # note, no `cp:` marker is needed type : \"slot=A chassis=ixr-r6 card=cpiom-ixr-r6 mda/1=m6-10g-sfp++4-25g-sfp28\" Node configuration # vr-sros nodes come up with a basic \"blank\" configuration where only the card/mda are provisioned, as well as the management interfaces such as Netconf, SNMP, gNMI. User defined config # It is possible to make SR OS nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container and used as a startup config: name : sros_lab topology : nodes : sros : kind : vr-sros config : myconfig.txt With such topology file containerlab is instructed to take a file myconfig.txt from the current working directory, copy it to the lab directory for that specific node under the /tftpboot/config.txt name and mount that dir to the container. This will result in this config to act as a startup config for the node. License # Path to a valid license must be provided for all vr-sros nodes with a license directive. File mounts # When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For vr-sros kind containerlab creates tftpboot directory where the license file will be copied. Lab examples # The following labs feature vr-sros node: SR Linux and vr-sros","title":"vr-sros - Nokia SR OS"},{"location":"manual/kinds/vr-sros/#nokia-sr-os","text":"Nokia SR OS virtualized router is identified with vr-sros kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-sros nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.","title":"Nokia SR OS"},{"location":"manual/kinds/vr-sros/#managing-vr-sros-nodes","text":"Note Containers with SR OS inside will take ~3min to fully boot. You can monitor the progress with watch docker ps waiting till the status will change to healthy . Nokia SR OS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-sros container: docker exec -it <container-name/id> bash CLI via SSH to connect to the SR OS CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u admin -p admin \\ capabilities Info Default user credentials: admin:admin","title":"Managing vr-sros nodes"},{"location":"manual/kinds/vr-sros/#interfaces-mapping","text":"vr-sros container uses the following mapping for its interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of SR OS line card eth2+ - second and subsequent data interface data interfaces need to be defined consequently Note, that data interfaces need to be defined in the topology file in a consequent fashion. Meaning that no gaps are allowed between the interfaces definition: links : - endpoints : [ \"srl:e1-1\" , \"sros:eth1\" ] # this is not allowed by vr-sros kind. # if eth5 is needed, define eth2-4 as well - endpoints : [ \"srl:e1-5\" , \"sros:eth5\" ] When containerlab launches vr-sros node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-sros/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-sros/#variants","text":"Virtual SR OS simulator can be run in multiple HW variants as explained in the vSIM installation guide . vr-sros container images come with pre-packaged SR OS variants as defined in the upstream repo as well as support custom variant definition . The pre-packaged variants are identified by the variant name and come up with cards and mda already configured. Custom variants, on the other hand, give users the full flexibility in emulated hardware configuration, but cards and mdas would need to be configured manually. To make vr-sros to boot in one of the pacakged variants use its name like that: topology : nodes : sros : kind : vr-sros image : vrnetlab/vr-sros:20.10.R1 type : sr-1s # if type omitted, the default sr-1 variant will be used license : license-sros20.txt Custom variant can be defined as simple as that: # for distributed chassis CPM and IOM are indicated with markers cp: and lc: # notice the delimeter string `___` that MUST be present between CPM and IOM portions # max_nics value is provided in `lc` part. # mem is provided in GB # quote the string value type : \"cp: cpu=2 ram=4 chassis=ixr-e slot=A card=cpm-ixr-e ___ lc: cpu=2 ram=4 max_nics=34 chassis=ixr-e slot=1 card=imm24-sfp++8-sfp28+2-qsfp28 mda/1=m24-sfp++8-sfp28+2-qsfp28\" # an integrated custom type definition # note, no `cp:` marker is needed type : \"slot=A chassis=ixr-r6 card=cpiom-ixr-r6 mda/1=m6-10g-sfp++4-25g-sfp28\"","title":"Variants"},{"location":"manual/kinds/vr-sros/#node-configuration","text":"vr-sros nodes come up with a basic \"blank\" configuration where only the card/mda are provisioned, as well as the management interfaces such as Netconf, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-sros/#user-defined-config","text":"It is possible to make SR OS nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container and used as a startup config: name : sros_lab topology : nodes : sros : kind : vr-sros config : myconfig.txt With such topology file containerlab is instructed to take a file myconfig.txt from the current working directory, copy it to the lab directory for that specific node under the /tftpboot/config.txt name and mount that dir to the container. This will result in this config to act as a startup config for the node.","title":"User defined config"},{"location":"manual/kinds/vr-sros/#license","text":"Path to a valid license must be provided for all vr-sros nodes with a license directive.","title":"License"},{"location":"manual/kinds/vr-sros/#file-mounts","text":"When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For vr-sros kind containerlab creates tftpboot directory where the license file will be copied.","title":"File mounts"},{"location":"manual/kinds/vr-sros/#lab-examples","text":"The following labs feature vr-sros node: SR Linux and vr-sros","title":"Lab examples"},{"location":"manual/kinds/vr-vmx/","text":"Juniper vMX # Juniper vMX virtualized router is identified with vr-vmx kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-vmx nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled. Managing vr-vmx nodes # Note Containers with vMX inside will take ~7min to fully boot. You can monitor the progress with docker logs -f <container-name> . Juniper vMX node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-vmx container: docker exec -it <container-name/id> bash CLI via SSH to connect to the vMX CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh admin@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u admin -p admin@123 \\ capabilities Info Default user credentials: admin:admin@123 Interfaces mapping # vr-vmx container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of vMX line card eth2+ - second and subsequent data interface When containerlab launches vr-vmx node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Node configuration # vr-vmx nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the admin users and management interfaces such as NETCONF, SNMP, gNMI. Lab examples # The following labs feature vr-vmx node: SR Linux and Juniper vMX Known issues and limitations # when listing docker containers, vr-vmx container will always report unhealthy status. Do not rely on this status. LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab. vMX requires Linux kernel 4.17+ To check the boot log, use docker logs -f <node-name> .","title":"vr-vmx - Juniper vMX"},{"location":"manual/kinds/vr-vmx/#juniper-vmx","text":"Juniper vMX virtualized router is identified with vr-vmx kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-vmx nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.","title":"Juniper vMX"},{"location":"manual/kinds/vr-vmx/#managing-vr-vmx-nodes","text":"Note Containers with vMX inside will take ~7min to fully boot. You can monitor the progress with docker logs -f <container-name> . Juniper vMX node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-vmx container: docker exec -it <container-name/id> bash CLI via SSH to connect to the vMX CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh admin@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u admin -p admin@123 \\ capabilities Info Default user credentials: admin:admin@123","title":"Managing vr-vmx nodes"},{"location":"manual/kinds/vr-vmx/#interfaces-mapping","text":"vr-vmx container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of vMX line card eth2+ - second and subsequent data interface When containerlab launches vr-vmx node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-vmx/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-vmx/#node-configuration","text":"vr-vmx nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the admin users and management interfaces such as NETCONF, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-vmx/#lab-examples","text":"The following labs feature vr-vmx node: SR Linux and Juniper vMX","title":"Lab examples"},{"location":"manual/kinds/vr-vmx/#known-issues-and-limitations","text":"when listing docker containers, vr-vmx container will always report unhealthy status. Do not rely on this status. LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab. vMX requires Linux kernel 4.17+ To check the boot log, use docker logs -f <node-name> .","title":"Known issues and limitations"},{"location":"manual/kinds/vr-xrv/","text":"Cisco XRv # Cisco XRv virtualized router is identified with vr-xrv kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-xrv nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled. Warning XRv image is discontinued by Cisco and supreceded by XRv 9000 image. It was added to containerlab because the image is lightweight, compared to XRv9k. If recent features are needed, use vr-xrv9k kind. Managing vr-xrv nodes # Note Containers with XRv inside will take ~5min to fully boot. You can monitor the progress with docker logs -f <container-name> . Cisco XRv node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-xrv container: docker exec -it <container-name/id> bash CLI via SSH to connect to the XRv CLI ssh clab@<container-name/id> NETCONF NETCONF server is running over port 830 ssh clab@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u clab -p clab@123 \\ capabilities Info Default user credentials: clab:clab@123 Interfaces mapping # vr-xrv container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of XRv line card eth2+ - second and subsequent data interface When containerlab launches vr-xrv node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Node configuration # vr-xrv nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the clab user and management interfaces such as NETCONF, SNMP, gNMI. Lab examples # The following labs feature vr-xrv node: SR Linux and Cisco XRv Known issues and limitations # LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.","title":"vr-xrv - Cisco XRv"},{"location":"manual/kinds/vr-xrv/#cisco-xrv","text":"Cisco XRv virtualized router is identified with vr-xrv kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-xrv nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled. Warning XRv image is discontinued by Cisco and supreceded by XRv 9000 image. It was added to containerlab because the image is lightweight, compared to XRv9k. If recent features are needed, use vr-xrv9k kind.","title":"Cisco XRv"},{"location":"manual/kinds/vr-xrv/#managing-vr-xrv-nodes","text":"Note Containers with XRv inside will take ~5min to fully boot. You can monitor the progress with docker logs -f <container-name> . Cisco XRv node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-xrv container: docker exec -it <container-name/id> bash CLI via SSH to connect to the XRv CLI ssh clab@<container-name/id> NETCONF NETCONF server is running over port 830 ssh clab@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u clab -p clab@123 \\ capabilities Info Default user credentials: clab:clab@123","title":"Managing vr-xrv nodes"},{"location":"manual/kinds/vr-xrv/#interfaces-mapping","text":"vr-xrv container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of XRv line card eth2+ - second and subsequent data interface When containerlab launches vr-xrv node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-xrv/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-xrv/#node-configuration","text":"vr-xrv nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the clab user and management interfaces such as NETCONF, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-xrv/#lab-examples","text":"The following labs feature vr-xrv node: SR Linux and Cisco XRv","title":"Lab examples"},{"location":"manual/kinds/vr-xrv/#known-issues-and-limitations","text":"LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.","title":"Known issues and limitations"},{"location":"manual/kinds/vr-xrv9k/","text":"Cisco XRv9k # Cisco XRv9k virtualized router is identified with vr-xrv9k kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-xrv9k nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled. Warning XRv9k node is a resource hungry image. As of XRv9k 7.2.1 version the minimum resources should be set to 2vcpu/14GB. These are the default setting set with containerlab for this kind. Image will take 25 minutes to fully boot, be patient. You can monitor the loading status with docker logs -f <container-name> . Managing vr-xrv9k nodes # Cisco XRv9k node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-xrv9k container: docker exec -it <container-name/id> bash CLI via SSH to connect to the XRv9kCLI ssh clab@<container-name/id> NETCONF NETCONF server is running over port 830 ssh clab@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u clab -p clab@123 \\ capabilities Info Default user credentials: clab:clab@123 Interfaces mapping # vr-xrv9k container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of XRv9k line card eth2+ - second and subsequent data interface When containerlab launches vr-xrv9k node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Node configuration # vr-xrv9k nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the clab user and management interfaces such as NETCONF, SNMP, gNMI. Lab examples # The following labs feature vr-xrv9k node: SR Linux and Cisco XRv9k Known issues and limitations # LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.","title":"vr-xrv9k - Cisco XRv9k"},{"location":"manual/kinds/vr-xrv9k/#cisco-xrv9k","text":"Cisco XRv9k virtualized router is identified with vr-xrv9k kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-xrv9k nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled. Warning XRv9k node is a resource hungry image. As of XRv9k 7.2.1 version the minimum resources should be set to 2vcpu/14GB. These are the default setting set with containerlab for this kind. Image will take 25 minutes to fully boot, be patient. You can monitor the loading status with docker logs -f <container-name> .","title":"Cisco XRv9k"},{"location":"manual/kinds/vr-xrv9k/#managing-vr-xrv9k-nodes","text":"Cisco XRv9k node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-xrv9k container: docker exec -it <container-name/id> bash CLI via SSH to connect to the XRv9kCLI ssh clab@<container-name/id> NETCONF NETCONF server is running over port 830 ssh clab@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u clab -p clab@123 \\ capabilities Info Default user credentials: clab:clab@123","title":"Managing vr-xrv9k nodes"},{"location":"manual/kinds/vr-xrv9k/#interfaces-mapping","text":"vr-xrv9k container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of XRv9k line card eth2+ - second and subsequent data interface When containerlab launches vr-xrv9k node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-xrv9k/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-xrv9k/#node-configuration","text":"vr-xrv9k nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the clab user and management interfaces such as NETCONF, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-xrv9k/#lab-examples","text":"The following labs feature vr-xrv9k node: SR Linux and Cisco XRv9k","title":"Lab examples"},{"location":"manual/kinds/vr-xrv9k/#known-issues-and-limitations","text":"LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.","title":"Known issues and limitations"}]}