{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"With the growing number of containerized Network Operating Systems grows the demand to easily run them in the user-defined, versatile lab topologies. Unfortunately, container orchestration tools like docker-compose are not a good fit for that purpose, as they do not allow a user to easily create connections between the containers which define a topology. Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them to create lab topologies of users choice and manages labs lifecycle. Containerlab focuses on the containerized Network Operating Systems which are typically used to test network features and designs, such as: Nokia SR-Linux Arista cEOS Azure SONiC Juniper cRPD In addition to native containerized NOSes, containerlab can launch traditional virtual-machine based routers using vrnetlab integration : Nokia virtual SR OS (vSim/VSR) Juniper vMX Cisco IOS XRv9k Arista vEOS And, of course, containerlab is perfectly capable of wiring up arbitrary linux containers which can host your network applications, virtual functions or simply be a test client. With all that, containerlab provides a single IaaC interface to manage labs which can span all the needed variants of nodes: This short clip briefly demonstrates containerlab features and explains its purpose: Features # IaaC approach Declarative way of defining the labs by means of the topology definition clab files . Network Operating Systems centric Focus on containerized Network Operating Systems. The sophisticated startup requirements of various NOS containers are abstracted with kinds which allows the user to focus on the use cases, rather than infrastructure hurdles. VM based nodes friendly With the vrnetlab integration it is possible to get the best of two worlds - running virtualized and containerized nodes alike with the same IaaC approach and workflows. Multi-vendor and open Although being kick-started by Nokia engineers, containerlab doesn't take sides and supports NOSes from other vendors and opensource projects. Lab orchestration Starting the containers and interconnecting them alone is already good, but containerlab packages even more features like managing lab lifecycle: deploy , destroy , save , inspect , graph operations. Scaled labs generator With generate capabilities of containerlab it possible to define/launch CLOS-based topologies of arbitrary scale. Just say how many tiers you need and how big each tier is, the rest will be done in a split second. Simplicity and convenience Starting from frictionless installation and upgrade capabilities and ranging to the behind-the-scenes link wiring machinery , containerlab does its best for you to enjoy the tool. Fast Blazing fast way to create container based labs on any Linux system with Docker. Automated TLS certificates provisioning The nodes which require TLS certs will get them automatically on boot. Documentation is a first-class citizen We do not let our users guess by making a complete, concise and clean documentation . Lab catalog The \"most-wanted\" lab topologies are documented and included with containerlab installation. Based on this cherry-picked selection you can start crafting the labs answering your needs. Use cases # Labs and demos Containerlab was meant to be a tool for provisioning networking labs built with containers. It is free, open and ubiquitous. No software apart from Docker is required! As with any lab environment it allows the users to validate features, topologies, perform interop testing, datapath testing, etc. It is also a perfect companion for your next demo. Deploy the lab fast, with all its configuration stored as a code -> destroy when done. Easily and securely share lab access if needed. Testing and CI Because of the containerlab's single-binary packaging and code-based lab definition files, it was never that easy to spin up a test bed for CI. Gitlab CI, Github Actions and virtually any CI system will be able to spin up containerlab topologies in a single simple command. Telemetry validation Coupling modern telemetry stacks with containerlab labs make a perfect fit for Telemetry use cases validation. Spin up a lab with containerized network functions with a telemetry on the side, and run comprehensive telemetry use cases.","title":"Home"},{"location":"#features","text":"IaaC approach Declarative way of defining the labs by means of the topology definition clab files . Network Operating Systems centric Focus on containerized Network Operating Systems. The sophisticated startup requirements of various NOS containers are abstracted with kinds which allows the user to focus on the use cases, rather than infrastructure hurdles. VM based nodes friendly With the vrnetlab integration it is possible to get the best of two worlds - running virtualized and containerized nodes alike with the same IaaC approach and workflows. Multi-vendor and open Although being kick-started by Nokia engineers, containerlab doesn't take sides and supports NOSes from other vendors and opensource projects. Lab orchestration Starting the containers and interconnecting them alone is already good, but containerlab packages even more features like managing lab lifecycle: deploy , destroy , save , inspect , graph operations. Scaled labs generator With generate capabilities of containerlab it possible to define/launch CLOS-based topologies of arbitrary scale. Just say how many tiers you need and how big each tier is, the rest will be done in a split second. Simplicity and convenience Starting from frictionless installation and upgrade capabilities and ranging to the behind-the-scenes link wiring machinery , containerlab does its best for you to enjoy the tool. Fast Blazing fast way to create container based labs on any Linux system with Docker. Automated TLS certificates provisioning The nodes which require TLS certs will get them automatically on boot. Documentation is a first-class citizen We do not let our users guess by making a complete, concise and clean documentation . Lab catalog The \"most-wanted\" lab topologies are documented and included with containerlab installation. Based on this cherry-picked selection you can start crafting the labs answering your needs.","title":"Features"},{"location":"#use-cases","text":"Labs and demos Containerlab was meant to be a tool for provisioning networking labs built with containers. It is free, open and ubiquitous. No software apart from Docker is required! As with any lab environment it allows the users to validate features, topologies, perform interop testing, datapath testing, etc. It is also a perfect companion for your next demo. Deploy the lab fast, with all its configuration stored as a code -> destroy when done. Easily and securely share lab access if needed. Testing and CI Because of the containerlab's single-binary packaging and code-based lab definition files, it was never that easy to spin up a test bed for CI. Gitlab CI, Github Actions and virtually any CI system will be able to spin up containerlab topologies in a single simple command. Telemetry validation Coupling modern telemetry stacks with containerlab labs make a perfect fit for Telemetry use cases validation. Spin up a lab with containerized network functions with a telemetry on the side, and run comprehensive telemetry use cases.","title":"Use cases"},{"location":"install/","text":"Containerlab is distributed as a Linux deb/rpm package and can be installed on any Debian- or RHEL-like distributive in a matter of a few seconds. Pre-requisites # The following requirements must be satisfied in order to let containerlab tool run successfully: A user should have sudo privileges to run containerlab. A Linux server/VM 2 and Docker installed. Load container images (e.g. Nokia SR Linux, Arista cEOS) which are not downloadable from a container registry. Containerlab will try to pull images at runtime if they do not exist locally. Install script # Containerlab can be installed using the installation script which detects the operating system type and installs the relevant package: Note Containerlab is distributed via deb/rpm packages, thus only Debian- and RHEL-like distributives can leverage package installation. Other systems can follow the manual installation procedure. # download and install the latest release (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" # download a specific version - 0.10.3 (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" -- -v 0 .10.3 # with wget bash -c \" $( wget -qO - https://get-clab.srlinux.dev ) \" Package managers # It is possible to install official containerlab releases via public APT/YUM repository. APT echo \"deb [trusted=yes] https://apt.fury.io/netdevops/ /\" | \\ sudo tee -a /etc/apt/sources.list.d/netdevops.list apt update && apt install containerlab YUM yum-config-manager --add-repo=https://yum.fury.io/netdevops/ && \\ echo \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/yum.fury.io_netdevops_.repo yum install containerlab Manual package installation Alternatively, users can manually download the deb/rpm package from the Github releases page. example: # manually install latest release with package managers LATEST = $( curl -s https://github.com/srl-labs/containerlab/releases/latest | sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/' ) # with yum yum install \"https://github.com/srl-labs/containerlab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _linux_amd64.rpm\" # with dpkg curl -sL -o /tmp/clab.deb \"https://github.com/srl-labs/containerlab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _linux_amd64.deb\" && dpkg -i /tmp/clab.deb # install specific release with yum yum install https://github.com/srl-labs/containerlab/releases/download/v0.7.0/containerlab_0.7.0_linux_386.rpm The package installer will put the containerlab binary in the /usr/bin directory as well as create the /usr/bin/clab -> /usr/bin/containerlab symlink. The symlink allows the users to save on typing when they use containerlab: clab <command> . Manual installation # If the linux distributive can't install deb/rpm packages, containerlab can be installed from the archive: # get the latest available tag LATEST = $( curl -s https://github.com/srl-labs/containerlab/releases/latest | \\ sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/' ) # download tar.gz archive curl -L -o /tmp/clab.tar.gz \"https://github.com/srl-labs/containerlab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _Linux_amd64.tar.gz\" # create containerlab directory mkdir -p /etc/containerlab # extract downloaded archive into the containerlab directory tar -zxvf /tmp/clab.tar.gz -C /etc/containerlab # (optional) move containerlab binary somewhere in the $PATH mv /etc/containerlab/containerlab /usr/bin && chmod a+x /usr/bin/containerlab Upgrade # To upgrade containerlab to the latest available version issue the following command 1 : containerlab version upgrade This command will fetch the installation script and will upgrade the tool to its most recent version. or leverage apt / yum utilities if containerlab repo was added as explained in the Package managers section. only available if installed from packages \u21a9 Most containerized NOS will require >1 vCPU. RAM size depends on the lab size. Architecture: AMD64. \u21a9","title":"Installation"},{"location":"install/#pre-requisites","text":"The following requirements must be satisfied in order to let containerlab tool run successfully: A user should have sudo privileges to run containerlab. A Linux server/VM 2 and Docker installed. Load container images (e.g. Nokia SR Linux, Arista cEOS) which are not downloadable from a container registry. Containerlab will try to pull images at runtime if they do not exist locally.","title":"Pre-requisites"},{"location":"install/#install-script","text":"Containerlab can be installed using the installation script which detects the operating system type and installs the relevant package: Note Containerlab is distributed via deb/rpm packages, thus only Debian- and RHEL-like distributives can leverage package installation. Other systems can follow the manual installation procedure. # download and install the latest release (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" # download a specific version - 0.10.3 (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" -- -v 0 .10.3 # with wget bash -c \" $( wget -qO - https://get-clab.srlinux.dev ) \"","title":"Install script"},{"location":"install/#package-managers","text":"It is possible to install official containerlab releases via public APT/YUM repository. APT echo \"deb [trusted=yes] https://apt.fury.io/netdevops/ /\" | \\ sudo tee -a /etc/apt/sources.list.d/netdevops.list apt update && apt install containerlab YUM yum-config-manager --add-repo=https://yum.fury.io/netdevops/ && \\ echo \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/yum.fury.io_netdevops_.repo yum install containerlab Manual package installation Alternatively, users can manually download the deb/rpm package from the Github releases page. example: # manually install latest release with package managers LATEST = $( curl -s https://github.com/srl-labs/containerlab/releases/latest | sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/' ) # with yum yum install \"https://github.com/srl-labs/containerlab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _linux_amd64.rpm\" # with dpkg curl -sL -o /tmp/clab.deb \"https://github.com/srl-labs/containerlab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _linux_amd64.deb\" && dpkg -i /tmp/clab.deb # install specific release with yum yum install https://github.com/srl-labs/containerlab/releases/download/v0.7.0/containerlab_0.7.0_linux_386.rpm The package installer will put the containerlab binary in the /usr/bin directory as well as create the /usr/bin/clab -> /usr/bin/containerlab symlink. The symlink allows the users to save on typing when they use containerlab: clab <command> .","title":"Package managers"},{"location":"install/#manual-installation","text":"If the linux distributive can't install deb/rpm packages, containerlab can be installed from the archive: # get the latest available tag LATEST = $( curl -s https://github.com/srl-labs/containerlab/releases/latest | \\ sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/' ) # download tar.gz archive curl -L -o /tmp/clab.tar.gz \"https://github.com/srl-labs/containerlab/releases/download/v ${ LATEST } /containerlab_ ${ LATEST } _Linux_amd64.tar.gz\" # create containerlab directory mkdir -p /etc/containerlab # extract downloaded archive into the containerlab directory tar -zxvf /tmp/clab.tar.gz -C /etc/containerlab # (optional) move containerlab binary somewhere in the $PATH mv /etc/containerlab/containerlab /usr/bin && chmod a+x /usr/bin/containerlab","title":"Manual installation"},{"location":"install/#upgrade","text":"To upgrade containerlab to the latest available version issue the following command 1 : containerlab version upgrade This command will fetch the installation script and will upgrade the tool to its most recent version. or leverage apt / yum utilities if containerlab repo was added as explained in the Package managers section. only available if installed from packages \u21a9 Most containerized NOS will require >1 vCPU. RAM size depends on the lab size. Architecture: AMD64. \u21a9","title":"Upgrade"},{"location":"quickstart/","text":"Installation # Getting containerlab is as easy as it gets. Thanks to the trivial installation procedure it can be set up in a matter of a few seconds on any RHEL or Debian based OS 1 . # download and install the latest release (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \" Topology definition file # Once installed, containerlab manages the labs defined in the so-called topology definition, clab files . A user can write a topology definition file from scratch, or start with looking at various lab examples we provide within the containerlab package. In this quickstart we will be using one of the provided labs which consists of Nokia SR Linux and Arista cEOS nodes connected one to another. The lab topology is defined in the srlceos01.clab.yml file. To make use of this lab example, we first may want to copy the corresponding lab files to some directory: # create a directory for the lab mkdir ~/clab-quickstart cd ~/clab-quickstart # copy over the lab files cp -a /etc/containerlab/lab-examples/srlceos01/* . Let's have a look at how this lab's topology is defined: name : srlceos01 topology : nodes : srl : kind : srl image : srlinux:20.6.3-145 license : license.key ceos : kind : ceos image : ceos:4.25.0F links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] A topology definition deep-dive document provides a complete reference of the topology definition syntax. In this quickstart we keep it short, glancing over the key components of the file: Each lab has a name . The lab topology is defined under the topology element. Topology is a set of nodes and links between them. The nodes are always of a certain kind . The kind defines the node configuration and behavior. Containerlab supports a fixed number of kinds . In the example above, the srl and ceos are one of the supported kinds . The actual nodes of the topology are defined in the nodes section which holds a map of node names. In the example above, nodes with names srl and ceos are defined. Node elements must have a kind parameter to indicate which kind this node belongs to. Under the nodes section you typically provide node-specific parameters. This lab uses node-specific parameters such as image and license . nodes are interconnected with links . Each link is defined by a set of endpoints . Container image # One of node's most important properties is the container image they use. In our example the nodes use a specific image which we imported upfront 2 . The image name follows the same rules as the images you use with, for example, Docker client. Container images versions Some lab examples use the images without a tag, i.e. image: srlinux . This means that the image with a latest tag must exist. A user needs to tag the image if the latest tag is missing. For example: docker tag srlinux:20.6.1-286 srlinux:latest License files # For the nodes/kinds which require a license to run (like Nokia SR Linux) the license element must specify a path to a valid license file. In the example we work with, the license path is set to license.key for srl kind. That means that containerlab will look for this file by the ${PWD}/license.key path. It is also possible to provide absolute paths as well. Before deploying our lab, we have to copy the license file in the ~/clab-quickstart directory to make it available by the specified relative path. Deploying a lab # Now when we know what a basic topology file consists of, sorted out the container image name and node's license file, we can proceed with deploying this lab. To keep things easy and guessable, the command to deploy a lab is called deploy . # checking that topology and license files are present in ~/clab-quickstart \u276f ls license.key srlceos01.clab.yml # checking that container images are available docker images | grep -E \"srlinux|ceos\" REPOSITORY TAG IMAGE ID CREATED SIZE srlinux 20 .6.3-145 79019d14cfc7 3 months ago 1 .32GB ceos 4 .25.0F 15a5f97fe8e8 3 months ago 1 .76GB # start the lab deployment by referencing the topology file containerlab deploy --topo srlceos01.clab.yml After a couple of seconds you will see the summary of the deployed nodes: +---+---------------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+---------------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlceos01-ceos | 2e2e04a42cea | ceos:4.25.0F | ceos | | running | 172.20.20.3/24 | 2001:172:20:20::3/80 | | 2 | clab-srlceos01-srl | 1b9568fcdb01 | srlinux:20.6.3-145 | srl | | running | 172.20.20.4/24 | 2001:172:20:20::4/80 | +---+---------------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ The node name presented in the summary table is the fully qualified node name, it is built using the following pattern: clab-{{lab-name}}-{{node-name}} . Connecting to the nodes # Since the topology nodes are regular containers, you can connect to them just like to any other container. Info For each supported kind we document the management interfaces and the ways to leverage them. For example, srl kind documentation provides the commands to leverage SSH and gNMI interfaces. ceos kind has its own instructions . With containerized network OSes like Nokia SR Linux or Arista cEOS SSH access can be achieved by either using the management address assigned to the container: \u276f ssh admin@172.20.20.3 admin@172.20.20.3's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:srl1# or by using node's fully qualified names, for which containerlab creates /etc/hosts entries: ssh admin@clab-srlceos01-srl The following tab view aggregates the ways to get CLI access per the lab node: Nokia SR Linux # access CLI docker exec -it <name> sr_cli # access bash docker exec -it <name> bash Arista cEOS # access CLI docker exec -it <name> Cli # access bash docker exec -it <name> bash Destroying a lab # To remove the lab, use the destroy command that takes a topology file as an argument: containerlab destroy --topo srlceos01.clab.yml What next? # To get a broader view on the containerlab features and components, refer to the User manual section. Do not forget to check out the Lab examples section where we provide complete and ready-to-run topology definition files. This is a great starting point to explore containerlab by doing. For other installation options such as package managers, manual binary downloads or instructions to get containerlab for non-RHEL/Debian distros, refer to the installation guide . \u21a9 Containerlab would try to pull the images upon the lab deployment, but if the images are not available publicly or you do not have the private repositories configured, then you need to import the images upfront. \u21a9","title":"Quick start"},{"location":"quickstart/#installation","text":"Getting containerlab is as easy as it gets. Thanks to the trivial installation procedure it can be set up in a matter of a few seconds on any RHEL or Debian based OS 1 . # download and install the latest release (may require sudo) bash -c \" $( curl -sL https://get-clab.srlinux.dev ) \"","title":"Installation"},{"location":"quickstart/#topology-definition-file","text":"Once installed, containerlab manages the labs defined in the so-called topology definition, clab files . A user can write a topology definition file from scratch, or start with looking at various lab examples we provide within the containerlab package. In this quickstart we will be using one of the provided labs which consists of Nokia SR Linux and Arista cEOS nodes connected one to another. The lab topology is defined in the srlceos01.clab.yml file. To make use of this lab example, we first may want to copy the corresponding lab files to some directory: # create a directory for the lab mkdir ~/clab-quickstart cd ~/clab-quickstart # copy over the lab files cp -a /etc/containerlab/lab-examples/srlceos01/* . Let's have a look at how this lab's topology is defined: name : srlceos01 topology : nodes : srl : kind : srl image : srlinux:20.6.3-145 license : license.key ceos : kind : ceos image : ceos:4.25.0F links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] A topology definition deep-dive document provides a complete reference of the topology definition syntax. In this quickstart we keep it short, glancing over the key components of the file: Each lab has a name . The lab topology is defined under the topology element. Topology is a set of nodes and links between them. The nodes are always of a certain kind . The kind defines the node configuration and behavior. Containerlab supports a fixed number of kinds . In the example above, the srl and ceos are one of the supported kinds . The actual nodes of the topology are defined in the nodes section which holds a map of node names. In the example above, nodes with names srl and ceos are defined. Node elements must have a kind parameter to indicate which kind this node belongs to. Under the nodes section you typically provide node-specific parameters. This lab uses node-specific parameters such as image and license . nodes are interconnected with links . Each link is defined by a set of endpoints .","title":"Topology definition file"},{"location":"quickstart/#container-image","text":"One of node's most important properties is the container image they use. In our example the nodes use a specific image which we imported upfront 2 . The image name follows the same rules as the images you use with, for example, Docker client. Container images versions Some lab examples use the images without a tag, i.e. image: srlinux . This means that the image with a latest tag must exist. A user needs to tag the image if the latest tag is missing. For example: docker tag srlinux:20.6.1-286 srlinux:latest","title":"Container image"},{"location":"quickstart/#license-files","text":"For the nodes/kinds which require a license to run (like Nokia SR Linux) the license element must specify a path to a valid license file. In the example we work with, the license path is set to license.key for srl kind. That means that containerlab will look for this file by the ${PWD}/license.key path. It is also possible to provide absolute paths as well. Before deploying our lab, we have to copy the license file in the ~/clab-quickstart directory to make it available by the specified relative path.","title":"License files"},{"location":"quickstart/#deploying-a-lab","text":"Now when we know what a basic topology file consists of, sorted out the container image name and node's license file, we can proceed with deploying this lab. To keep things easy and guessable, the command to deploy a lab is called deploy . # checking that topology and license files are present in ~/clab-quickstart \u276f ls license.key srlceos01.clab.yml # checking that container images are available docker images | grep -E \"srlinux|ceos\" REPOSITORY TAG IMAGE ID CREATED SIZE srlinux 20 .6.3-145 79019d14cfc7 3 months ago 1 .32GB ceos 4 .25.0F 15a5f97fe8e8 3 months ago 1 .76GB # start the lab deployment by referencing the topology file containerlab deploy --topo srlceos01.clab.yml After a couple of seconds you will see the summary of the deployed nodes: +---+---------------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+---------------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlceos01-ceos | 2e2e04a42cea | ceos:4.25.0F | ceos | | running | 172.20.20.3/24 | 2001:172:20:20::3/80 | | 2 | clab-srlceos01-srl | 1b9568fcdb01 | srlinux:20.6.3-145 | srl | | running | 172.20.20.4/24 | 2001:172:20:20::4/80 | +---+---------------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ The node name presented in the summary table is the fully qualified node name, it is built using the following pattern: clab-{{lab-name}}-{{node-name}} .","title":"Deploying a lab"},{"location":"quickstart/#connecting-to-the-nodes","text":"Since the topology nodes are regular containers, you can connect to them just like to any other container. Info For each supported kind we document the management interfaces and the ways to leverage them. For example, srl kind documentation provides the commands to leverage SSH and gNMI interfaces. ceos kind has its own instructions . With containerized network OSes like Nokia SR Linux or Arista cEOS SSH access can be achieved by either using the management address assigned to the container: \u276f ssh admin@172.20.20.3 admin@172.20.20.3's password: Using configuration file(s): [] Welcome to the srlinux CLI. Type 'help' (and press <ENTER>) if you need any help using this. --{ running }--[ ]-- A:srl1# or by using node's fully qualified names, for which containerlab creates /etc/hosts entries: ssh admin@clab-srlceos01-srl The following tab view aggregates the ways to get CLI access per the lab node: Nokia SR Linux # access CLI docker exec -it <name> sr_cli # access bash docker exec -it <name> bash Arista cEOS # access CLI docker exec -it <name> Cli # access bash docker exec -it <name> bash","title":"Connecting to the nodes"},{"location":"quickstart/#destroying-a-lab","text":"To remove the lab, use the destroy command that takes a topology file as an argument: containerlab destroy --topo srlceos01.clab.yml","title":"Destroying a lab"},{"location":"quickstart/#what-next","text":"To get a broader view on the containerlab features and components, refer to the User manual section. Do not forget to check out the Lab examples section where we provide complete and ready-to-run topology definition files. This is a great starting point to explore containerlab by doing. For other installation options such as package managers, manual binary downloads or instructions to get containerlab for non-RHEL/Debian distros, refer to the installation guide . \u21a9 Containerlab would try to pull the images upon the lab deployment, but if the images are not available publicly or you do not have the private repositories configured, then you need to import the images upfront. \u21a9","title":"What next?"},{"location":"cmd/completion/","text":"shell completions # Description # The completion command generates shell completions for bash/zsh/fish shells. Usage # containerlab completion [arg] Bash completions # source < ( containerlab completion bash ) To load completions for each session, execute once: # linux $ containerlab completion bash > /etc/bash_completion.d/containerlab # macOS $ containerlab completion bash > /usr/local/etc/bash_completion.d/containerlab ZSH completions # If shell completion is not already enabled in your environment, users will need to enable it. To do so, execute the following once: echo \"autoload -U compinit; compinit\" >> ~/.zshrc To load completions for each session, execute once: containerlab completion zsh > \" ${ fpath [1] } /_containerlab\" Start a new shell for this setup to take effect. Fish completions # containerlab completion fish | source To load completions for each session, execute once: containerlab completion fish > ~/.config/fish/completions/containerlab.fish","title":"completions"},{"location":"cmd/completion/#shell-completions","text":"","title":"shell completions"},{"location":"cmd/completion/#description","text":"The completion command generates shell completions for bash/zsh/fish shells.","title":"Description"},{"location":"cmd/completion/#usage","text":"containerlab completion [arg]","title":"Usage"},{"location":"cmd/completion/#bash-completions","text":"source < ( containerlab completion bash ) To load completions for each session, execute once: # linux $ containerlab completion bash > /etc/bash_completion.d/containerlab # macOS $ containerlab completion bash > /usr/local/etc/bash_completion.d/containerlab","title":"Bash completions"},{"location":"cmd/completion/#zsh-completions","text":"If shell completion is not already enabled in your environment, users will need to enable it. To do so, execute the following once: echo \"autoload -U compinit; compinit\" >> ~/.zshrc To load completions for each session, execute once: containerlab completion zsh > \" ${ fpath [1] } /_containerlab\" Start a new shell for this setup to take effect.","title":"ZSH completions"},{"location":"cmd/completion/#fish-completions","text":"containerlab completion fish | source To load completions for each session, execute once: containerlab completion fish > ~/.config/fish/completions/containerlab.fish","title":"Fish completions"},{"location":"cmd/deploy/","text":"deploy command # Description # The deploy command spins up a lab using the topology expressed via topology definition file . Usage # containerlab [global-flags] deploy [local-flags] aliases: dep Flags # topology # With the global --topo | -t flag a user sets the path to the topology definition file that will be used to spin up a lab. name # With the global --name | -n flag a user sets a lab name. This value will override the lab name value passed in the topology definition file. reconfigure # The local --reconfigure flag instructs containerlab to first destroy the lab and all its directories and then start the deployment process. That will result in a clean (re)deployment where every configuration artefact will be generated (TLS, node config) from scratch. Without this flag present, containerlab will reuse the available configuration artifacts found in the lab directory. Refer to the configuration artifacts page to get more information on the lab directory contents. max-workers # With --max-workers flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create. Examples # # deploy a lab from mylab.clab.yml file located in the same dir containerlab deploy -t mylab.clab.yml # deploy a lab from mylab.clab.yml file and regenerate all configuration artifacts containerlab deploy -t mylab.clab.yml --reconfigure","title":"deploy"},{"location":"cmd/deploy/#deploy-command","text":"","title":"deploy command"},{"location":"cmd/deploy/#description","text":"The deploy command spins up a lab using the topology expressed via topology definition file .","title":"Description"},{"location":"cmd/deploy/#usage","text":"containerlab [global-flags] deploy [local-flags] aliases: dep","title":"Usage"},{"location":"cmd/deploy/#flags","text":"","title":"Flags"},{"location":"cmd/deploy/#topology","text":"With the global --topo | -t flag a user sets the path to the topology definition file that will be used to spin up a lab.","title":"topology"},{"location":"cmd/deploy/#name","text":"With the global --name | -n flag a user sets a lab name. This value will override the lab name value passed in the topology definition file.","title":"name"},{"location":"cmd/deploy/#reconfigure","text":"The local --reconfigure flag instructs containerlab to first destroy the lab and all its directories and then start the deployment process. That will result in a clean (re)deployment where every configuration artefact will be generated (TLS, node config) from scratch. Without this flag present, containerlab will reuse the available configuration artifacts found in the lab directory. Refer to the configuration artifacts page to get more information on the lab directory contents.","title":"reconfigure"},{"location":"cmd/deploy/#max-workers","text":"With --max-workers flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create.","title":"max-workers"},{"location":"cmd/deploy/#examples","text":"# deploy a lab from mylab.clab.yml file located in the same dir containerlab deploy -t mylab.clab.yml # deploy a lab from mylab.clab.yml file and regenerate all configuration artifacts containerlab deploy -t mylab.clab.yml --reconfigure","title":"Examples"},{"location":"cmd/destroy/","text":"destroy command # Description # The destroy command destroys a lab referenced by its topology definition file . Usage # containerlab [global-flags] destroy [local-flags] aliases: des Flags # topology # With the global --topo | -t flag a user sets the path to the topology definition file that will be used get the elements of a lab that will be destroyed. cleanup # The local --cleanup flag instructs containerlab to remove the lab directory and all its content. Without this flag present, containerlab will keep the lab directory and all files inside of it. Refer to the configuration artifacts page to get more information on the lab directory contents. graceful # To make containerlab attempt a graceful shutdown of the running containers, add the --graceful flag to destroy cmd. Without it, containers will be removed forcefully without even attempting to stop them. all # Destroy command provided with --all | -a flag will perform the deletion of all the labs running on the container host. It will not touch containers launched manually. Examples # # destroy a lab based on mylab.clab.yml topology file located in the same dir containerlab destroy -t mylab.clab.yml # destroy a lab and also remove the Lab Directory containerlab destroy -t mylab.clab.yml --cleanup # destroy all labs deployed with containerlab # using shortcut names clab des -a","title":"destroy"},{"location":"cmd/destroy/#destroy-command","text":"","title":"destroy command"},{"location":"cmd/destroy/#description","text":"The destroy command destroys a lab referenced by its topology definition file .","title":"Description"},{"location":"cmd/destroy/#usage","text":"containerlab [global-flags] destroy [local-flags] aliases: des","title":"Usage"},{"location":"cmd/destroy/#flags","text":"","title":"Flags"},{"location":"cmd/destroy/#topology","text":"With the global --topo | -t flag a user sets the path to the topology definition file that will be used get the elements of a lab that will be destroyed.","title":"topology"},{"location":"cmd/destroy/#cleanup","text":"The local --cleanup flag instructs containerlab to remove the lab directory and all its content. Without this flag present, containerlab will keep the lab directory and all files inside of it. Refer to the configuration artifacts page to get more information on the lab directory contents.","title":"cleanup"},{"location":"cmd/destroy/#graceful","text":"To make containerlab attempt a graceful shutdown of the running containers, add the --graceful flag to destroy cmd. Without it, containers will be removed forcefully without even attempting to stop them.","title":"graceful"},{"location":"cmd/destroy/#all","text":"Destroy command provided with --all | -a flag will perform the deletion of all the labs running on the container host. It will not touch containers launched manually.","title":"all"},{"location":"cmd/destroy/#examples","text":"# destroy a lab based on mylab.clab.yml topology file located in the same dir containerlab destroy -t mylab.clab.yml # destroy a lab and also remove the Lab Directory containerlab destroy -t mylab.clab.yml --cleanup # destroy all labs deployed with containerlab # using shortcut names clab des -a","title":"Examples"},{"location":"cmd/generate/","text":"generate command # Description # The generate command generates the topology definition file based on the user input provided via CLI flags. With this command it is possible to generate definition file for a CLOS fabric by just providing the number of nodes on each tier. The generated topology can be saved in a file or immediately scheduled for deployment. It is assumed, that the interconnection between the tiers is done in a full-mesh fashion. Such as tier1 nodes are fully meshed with tier2, tier2 is meshed with tier3 and so on. Usage # containerlab [global-flags] generate [local-flags] aliases: gen Flags # name # With the global --name | -n flag a user sets the name of the lab that will be generated. nodes # The user configures the CLOS fabric topology by using the --nodes flag. The flag value is a comma separated list of CLOS tiers where each tier is defined by the number of nodes, its kind and type. Multiple --node flags can be specified. For example, the following flag value will define a 2-tier CLOS fabric with tier1 (leafs) consists of 4x SR Linux containers of IXR6 type and the 2x Arista cEOS spines: 4:srl:ixr6,2:ceos Note, that the default kind is srl , so you can omit the kind for SR Linux node. The same nodes value can be expressed like that: 4:ixr6,2:ceos kind # With --kind flag it is possible to set the default kind that will be set for the nodes which do not have a kind specified in the --nodes flag. For example the following value will generate a 3-tier CLOS fabric of cEOS nodes: # cEOS fabric containerlab gen -n 3tier --kind ceos --nodes 4 ,2,1 # since SR Linux kind is assumed by default # SRL fabric command is even shorter containerlab gen -n 3tier --nodes 4 ,2,1 image # Use --image flag to specify the container image that should be used by a given kind. The value of this flag follows the kind=image pattern. For example, to set the container image ceos:4.21.F for the ceos kind the flag will be: --image ceos=ceos:4.21.F . To set images for multiple kinds repeat the flag: --image srl=srlinux:latest --image ceos=ceos:4.21.F or use the comma separated form: --image srl=srlinux:latest,ceos=ceos:latest If the kind information is not provided in the image flag, the kind value will be taken from the --kind flag. license # With --license flag it is possible to set the license path that should be used by a given kind. The value of this flag follows the kind=path pattern. For example, to set the license path for the srl kind: --license srl=/tmp/license.key . To set license for multiple kinds repeat the flag: --license <kind1>=/path1 --image <kind2>=/path2 or use the comma separated form: --license <kind1>=/path1,<kind2>=/path2 deploy # When --deploy flag is present, the lab deployment process starts using the generated topology definition file. The generated definition file is first saved by the path set with --file or, if file path is not set, by the default path of <lab-name>.clab.yml . Then the equivalent of the deploy -t <file> --reconfigure command is executed. max-workers # With --max-workers flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create. If during the deployment of a large scaled lab you see errors about max number of opened files reached, limit the max workers with this flag. file # With --file flag its possible to save the generated topology definition in a file by a given path. node-prefix # With --node-prefix flag a user sets the name prefix of every node in a lab. Nodes will be named by the following template: <node-prefix>-<tier>-<node-number> . So a node named node1-3 means this is the third node in a first tier of a topology. Default prefix: node . group-prefix # With --group-prefix it is possible to change the Group value of a node. Group information is used in the topology graph rendering. network # With --network flag a user sets the name of the management network that will be created by container orchestration system such as docker. Default: clab . ipv4-subnet | ipv6-subnet # With --ipv4-subnet and ipv6-subnet its possible to change the address ranges of the management network. Nodes will receive IP addresses from these ranges if they are configured with DHCP. Examples # # generate and deploy a lab topology for 3-tier CLOS network # with 8 leafs, 4 spines and 2 superspines # all using Nokia SR Linux nodes with license and image provided. # Note that `srl` kind in the image and license flags might be omitted, # as it is implied by default) containerlab generate --name 3tier --image srl = srlinux:latest \\ --license srl = license.key \\ --nodes 8 ,4,2 --deploy","title":"generate"},{"location":"cmd/generate/#generate-command","text":"","title":"generate command"},{"location":"cmd/generate/#description","text":"The generate command generates the topology definition file based on the user input provided via CLI flags. With this command it is possible to generate definition file for a CLOS fabric by just providing the number of nodes on each tier. The generated topology can be saved in a file or immediately scheduled for deployment. It is assumed, that the interconnection between the tiers is done in a full-mesh fashion. Such as tier1 nodes are fully meshed with tier2, tier2 is meshed with tier3 and so on.","title":"Description"},{"location":"cmd/generate/#usage","text":"containerlab [global-flags] generate [local-flags] aliases: gen","title":"Usage"},{"location":"cmd/generate/#flags","text":"","title":"Flags"},{"location":"cmd/generate/#name","text":"With the global --name | -n flag a user sets the name of the lab that will be generated.","title":"name"},{"location":"cmd/generate/#nodes","text":"The user configures the CLOS fabric topology by using the --nodes flag. The flag value is a comma separated list of CLOS tiers where each tier is defined by the number of nodes, its kind and type. Multiple --node flags can be specified. For example, the following flag value will define a 2-tier CLOS fabric with tier1 (leafs) consists of 4x SR Linux containers of IXR6 type and the 2x Arista cEOS spines: 4:srl:ixr6,2:ceos Note, that the default kind is srl , so you can omit the kind for SR Linux node. The same nodes value can be expressed like that: 4:ixr6,2:ceos","title":"nodes"},{"location":"cmd/generate/#kind","text":"With --kind flag it is possible to set the default kind that will be set for the nodes which do not have a kind specified in the --nodes flag. For example the following value will generate a 3-tier CLOS fabric of cEOS nodes: # cEOS fabric containerlab gen -n 3tier --kind ceos --nodes 4 ,2,1 # since SR Linux kind is assumed by default # SRL fabric command is even shorter containerlab gen -n 3tier --nodes 4 ,2,1","title":"kind"},{"location":"cmd/generate/#image","text":"Use --image flag to specify the container image that should be used by a given kind. The value of this flag follows the kind=image pattern. For example, to set the container image ceos:4.21.F for the ceos kind the flag will be: --image ceos=ceos:4.21.F . To set images for multiple kinds repeat the flag: --image srl=srlinux:latest --image ceos=ceos:4.21.F or use the comma separated form: --image srl=srlinux:latest,ceos=ceos:latest If the kind information is not provided in the image flag, the kind value will be taken from the --kind flag.","title":"image"},{"location":"cmd/generate/#license","text":"With --license flag it is possible to set the license path that should be used by a given kind. The value of this flag follows the kind=path pattern. For example, to set the license path for the srl kind: --license srl=/tmp/license.key . To set license for multiple kinds repeat the flag: --license <kind1>=/path1 --image <kind2>=/path2 or use the comma separated form: --license <kind1>=/path1,<kind2>=/path2","title":"license"},{"location":"cmd/generate/#deploy","text":"When --deploy flag is present, the lab deployment process starts using the generated topology definition file. The generated definition file is first saved by the path set with --file or, if file path is not set, by the default path of <lab-name>.clab.yml . Then the equivalent of the deploy -t <file> --reconfigure command is executed.","title":"deploy"},{"location":"cmd/generate/#max-workers","text":"With --max-workers flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create. If during the deployment of a large scaled lab you see errors about max number of opened files reached, limit the max workers with this flag.","title":"max-workers"},{"location":"cmd/generate/#file","text":"With --file flag its possible to save the generated topology definition in a file by a given path.","title":"file"},{"location":"cmd/generate/#node-prefix","text":"With --node-prefix flag a user sets the name prefix of every node in a lab. Nodes will be named by the following template: <node-prefix>-<tier>-<node-number> . So a node named node1-3 means this is the third node in a first tier of a topology. Default prefix: node .","title":"node-prefix"},{"location":"cmd/generate/#group-prefix","text":"With --group-prefix it is possible to change the Group value of a node. Group information is used in the topology graph rendering.","title":"group-prefix"},{"location":"cmd/generate/#network","text":"With --network flag a user sets the name of the management network that will be created by container orchestration system such as docker. Default: clab .","title":"network"},{"location":"cmd/generate/#ipv4-subnet-ipv6-subnet","text":"With --ipv4-subnet and ipv6-subnet its possible to change the address ranges of the management network. Nodes will receive IP addresses from these ranges if they are configured with DHCP.","title":"ipv4-subnet | ipv6-subnet"},{"location":"cmd/generate/#examples","text":"# generate and deploy a lab topology for 3-tier CLOS network # with 8 leafs, 4 spines and 2 superspines # all using Nokia SR Linux nodes with license and image provided. # Note that `srl` kind in the image and license flags might be omitted, # as it is implied by default) containerlab generate --name 3tier --image srl = srlinux:latest \\ --license srl = license.key \\ --nodes 8 ,4,2 --deploy","title":"Examples"},{"location":"cmd/graph/","text":"graph command # Description # The graph command generates graphical representations of the topology. Two graphing options are available: an HTML page with embedded graphics generated by containerlab based on a Go HTML template a graph description file in dot format that can be rendered using Graphviz or viewed online . HTML # The HTML based graph is the default graphing option. The topology will be graphed and served online using the embedded web server. The graph is created by rendering a Go HTML template against a data structure containing the topology name as well as a json string where 2 lists are present: nodes and links . nodes contains data about the lab nodes, such as name, kind, type, image, state, IP addresses,... links contains the list of links defined by source node and target node, as well as the endpoint names example of the json string { \"nodes\" : [ { \"name\" : \"node1-1\" , \"image\" : \"srlinux:20.6.1-286\" , \"kind\" : \"srl\" , \"group\" : \"tier-1\" , \"state\" : \"running/Up 21 seconds\" , \"ipv4_address\" : \"172.23.23.3/24\" , \"ipv6_address\" : \"2001:172:23:23::3/80\" }, // omi tte d res t o f n odes ], \"links\" : [ { \"source\" : \"node1-2\" , \"source_endpoint\" : \"e1-1\" , \"target\" : \"node2-1\" , \"target_endpoint\" : \"e1-2\" }, { \"source\" : \"node2-1\" , \"source_endpoint\" : \"e1-4\" , \"target\" : \"node3-1\" , \"target_endpoint\" : \"e1-1\" }, // t he res t is omi tte d ] } Within the template, Javascript libraries such as d3js directed force graph or vis.js network can be used to generate custom topology graphs. containerlab comes with a (minimalistic) default template using d3js . After the graph generation, it's possible to move the nodes to a desired position and export the graph in PNG format. Graphviz # When graph command is called without the --srv flag, containerlab will generate a graph description file in dot format . The dot file can be used to view the graphical representation of the topology either by rendering the dot file into a PNG file or using online dot viewer . Online vs offline graphing # When HTML graph option is used, containerlab will try to build the topology graph by inspecting the running containers which are part of the lab. This essentially means, that the lab must be running. Although this method provides some additional details (like IP addresses), it is not always convenient to run a lab to see its graph. The other option is to use the topology file solely to build the graph. This is done by adding --offline flag. If --offline flag was not provided and no containers were found matching the lab name, containerlab will use the topo file only (as if offline mode was set). Usage # containerlab [global-flags] graph [local-flags] Flags # topology # With the global --topo | -t flag a user sets the path to the topology file that will be used to get the . srv # The --srv flag allows a user to customize the HTTP address and port for the web server. Default value is :50080 . A single path / is served, where the graph is generated based on either a default template or on the template supplied using --template . template # The --template flag allows to customize the HTML based graph by supplying a user defined template that will be rendered and exposed on the address specified by --srv . dot # With --dot flag provided containerlab will generate the dot file instead of serving the topology with embedded HTTP server. Examples # # render a graph from running lab or topo file if lab is not running# # using HTML graph option with default server address :50080 containerlab graph --topo /path/to/topo1.clab.yml # start an http server on :3002 where topo1 graph will be rendered using a custom template my_template.html containerlab graph --topo /path/to/topo1.clab.yml --srv \":3002\" --template my_template.html","title":"graph"},{"location":"cmd/graph/#graph-command","text":"","title":"graph command"},{"location":"cmd/graph/#description","text":"The graph command generates graphical representations of the topology. Two graphing options are available: an HTML page with embedded graphics generated by containerlab based on a Go HTML template a graph description file in dot format that can be rendered using Graphviz or viewed online .","title":"Description"},{"location":"cmd/graph/#html","text":"The HTML based graph is the default graphing option. The topology will be graphed and served online using the embedded web server. The graph is created by rendering a Go HTML template against a data structure containing the topology name as well as a json string where 2 lists are present: nodes and links . nodes contains data about the lab nodes, such as name, kind, type, image, state, IP addresses,... links contains the list of links defined by source node and target node, as well as the endpoint names example of the json string { \"nodes\" : [ { \"name\" : \"node1-1\" , \"image\" : \"srlinux:20.6.1-286\" , \"kind\" : \"srl\" , \"group\" : \"tier-1\" , \"state\" : \"running/Up 21 seconds\" , \"ipv4_address\" : \"172.23.23.3/24\" , \"ipv6_address\" : \"2001:172:23:23::3/80\" }, // omi tte d res t o f n odes ], \"links\" : [ { \"source\" : \"node1-2\" , \"source_endpoint\" : \"e1-1\" , \"target\" : \"node2-1\" , \"target_endpoint\" : \"e1-2\" }, { \"source\" : \"node2-1\" , \"source_endpoint\" : \"e1-4\" , \"target\" : \"node3-1\" , \"target_endpoint\" : \"e1-1\" }, // t he res t is omi tte d ] } Within the template, Javascript libraries such as d3js directed force graph or vis.js network can be used to generate custom topology graphs. containerlab comes with a (minimalistic) default template using d3js . After the graph generation, it's possible to move the nodes to a desired position and export the graph in PNG format.","title":"HTML"},{"location":"cmd/graph/#graphviz","text":"When graph command is called without the --srv flag, containerlab will generate a graph description file in dot format . The dot file can be used to view the graphical representation of the topology either by rendering the dot file into a PNG file or using online dot viewer .","title":"Graphviz"},{"location":"cmd/graph/#online-vs-offline-graphing","text":"When HTML graph option is used, containerlab will try to build the topology graph by inspecting the running containers which are part of the lab. This essentially means, that the lab must be running. Although this method provides some additional details (like IP addresses), it is not always convenient to run a lab to see its graph. The other option is to use the topology file solely to build the graph. This is done by adding --offline flag. If --offline flag was not provided and no containers were found matching the lab name, containerlab will use the topo file only (as if offline mode was set).","title":"Online vs offline graphing"},{"location":"cmd/graph/#usage","text":"containerlab [global-flags] graph [local-flags]","title":"Usage"},{"location":"cmd/graph/#flags","text":"","title":"Flags"},{"location":"cmd/graph/#topology","text":"With the global --topo | -t flag a user sets the path to the topology file that will be used to get the .","title":"topology"},{"location":"cmd/graph/#srv","text":"The --srv flag allows a user to customize the HTTP address and port for the web server. Default value is :50080 . A single path / is served, where the graph is generated based on either a default template or on the template supplied using --template .","title":"srv"},{"location":"cmd/graph/#template","text":"The --template flag allows to customize the HTML based graph by supplying a user defined template that will be rendered and exposed on the address specified by --srv .","title":"template"},{"location":"cmd/graph/#dot","text":"With --dot flag provided containerlab will generate the dot file instead of serving the topology with embedded HTTP server.","title":"dot"},{"location":"cmd/graph/#examples","text":"# render a graph from running lab or topo file if lab is not running# # using HTML graph option with default server address :50080 containerlab graph --topo /path/to/topo1.clab.yml # start an http server on :3002 where topo1 graph will be rendered using a custom template my_template.html containerlab graph --topo /path/to/topo1.clab.yml --srv \":3002\" --template my_template.html","title":"Examples"},{"location":"cmd/inspect/","text":"inspect command # Description # The inspect command provides the information about the deployed labs. Usage # containerlab [global-flags] inspect [local-flags] Flags # all # With the local --all flag its possible to list all deployed labs in a single table. The output will also show the relative path to the topology file that was used to spawn this lab. The lab name and path values will be set for the first node of such lab, to reduce the clutter. Refer to the examples section for more details. topology | name # With the global --topo | -t or --name | -n flag a user specifies which particular lab they want to get the information about. format # The local --format flag enables different output stylings. By default the table view will be used. Currently, the only other format option is json that will produce the output in the JSON format. details # The inspect command produces a brief summary about the running lab components. It is also possible to get a full view on the running containers by adding --details flag. With this flag inspect command will output every bit of information about the running containers. This is what docker inspect command provides. Examples # # list all running labs on the host containerlab inspect --all +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | # | Topo Path | Lab Name | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | 1 | newlab.yml | newlab | clab-newlab-n1 | 3c8262034088 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.4/24 | 2001 :172:20:20::4/80 | | 2 | | | clab-newlab-n2 | 79c562b71997 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.5/24 | 2001 :172:20:20::5/80 | | 3 | srl02.yml | srl01 | clab-srl01-srl | 13c9e7543771 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | | 4 | | | clab-srl01-srl2 | 8cfca93b7b6f | srlinux:20.6.3-145 | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ # provide information about the running lab named srl02 containerlab inspect --name srlceos01 +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlceos01-ceos | 90bebb1e2c5f | ceos | ceos | | running | 172 .20.20.4/24 | 2001 :172:20:20::4/80 | | 2 | clab-srlceos01-srl | 82e9aa3c7e6b | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ # now in json format containerlab inspect --name srlceos01 -f json [ { \"lab_name\" : \"srlceos01\" , \"name\" : \"clab-srlceos01-srl\" , \"container_id\" : \"82e9aa3c7e6b\" , \"image\" : \"srlinux\" , \"kind\" : \"srl\" , \"state\" : \"running\" , \"ipv4_address\" : \"172.20.20.3/24\" , \"ipv6_address\" : \"2001:172:20:20::3/80\" } , { \"lab_name\" : \"srlceos01\" , \"name\" : \"clab-srlceos01-ceos\" , \"container_id\" : \"90bebb1e2c5f\" , \"image\" : \"ceos\" , \"kind\" : \"ceos\" , \"state\" : \"running\" , \"ipv4_address\" : \"172.20.20.4/24\" , \"ipv6_address\" : \"2001:172:20:20::4/80\" } ]","title":"inspect"},{"location":"cmd/inspect/#inspect-command","text":"","title":"inspect command"},{"location":"cmd/inspect/#description","text":"The inspect command provides the information about the deployed labs.","title":"Description"},{"location":"cmd/inspect/#usage","text":"containerlab [global-flags] inspect [local-flags]","title":"Usage"},{"location":"cmd/inspect/#flags","text":"","title":"Flags"},{"location":"cmd/inspect/#all","text":"With the local --all flag its possible to list all deployed labs in a single table. The output will also show the relative path to the topology file that was used to spawn this lab. The lab name and path values will be set for the first node of such lab, to reduce the clutter. Refer to the examples section for more details.","title":"all"},{"location":"cmd/inspect/#topology-name","text":"With the global --topo | -t or --name | -n flag a user specifies which particular lab they want to get the information about.","title":"topology | name"},{"location":"cmd/inspect/#format","text":"The local --format flag enables different output stylings. By default the table view will be used. Currently, the only other format option is json that will produce the output in the JSON format.","title":"format"},{"location":"cmd/inspect/#details","text":"The inspect command produces a brief summary about the running lab components. It is also possible to get a full view on the running containers by adding --details flag. With this flag inspect command will output every bit of information about the running containers. This is what docker inspect command provides.","title":"details"},{"location":"cmd/inspect/#examples","text":"# list all running labs on the host containerlab inspect --all +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | # | Topo Path | Lab Name | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ | 1 | newlab.yml | newlab | clab-newlab-n1 | 3c8262034088 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.4/24 | 2001 :172:20:20::4/80 | | 2 | | | clab-newlab-n2 | 79c562b71997 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.5/24 | 2001 :172:20:20::5/80 | | 3 | srl02.yml | srl01 | clab-srl01-srl | 13c9e7543771 | srlinux:20.6.3-145 | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | | 4 | | | clab-srl01-srl2 | 8cfca93b7b6f | srlinux:20.6.3-145 | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | +---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+ # provide information about the running lab named srl02 containerlab inspect --name srlceos01 +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srlceos01-ceos | 90bebb1e2c5f | ceos | ceos | | running | 172 .20.20.4/24 | 2001 :172:20:20::4/80 | | 2 | clab-srlceos01-srl | 82e9aa3c7e6b | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | +---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+ # now in json format containerlab inspect --name srlceos01 -f json [ { \"lab_name\" : \"srlceos01\" , \"name\" : \"clab-srlceos01-srl\" , \"container_id\" : \"82e9aa3c7e6b\" , \"image\" : \"srlinux\" , \"kind\" : \"srl\" , \"state\" : \"running\" , \"ipv4_address\" : \"172.20.20.3/24\" , \"ipv6_address\" : \"2001:172:20:20::3/80\" } , { \"lab_name\" : \"srlceos01\" , \"name\" : \"clab-srlceos01-ceos\" , \"container_id\" : \"90bebb1e2c5f\" , \"image\" : \"ceos\" , \"kind\" : \"ceos\" , \"state\" : \"running\" , \"ipv4_address\" : \"172.20.20.4/24\" , \"ipv6_address\" : \"2001:172:20:20::4/80\" } ]","title":"Examples"},{"location":"cmd/save/","text":"save command # Description # The save command perform configuration save for all the containers running in a lab. The exact command that performs configuration save depends on a given kind. The below table explains the method used for each kind: Kind Command Notes Nokia SR Linux sr_cli -d tools system configuration generate-checkpoint configuration is saved in a checkpoint file Arista cEOS not yet implemented Usage # containerlab [global-flags] save [local-flags] Flags # topology | name # With the global --topo | -t or --name | -n flag a user specifies from which lab to take the containers and perform the save configuration task. Examples # # save the configuration of the containers running in lab named srl02 \u276f containerlab save -n srl02 INFO [ 0001 ] clab-srl02-srl1: stdout: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:54.998Z' and comment '' INFO [ 0002 ] clab-srl02-srl2: stdout: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:56.444Z' and comment ''","title":"save"},{"location":"cmd/save/#save-command","text":"","title":"save command"},{"location":"cmd/save/#description","text":"The save command perform configuration save for all the containers running in a lab. The exact command that performs configuration save depends on a given kind. The below table explains the method used for each kind: Kind Command Notes Nokia SR Linux sr_cli -d tools system configuration generate-checkpoint configuration is saved in a checkpoint file Arista cEOS not yet implemented","title":"Description"},{"location":"cmd/save/#usage","text":"containerlab [global-flags] save [local-flags]","title":"Usage"},{"location":"cmd/save/#flags","text":"","title":"Flags"},{"location":"cmd/save/#topology-name","text":"With the global --topo | -t or --name | -n flag a user specifies from which lab to take the containers and perform the save configuration task.","title":"topology | name"},{"location":"cmd/save/#examples","text":"# save the configuration of the containers running in lab named srl02 \u276f containerlab save -n srl02 INFO [ 0001 ] clab-srl02-srl1: stdout: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:54.998Z' and comment '' INFO [ 0002 ] clab-srl02-srl2: stdout: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:56.444Z' and comment ''","title":"Examples"},{"location":"cmd/tools/disable-tx-offload/","text":"disable-tx-offload command # Description # The disable-tx-offload command under the tools command disables tx checksum offload for eth0 interface of a container referenced by its name. The need for disable-tx-offload might arise when you launch a container outside of containerlab or restart a container. Some nodes, like SR Linux, will require to have correct checksums in TCP packets, thus its needed to disable checksum offload on those containers for them to do checksum calculations instead of offloading it. Usage # containerlab tools disable-tx-offload [local-flags] Flags # container # With the local mandatory --container | -c flag a user specifies which container to remove tx offload in. Examples # # disable tx checksum on gnmic container \u276f clab tools disable-checksum -c clab-st-gnmic INFO [ 0000 ] getting container 'clab-st-gnmic' information INFO [ 0000 ] Tx checksum offload disabled for eth0 interface of clab-st-gnmic container","title":"disable-tx-offload"},{"location":"cmd/tools/disable-tx-offload/#disable-tx-offload-command","text":"","title":"disable-tx-offload command"},{"location":"cmd/tools/disable-tx-offload/#description","text":"The disable-tx-offload command under the tools command disables tx checksum offload for eth0 interface of a container referenced by its name. The need for disable-tx-offload might arise when you launch a container outside of containerlab or restart a container. Some nodes, like SR Linux, will require to have correct checksums in TCP packets, thus its needed to disable checksum offload on those containers for them to do checksum calculations instead of offloading it.","title":"Description"},{"location":"cmd/tools/disable-tx-offload/#usage","text":"containerlab tools disable-tx-offload [local-flags]","title":"Usage"},{"location":"cmd/tools/disable-tx-offload/#flags","text":"","title":"Flags"},{"location":"cmd/tools/disable-tx-offload/#container","text":"With the local mandatory --container | -c flag a user specifies which container to remove tx offload in.","title":"container"},{"location":"cmd/tools/disable-tx-offload/#examples","text":"# disable tx checksum on gnmic container \u276f clab tools disable-checksum -c clab-st-gnmic INFO [ 0000 ] getting container 'clab-st-gnmic' information INFO [ 0000 ] Tx checksum offload disabled for eth0 interface of clab-st-gnmic container","title":"Examples"},{"location":"cmd/tools/cert/sign/","text":"Cert sign # Description # The sign sub-command under the tools cert command creates a private key and a certificate and signs the created certificate with a given Certificate Authority. Usage # containerlab tools cert sign [local-flags] Flags # Name # To set a name under which the certificate and key files will be save the --name | -n flag can be used. A name set to mynode will create files mynode.pem , mynode-key.pem and mynode.csr . Default value is cert . Path # A directory path under which the generated files will be placed is set with --path | -p flag. Defaults to current working directory. CA Cert and CA Key # To indicate which CA should sign the certificate request, the command takes a path to CA certificate and CA key files. --ca-cert flag sets the path to the CA certificate file. --ca-key flag sets the path to the CA private key file. Common Name # Certificate Common Name (CN) field is set with --cn flag. Defaults to containerlab.srlinux.dev . Hosts # To add Subject Alternative Names (SAN) use the --hosts flag that takes a comma separate list of SAN values. Users can provide both DNS names and IP address, and the values will be placed into the DSN SAN and IP SAN automatically. Country # Certificate Country (C) field is set with --c flag. Defaults to Internet . Locality # Certificate Locality (L) field is set with --l flag. Defaults to Server . Organization # Certificate Organization (O) field is set with --o flag. Defaults to Containerlab . Organization Unit # Certificate Organization Unit (OU) field is set with --ou flag. Defaults to Containerlab Tools . Examples # # create a private key and certificate and sign the latter # with the Hosts list of [node.io, 192.168.0.1] # saving both files under the default name `cert` in the PWD # and signed by the CA identified by cert ca.pem and key ca-key.pem containerlab tools cert sign --ca-cert /tmp/ca.pem \\ --ca-key /tmp/ca-key.pem \\ --hosts node.io,192.168.0.1 Generated certificate can be verified/viewed with openssl tool: openssl x509 -in ca.pem -text Certificate: Data: Version: 3 (0x2) Serial Number: 3f:a7:77:54:e1:2f:47:d6:ca:56:72:e1:d1:d8:c9:0c:e8:46:fd:65 <SNIP>","title":"sign"},{"location":"cmd/tools/cert/sign/#cert-sign","text":"","title":"Cert sign"},{"location":"cmd/tools/cert/sign/#description","text":"The sign sub-command under the tools cert command creates a private key and a certificate and signs the created certificate with a given Certificate Authority.","title":"Description"},{"location":"cmd/tools/cert/sign/#usage","text":"containerlab tools cert sign [local-flags]","title":"Usage"},{"location":"cmd/tools/cert/sign/#flags","text":"","title":"Flags"},{"location":"cmd/tools/cert/sign/#name","text":"To set a name under which the certificate and key files will be save the --name | -n flag can be used. A name set to mynode will create files mynode.pem , mynode-key.pem and mynode.csr . Default value is cert .","title":"Name"},{"location":"cmd/tools/cert/sign/#path","text":"A directory path under which the generated files will be placed is set with --path | -p flag. Defaults to current working directory.","title":"Path"},{"location":"cmd/tools/cert/sign/#ca-cert-and-ca-key","text":"To indicate which CA should sign the certificate request, the command takes a path to CA certificate and CA key files. --ca-cert flag sets the path to the CA certificate file. --ca-key flag sets the path to the CA private key file.","title":"CA Cert and CA Key"},{"location":"cmd/tools/cert/sign/#common-name","text":"Certificate Common Name (CN) field is set with --cn flag. Defaults to containerlab.srlinux.dev .","title":"Common Name"},{"location":"cmd/tools/cert/sign/#hosts","text":"To add Subject Alternative Names (SAN) use the --hosts flag that takes a comma separate list of SAN values. Users can provide both DNS names and IP address, and the values will be placed into the DSN SAN and IP SAN automatically.","title":"Hosts"},{"location":"cmd/tools/cert/sign/#country","text":"Certificate Country (C) field is set with --c flag. Defaults to Internet .","title":"Country"},{"location":"cmd/tools/cert/sign/#locality","text":"Certificate Locality (L) field is set with --l flag. Defaults to Server .","title":"Locality"},{"location":"cmd/tools/cert/sign/#organization","text":"Certificate Organization (O) field is set with --o flag. Defaults to Containerlab .","title":"Organization"},{"location":"cmd/tools/cert/sign/#organization-unit","text":"Certificate Organization Unit (OU) field is set with --ou flag. Defaults to Containerlab Tools .","title":"Organization Unit"},{"location":"cmd/tools/cert/sign/#examples","text":"# create a private key and certificate and sign the latter # with the Hosts list of [node.io, 192.168.0.1] # saving both files under the default name `cert` in the PWD # and signed by the CA identified by cert ca.pem and key ca-key.pem containerlab tools cert sign --ca-cert /tmp/ca.pem \\ --ca-key /tmp/ca-key.pem \\ --hosts node.io,192.168.0.1 Generated certificate can be verified/viewed with openssl tool: openssl x509 -in ca.pem -text Certificate: Data: Version: 3 (0x2) Serial Number: 3f:a7:77:54:e1:2f:47:d6:ca:56:72:e1:d1:d8:c9:0c:e8:46:fd:65 <SNIP>","title":"Examples"},{"location":"cmd/tools/cert/ca/create/","text":"CA Create # Description # The create sub-command under the tools cert ca command creates a Certificate Authority (CA) certificate and its private key. Usage # containerlab tools cert ca create [local-flags] Flags # Name # To set a name under which the certificate and key files will be save the --name | -n flag can be used. A name set to myname will create files myname.pem , myname-key.pem and myname.csr . Default value is ca . Path # A directory path under which the generated files will be placed is set with --path | -p flag. Defaults to current working directory. Expiry # Certificate validity period is set as a duration interval with --expiry | -e flag. Defaults to 87600h , which is 10 years. Common Name # Certificate Common Name (CN) field is set with --cn flag. Defaults to containerlab.srlinux.dev . Country # Certificate Country (C) field is set with --c flag. Defaults to Internet . Locality # Certificate Locality (L) field is set with --l flag. Defaults to Server . Organization # Certificate Organization (O) field is set with --o flag. Defaults to Containerlab . Organization Unit # Certificate Organization Unit (OU) field is set with --ou flag. Defaults to Containerlab Tools . Examples # # create CA cert and key in the current dir. # uses default values for all certificate attributes # as a result, ca.pem and ca-cert.pem files will be written to the # current working directory containerlab tools cert ca create # create CA cert and key by the specified path with a filename root-ca # and a validity period of 1 minute containerlab tools cert ca create --path /tmp/certs/myca --name root-ca \\ --expiry 1m openssl x509 -in /tmp/certs/myca/root-ca.pem -text | grep -A 2 Validity Validity Not Before: Mar 25 15 :28:00 2021 GMT Not After : Mar 25 15 :29:00 2021 GMT Generated certificate can be verified/viewed with openssl tool: openssl x509 -in ca.pem -text Certificate: Data: Version: 3 (0x2) Serial Number: 3f:a7:77:54:e1:2f:47:d6:ca:56:72:e1:d1:d8:c9:0c:e8:46:fd:65 <SNIP>","title":"create"},{"location":"cmd/tools/cert/ca/create/#ca-create","text":"","title":"CA Create"},{"location":"cmd/tools/cert/ca/create/#description","text":"The create sub-command under the tools cert ca command creates a Certificate Authority (CA) certificate and its private key.","title":"Description"},{"location":"cmd/tools/cert/ca/create/#usage","text":"containerlab tools cert ca create [local-flags]","title":"Usage"},{"location":"cmd/tools/cert/ca/create/#flags","text":"","title":"Flags"},{"location":"cmd/tools/cert/ca/create/#name","text":"To set a name under which the certificate and key files will be save the --name | -n flag can be used. A name set to myname will create files myname.pem , myname-key.pem and myname.csr . Default value is ca .","title":"Name"},{"location":"cmd/tools/cert/ca/create/#path","text":"A directory path under which the generated files will be placed is set with --path | -p flag. Defaults to current working directory.","title":"Path"},{"location":"cmd/tools/cert/ca/create/#expiry","text":"Certificate validity period is set as a duration interval with --expiry | -e flag. Defaults to 87600h , which is 10 years.","title":"Expiry"},{"location":"cmd/tools/cert/ca/create/#common-name","text":"Certificate Common Name (CN) field is set with --cn flag. Defaults to containerlab.srlinux.dev .","title":"Common Name"},{"location":"cmd/tools/cert/ca/create/#country","text":"Certificate Country (C) field is set with --c flag. Defaults to Internet .","title":"Country"},{"location":"cmd/tools/cert/ca/create/#locality","text":"Certificate Locality (L) field is set with --l flag. Defaults to Server .","title":"Locality"},{"location":"cmd/tools/cert/ca/create/#organization","text":"Certificate Organization (O) field is set with --o flag. Defaults to Containerlab .","title":"Organization"},{"location":"cmd/tools/cert/ca/create/#organization-unit","text":"Certificate Organization Unit (OU) field is set with --ou flag. Defaults to Containerlab Tools .","title":"Organization Unit"},{"location":"cmd/tools/cert/ca/create/#examples","text":"# create CA cert and key in the current dir. # uses default values for all certificate attributes # as a result, ca.pem and ca-cert.pem files will be written to the # current working directory containerlab tools cert ca create # create CA cert and key by the specified path with a filename root-ca # and a validity period of 1 minute containerlab tools cert ca create --path /tmp/certs/myca --name root-ca \\ --expiry 1m openssl x509 -in /tmp/certs/myca/root-ca.pem -text | grep -A 2 Validity Validity Not Before: Mar 25 15 :28:00 2021 GMT Not After : Mar 25 15 :29:00 2021 GMT Generated certificate can be verified/viewed with openssl tool: openssl x509 -in ca.pem -text Certificate: Data: Version: 3 (0x2) Serial Number: 3f:a7:77:54:e1:2f:47:d6:ca:56:72:e1:d1:d8:c9:0c:e8:46:fd:65 <SNIP>","title":"Examples"},{"location":"cmd/tools/mysocketio/login/","text":"Mysocketio login # Description # The login sub-command under the tools mysocketio command performs a login to mysocketio service and saves the acquired authentication token 1 . The token is saved as $PWD/.mysocketio_token file. Usage # containerlab tools mysocketio login [local-flags] Flags # email # With mandatory --email | -e flag user sets an email address used to register with mysocketio service password # The --password | -p sets the password for a user. If flag is not set, the prompt will appear on the terminal to allow for safe enter of the password. Examples # # Login with password entered from the prompt containerlab tools mysocketio login -e myemail@dot.com Password: INFO [ 0000 ] Written mysocketio token to a file /root/containerlab/.mysocketio_token # Login with password passed as a flag containerlab tools mysocketio login -e myemail@dot.com -p Pa $$ word Password: INFO [ 0000 ] Written mysocketio token to a file /root/containerlab/.mysocketio_token Authentication token is used to publish ports of a containerlab nodes. \u21a9","title":"login"},{"location":"cmd/tools/mysocketio/login/#mysocketio-login","text":"","title":"Mysocketio login"},{"location":"cmd/tools/mysocketio/login/#description","text":"The login sub-command under the tools mysocketio command performs a login to mysocketio service and saves the acquired authentication token 1 . The token is saved as $PWD/.mysocketio_token file.","title":"Description"},{"location":"cmd/tools/mysocketio/login/#usage","text":"containerlab tools mysocketio login [local-flags]","title":"Usage"},{"location":"cmd/tools/mysocketio/login/#flags","text":"","title":"Flags"},{"location":"cmd/tools/mysocketio/login/#email","text":"With mandatory --email | -e flag user sets an email address used to register with mysocketio service","title":"email"},{"location":"cmd/tools/mysocketio/login/#password","text":"The --password | -p sets the password for a user. If flag is not set, the prompt will appear on the terminal to allow for safe enter of the password.","title":"password"},{"location":"cmd/tools/mysocketio/login/#examples","text":"# Login with password entered from the prompt containerlab tools mysocketio login -e myemail@dot.com Password: INFO [ 0000 ] Written mysocketio token to a file /root/containerlab/.mysocketio_token # Login with password passed as a flag containerlab tools mysocketio login -e myemail@dot.com -p Pa $$ word Password: INFO [ 0000 ] Written mysocketio token to a file /root/containerlab/.mysocketio_token Authentication token is used to publish ports of a containerlab nodes. \u21a9","title":"Examples"},{"location":"cmd/tools/veth/create/","text":"vEth create # Description # The create sub-command under the tools veth command creates a vEth interface between the following combination of nodes: container <-> container container <-> linux bridge container <-> ovs bridge container <-> host To specify the both endpoints of the veth interface pair the following two notations are used: two elements notation: <node-name>:<interface-name> this notation is used for container <-> container or container <-> host attachments. three elements notation: <kind>:<node-name>:<interface-name> this notation is used for container <-> bridge and container <-> ovs-bridge attachments Check out examples to see how these notations are used. Usage # containerlab tools veth create [local-flags] Flags # a-endpoint # vEth interface endpoint A is set with --a-endpoint | -a flag. b-endpoint # vEth interface endpoint B is set with --b-endpoint | -b flag. mtu # vEth interface MTU is set to 65000 by default, and can be changed with --mtu | -m flag. Examples # # create veth interface between containers clab-demo-node1 and clab-demo-node2 # both ends of veth pair will be named `eth1` containerlab tools veth create -a clab-demo-node1:eth1 -b clab-demo-node2:eth1 # create veth interface between container clab-demo-node1 and linux bridge br-1 containerlab tools veth create -a clab-demo-node1:eth1 -b bridge:br-1:br-eth1 # create veth interface between container clab-demo-node1 and OVS bridge ovsbr-1 containerlab tools veth create -a clab-demo-node1:eth1 -b ovs-bridge:ovsbr-1:br-eth1 # create veth interface between container clab-demo-node1 and host # note that a special node-name `host` is reserved to indicate that attachment is destined for container host system containerlab tools veth create -a clab-demo-node1:eth1 -b host:veth-eth1","title":"create"},{"location":"cmd/tools/veth/create/#veth-create","text":"","title":"vEth create"},{"location":"cmd/tools/veth/create/#description","text":"The create sub-command under the tools veth command creates a vEth interface between the following combination of nodes: container <-> container container <-> linux bridge container <-> ovs bridge container <-> host To specify the both endpoints of the veth interface pair the following two notations are used: two elements notation: <node-name>:<interface-name> this notation is used for container <-> container or container <-> host attachments. three elements notation: <kind>:<node-name>:<interface-name> this notation is used for container <-> bridge and container <-> ovs-bridge attachments Check out examples to see how these notations are used.","title":"Description"},{"location":"cmd/tools/veth/create/#usage","text":"containerlab tools veth create [local-flags]","title":"Usage"},{"location":"cmd/tools/veth/create/#flags","text":"","title":"Flags"},{"location":"cmd/tools/veth/create/#a-endpoint","text":"vEth interface endpoint A is set with --a-endpoint | -a flag.","title":"a-endpoint"},{"location":"cmd/tools/veth/create/#b-endpoint","text":"vEth interface endpoint B is set with --b-endpoint | -b flag.","title":"b-endpoint"},{"location":"cmd/tools/veth/create/#mtu","text":"vEth interface MTU is set to 65000 by default, and can be changed with --mtu | -m flag.","title":"mtu"},{"location":"cmd/tools/veth/create/#examples","text":"# create veth interface between containers clab-demo-node1 and clab-demo-node2 # both ends of veth pair will be named `eth1` containerlab tools veth create -a clab-demo-node1:eth1 -b clab-demo-node2:eth1 # create veth interface between container clab-demo-node1 and linux bridge br-1 containerlab tools veth create -a clab-demo-node1:eth1 -b bridge:br-1:br-eth1 # create veth interface between container clab-demo-node1 and OVS bridge ovsbr-1 containerlab tools veth create -a clab-demo-node1:eth1 -b ovs-bridge:ovsbr-1:br-eth1 # create veth interface between container clab-demo-node1 and host # note that a special node-name `host` is reserved to indicate that attachment is destined for container host system containerlab tools veth create -a clab-demo-node1:eth1 -b host:veth-eth1","title":"Examples"},{"location":"cmd/tools/vxlan/create/","text":"vxlan create # Description # The create sub-command under the tools vxlan command creates a VxLAN interface and sets tc rules to redirect traffic to/from a specified interface available in root namespace of a container host. This combination of a VxLAN interface and tc rules make possible to transparently connect lab nodes running on different VMs/hosts. VxLAN interface name will be a catenation of a prefix vx- and the interface name that is used to redirect traffic. If the existing interface is named srl_e1-1 , then VxLAN interface created for this interface will be named vx-srl_e1-1 . Usage # containerlab tools vxlan create [local-flags] Flags # remote # VxLAN tunnels set up with this command are unidirectional in nature. To set the remote endpoint address the --remote | -r flag should be used. id # VNI that the VxLAN tunnel will use is set with --id | -i flag. Defaults to 10 . link # As mentioned above, the tunnels are set up with a goal to transparently connect containers deployed on different hosts. To indicate which interface will be \"piped\" to a VxLAN tunnel the --link | -l flag should be used. dev # With --dev flag users can set the linux device that should be used in setting up the tunnel. Normally this flag can be omitted, since containerlab will take the device name which is used to reach the remote address as seen by the kernel routing table. mtu # With --mtu | -m flag it is possible to set VxLAN MTU. Max MTU is automatically set, so this flag is only needed when MTU lower than max is needed to be provisioned. Examples # # create vxlan tunnel and redirect traffic to/from existing interface srl_e1-1 to it # this effectively means anything that appears on srl_e1-1 interface will be piped to vxlan interface # and vice versa. # srl_e1-1 interface exists in root namespace \u276f ip l show srl_e1-1 617 : srl_e1-1@if618: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether fa:4c:16:11:11:05 brd ff:ff:ff:ff:ff:ff link-netns clab-vx-srl1 # create a vxlan tunnel to a remote vtep 10.0.0.20 with VNI 10 and redirect traffic to srl_e1-1 interface \u276f clab tools vxlan create -r 10 .0.0.20 -l srl_e1-1 --id 10 INFO [ 0000 ] Adding VxLAN link vx-srl_e1-1 under ens3 to remote address 10 .0.0.20 with VNI 10 INFO [ 0000 ] configuring ingress mirroring with tc in the direction of vx-srl_e1-1 -> srl_e1-1 INFO [ 0000 ] configuring ingress mirroring with tc in the direction of srl_e1-1 -> vx-srl_e1-1 # check the created interface \u276f ip l show vx-srl_e1-1 619 : vx-srl_e1-1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 7a:6e:ba:82:a4:6f brd ff:ff:ff:ff:ff:ff","title":"create"},{"location":"cmd/tools/vxlan/create/#vxlan-create","text":"","title":"vxlan create"},{"location":"cmd/tools/vxlan/create/#description","text":"The create sub-command under the tools vxlan command creates a VxLAN interface and sets tc rules to redirect traffic to/from a specified interface available in root namespace of a container host. This combination of a VxLAN interface and tc rules make possible to transparently connect lab nodes running on different VMs/hosts. VxLAN interface name will be a catenation of a prefix vx- and the interface name that is used to redirect traffic. If the existing interface is named srl_e1-1 , then VxLAN interface created for this interface will be named vx-srl_e1-1 .","title":"Description"},{"location":"cmd/tools/vxlan/create/#usage","text":"containerlab tools vxlan create [local-flags]","title":"Usage"},{"location":"cmd/tools/vxlan/create/#flags","text":"","title":"Flags"},{"location":"cmd/tools/vxlan/create/#remote","text":"VxLAN tunnels set up with this command are unidirectional in nature. To set the remote endpoint address the --remote | -r flag should be used.","title":"remote"},{"location":"cmd/tools/vxlan/create/#id","text":"VNI that the VxLAN tunnel will use is set with --id | -i flag. Defaults to 10 .","title":"id"},{"location":"cmd/tools/vxlan/create/#link","text":"As mentioned above, the tunnels are set up with a goal to transparently connect containers deployed on different hosts. To indicate which interface will be \"piped\" to a VxLAN tunnel the --link | -l flag should be used.","title":"link"},{"location":"cmd/tools/vxlan/create/#dev","text":"With --dev flag users can set the linux device that should be used in setting up the tunnel. Normally this flag can be omitted, since containerlab will take the device name which is used to reach the remote address as seen by the kernel routing table.","title":"dev"},{"location":"cmd/tools/vxlan/create/#mtu","text":"With --mtu | -m flag it is possible to set VxLAN MTU. Max MTU is automatically set, so this flag is only needed when MTU lower than max is needed to be provisioned.","title":"mtu"},{"location":"cmd/tools/vxlan/create/#examples","text":"# create vxlan tunnel and redirect traffic to/from existing interface srl_e1-1 to it # this effectively means anything that appears on srl_e1-1 interface will be piped to vxlan interface # and vice versa. # srl_e1-1 interface exists in root namespace \u276f ip l show srl_e1-1 617 : srl_e1-1@if618: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether fa:4c:16:11:11:05 brd ff:ff:ff:ff:ff:ff link-netns clab-vx-srl1 # create a vxlan tunnel to a remote vtep 10.0.0.20 with VNI 10 and redirect traffic to srl_e1-1 interface \u276f clab tools vxlan create -r 10 .0.0.20 -l srl_e1-1 --id 10 INFO [ 0000 ] Adding VxLAN link vx-srl_e1-1 under ens3 to remote address 10 .0.0.20 with VNI 10 INFO [ 0000 ] configuring ingress mirroring with tc in the direction of vx-srl_e1-1 -> srl_e1-1 INFO [ 0000 ] configuring ingress mirroring with tc in the direction of srl_e1-1 -> vx-srl_e1-1 # check the created interface \u276f ip l show vx-srl_e1-1 619 : vx-srl_e1-1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 7a:6e:ba:82:a4:6f brd ff:ff:ff:ff:ff:ff","title":"Examples"},{"location":"cmd/tools/vxlan/delete/","text":"vxlan delete # Description # The delete sub-command under the tools vxlan command deletes VxLAN interfaces which name matches a user specified prefix. The delete command is typically used to remove VxLAN interfaces created with create command. Usage # containerlab tools vxlan delete [local-flags] Flags # prefix # Set a prefix with --prefix | -p flag. The VxLAN interfaces which name is matched by the prefix will be deleted. Default prefix is vx- which is matched the default prefix used by create command. Examples # # delete all VxLAN interfaces created by containerlab \u276f clab tools vxlan create delete INFO [ 0000 ] Deleting VxLAN link vx-srl_e1-1","title":"delete"},{"location":"cmd/tools/vxlan/delete/#vxlan-delete","text":"","title":"vxlan delete"},{"location":"cmd/tools/vxlan/delete/#description","text":"The delete sub-command under the tools vxlan command deletes VxLAN interfaces which name matches a user specified prefix. The delete command is typically used to remove VxLAN interfaces created with create command.","title":"Description"},{"location":"cmd/tools/vxlan/delete/#usage","text":"containerlab tools vxlan delete [local-flags]","title":"Usage"},{"location":"cmd/tools/vxlan/delete/#flags","text":"","title":"Flags"},{"location":"cmd/tools/vxlan/delete/#prefix","text":"Set a prefix with --prefix | -p flag. The VxLAN interfaces which name is matched by the prefix will be deleted. Default prefix is vx- which is matched the default prefix used by create command.","title":"prefix"},{"location":"cmd/tools/vxlan/delete/#examples","text":"# delete all VxLAN interfaces created by containerlab \u276f clab tools vxlan create delete INFO [ 0000 ] Deleting VxLAN link vx-srl_e1-1","title":"Examples"},{"location":"lab-examples/bgp-vpls-nok-jun/","text":"Description BGP VPLS between Nokia SR OS and Juniper vMX Components Nokia SR OS, Juniper vMX Resource requirements 1 2 7-10 GB Lab location hellt/bgp-vpls-lab Topology file vpls.clab.yml Version information 2 containerlab:0.10.1 , vr-sros:20.10.R1 , vr-vmx:20.4R1.12 , docker-ce:19.03.13 , vrnetlab 3 Description # This lab demonstrates how containerlab can be used in a classical networking labs where the prime focus is not on the containerized NOS, but on a classic VM-based routers. The topology created in this lab matches the network used in the BGP VPLS Deep Dive article: It allows readers to follow through the article with the author and create BGP VPLS service between the Nokia and Juniper routers using configuration snippets provided within the lab repository. As the article was done before Nokia introduced MD-CLI, the configuration snippets for SR OS were translated to MD-CLI. Quickstart # Ensure that your host supports virtualization and/or nested virtualization in case of a VM. Install 4 containerlab. Build if needed, vrnetlab container images for the routers used in the lab. Clone lab repository . Deploy the lab topology clab dep -t vpls.clab.yml Resource requirements are provisional. Consult with the installation guides for additional information. Memory deduplication techniques like UKMS might help with RAM consumption. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9 Router images are built with vrnetlab aebe377 . To reproduce the image, checkout to this commit and build the relevant images. Note, that you might need to use containerlab of the version that is stated in the description. \u21a9 If installing the latest containerlab, make sure to use the latest hellt/vrnetlab project as well, as there might have been changes with the integration. If unsure, install the containerlab version that is specified in the lab description. \u21a9","title":"BGP VPLS between Nokia and Juniper"},{"location":"lab-examples/bgp-vpls-nok-jun/#description","text":"This lab demonstrates how containerlab can be used in a classical networking labs where the prime focus is not on the containerized NOS, but on a classic VM-based routers. The topology created in this lab matches the network used in the BGP VPLS Deep Dive article: It allows readers to follow through the article with the author and create BGP VPLS service between the Nokia and Juniper routers using configuration snippets provided within the lab repository. As the article was done before Nokia introduced MD-CLI, the configuration snippets for SR OS were translated to MD-CLI.","title":"Description"},{"location":"lab-examples/bgp-vpls-nok-jun/#quickstart","text":"Ensure that your host supports virtualization and/or nested virtualization in case of a VM. Install 4 containerlab. Build if needed, vrnetlab container images for the routers used in the lab. Clone lab repository . Deploy the lab topology clab dep -t vpls.clab.yml Resource requirements are provisional. Consult with the installation guides for additional information. Memory deduplication techniques like UKMS might help with RAM consumption. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9 Router images are built with vrnetlab aebe377 . To reproduce the image, checkout to this commit and build the relevant images. Note, that you might need to use containerlab of the version that is stated in the description. \u21a9 If installing the latest containerlab, make sure to use the latest hellt/vrnetlab project as well, as there might have been changes with the integration. If unsure, install the containerlab version that is specified in the lab description. \u21a9","title":"Quickstart"},{"location":"lab-examples/ext-bridge/","text":"Description Connecting nodes via linux bridges Components Nokia SR Linux Resource requirements 1 2 2 GB Topology file br01.clab.yml Name br01 Description # This lab consists of three Nokia SR Linux nodes connected to a linux bridge. Note containerlab will not create/remove the bridge interface on your behalf. bridge element must be part of the lab nodes. Consult with the topology file to see how to reference a bridge. Use cases # By introducing a link of bridge type to the containerlab topology, we are opening ourselves to some additional scenarios: interconnect nodes via a broadcast domain connect multiple fabrics together connect containerlab nodes to the applications/nodes running outside of the lab host Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"External bridge capability"},{"location":"lab-examples/ext-bridge/#description","text":"This lab consists of three Nokia SR Linux nodes connected to a linux bridge. Note containerlab will not create/remove the bridge interface on your behalf. bridge element must be part of the lab nodes. Consult with the topology file to see how to reference a bridge.","title":"Description"},{"location":"lab-examples/ext-bridge/#use-cases","text":"By introducing a link of bridge type to the containerlab topology, we are opening ourselves to some additional scenarios: interconnect nodes via a broadcast domain connect multiple fabrics together connect containerlab nodes to the applications/nodes running outside of the lab host Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"Use cases"},{"location":"lab-examples/lab-examples/","text":"About lab examples # containerlab aims to provide a simple, intuitive and yet customizable way to run container based labs. To help our users to have a running and functional lab as quickly as possible, we ship some essential lab topologies within the containerlab package. These lab examples are meant to be used as-is or as a base layer to a more customized or elaborated lab scenarios. Once containerlab is installed, you will find the lab examples directories by the /etc/containerlab/lab-examples path. Copy those directories over to your working directory to start using the provided labs. Container images versions Some lab examples may use the images without a tag, i.e. image: srlinux . This means that the image with a latest tag must exist. A user needs to tag the image themselves if the latest tag is missing. For example: docker tag srlinux:20.6.1-286 srlinux:latest The source code of the lab examples is contained within the containerlab repo unless mentioned otherwise; any questions, issues or contributions related to the provided examples can be addressed via Github issues . Each lab comes with a definitive description that can be found in this documentation section. How to deploy a lab from the lab catalog? # Running the labs from the catalog is easy. Copy lab catalog # First, you need to copy the lab catalog to some place, for example to a current working directory. By copying labs from their original place we ensure that the changes we might make to the lab files will not be overwritten once we upgrade containerlab. To copy the entire catalog into your working directory: # copy over the srl02 lab files cp -a /etc/containerlab/lab-examples/* . as a result of this command you will get several directories copied to the current working directory. Note Some big labs or community provided labs are typically stored in a separate git repository. To fetch those labs you will need to clone the lab' repo instead of copying the directories from /etc/containerlab/lab-examples . Get the lab name # Every lab in the catalog has a unique short name. For example this lab states in the summary table that it's name is srl02 . You will find a folder matching this name in your working directory, change into it: cd srl02 Check images and licenses # Within the lab directory you will find the files that are used in the lab. Usually, only the topology definition file and, sometimes, config files are present in the lab directory. If you check the topology file you will see if any license files are required and what images are specified for each node/kind. Either change the topology file to point to the right image/license or change the image/license to match the topo definition file values. Deploy the lab # You are ready to deploy! containerlab deploy -t <topo-file>","title":"About"},{"location":"lab-examples/lab-examples/#about-lab-examples","text":"containerlab aims to provide a simple, intuitive and yet customizable way to run container based labs. To help our users to have a running and functional lab as quickly as possible, we ship some essential lab topologies within the containerlab package. These lab examples are meant to be used as-is or as a base layer to a more customized or elaborated lab scenarios. Once containerlab is installed, you will find the lab examples directories by the /etc/containerlab/lab-examples path. Copy those directories over to your working directory to start using the provided labs. Container images versions Some lab examples may use the images without a tag, i.e. image: srlinux . This means that the image with a latest tag must exist. A user needs to tag the image themselves if the latest tag is missing. For example: docker tag srlinux:20.6.1-286 srlinux:latest The source code of the lab examples is contained within the containerlab repo unless mentioned otherwise; any questions, issues or contributions related to the provided examples can be addressed via Github issues . Each lab comes with a definitive description that can be found in this documentation section.","title":"About lab examples"},{"location":"lab-examples/lab-examples/#how-to-deploy-a-lab-from-the-lab-catalog","text":"Running the labs from the catalog is easy.","title":"How to deploy a lab from the lab catalog?"},{"location":"lab-examples/lab-examples/#copy-lab-catalog","text":"First, you need to copy the lab catalog to some place, for example to a current working directory. By copying labs from their original place we ensure that the changes we might make to the lab files will not be overwritten once we upgrade containerlab. To copy the entire catalog into your working directory: # copy over the srl02 lab files cp -a /etc/containerlab/lab-examples/* . as a result of this command you will get several directories copied to the current working directory. Note Some big labs or community provided labs are typically stored in a separate git repository. To fetch those labs you will need to clone the lab' repo instead of copying the directories from /etc/containerlab/lab-examples .","title":"Copy lab catalog"},{"location":"lab-examples/lab-examples/#get-the-lab-name","text":"Every lab in the catalog has a unique short name. For example this lab states in the summary table that it's name is srl02 . You will find a folder matching this name in your working directory, change into it: cd srl02","title":"Get the lab name"},{"location":"lab-examples/lab-examples/#check-images-and-licenses","text":"Within the lab directory you will find the files that are used in the lab. Usually, only the topology definition file and, sometimes, config files are present in the lab directory. If you check the topology file you will see if any license files are required and what images are specified for each node/kind. Either change the topology file to point to the right image/license or change the image/license to match the topo definition file values.","title":"Check images and licenses"},{"location":"lab-examples/lab-examples/#deploy-the-lab","text":"You are ready to deploy! containerlab deploy -t <topo-file>","title":"Deploy the lab"},{"location":"lab-examples/min-5clos/","text":"Description A 5-stage CLOS topology based on Nokia SR Linux Components Nokia SR Linux Resource requirements 1 4 8 GB Topology file clos02.clab.yml Name clos02 Description # This labs provides a lightweight folded 5-stage CLOS fabric with Super Spine level bridging two PODs. The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation. Use cases # With this lightweight CLOS topology a user can exhibit the following scenarios: perform configuration tasks applied to the 5-stage CLOS fabric demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"5-stage CLOS"},{"location":"lab-examples/min-5clos/#description","text":"This labs provides a lightweight folded 5-stage CLOS fabric with Super Spine level bridging two PODs. The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation.","title":"Description"},{"location":"lab-examples/min-5clos/#use-cases","text":"With this lightweight CLOS topology a user can exhibit the following scenarios: perform configuration tasks applied to the 5-stage CLOS fabric demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"Use cases"},{"location":"lab-examples/min-clos/","text":"Description A minimal CLOS topology with two leafs and a spine Components Nokia SR Linux Resource requirements 1 2 3 GB Topology file clos01.clab.yml Name clos01 Description # This labs provides a lightweight folded CLOS fabric topology using a minimal set of nodes: two leaves and a single spine. The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation. Use cases # With this lightweight CLOS topology a user can exhibit the following scenarios: perform configuration tasks applied to the 3-stage CLOS fabric demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"3-nodes CLOS"},{"location":"lab-examples/min-clos/#description","text":"This labs provides a lightweight folded CLOS fabric topology using a minimal set of nodes: two leaves and a single spine. The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation.","title":"Description"},{"location":"lab-examples/min-clos/#use-cases","text":"With this lightweight CLOS topology a user can exhibit the following scenarios: perform configuration tasks applied to the 3-stage CLOS fabric demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"Use cases"},{"location":"lab-examples/multinode/","text":"Description A lab demonstrating multi-node (multi-vm) capabilities Components Nokia SR OS, Juniper vMX Resource requirements 1 2 6 GB per node Topology file vxlan-vmx.clab.yml , vxlan-sros.clab.yml Name vxlan01 Version information 2 containerlab:0.11.0 , vr-sros:20.2.R1 , vr-vmx:20.4R1.12 , docker-ce:20.10.2 Description # This lab demonstrates how containerlab can deploy labs on different machines and stitch the interfaces of the running nodes via VxLAN tunnels. With such approach users are allowed to spread the load between multiple VMs and still have the nodes connected via p2p links as if they were sitting on the same virtual machine. For the sake of the demonstration the topology used in this lab consists of just two virtualized routers packaged in a container format - Nokia SR OS and Juniper vMX. Although the routers are running on different VMs, they logically form a back-to-back connection over a pair of interfaces aggregated in a logical bundle. Upon succesful lab deployment and configuration, the routers will be able to exchange LACP frames, thus proving a transparent L2 connectivity and will be able to ping each other. Deployment # Since this lab is of a multi-node nature, a user needs to have two machines/VMs and perform lab deployment process on each of them. The lab directory has topology files named vxlan-sros.clab.yml and vxlan-vmx.clab.yml which are meant to be deployed on VM1 and VM2 accordingly. The following command will deploy a lab on a specified host: VM1 (SROS) clab dep -t vxlan-sros.clab.yml VM2 (VMX) clab dep -t vxlan-vmx.clab.yml host links # Both topology files leverage host link feature which allows a container to have its interface to be connected to a container host namespace. Once the topology is created you will have one side of the veth link visible in the root namespace by the names specified in topo file. For example, vxlan-sros.clab.yml file has the following links section: links : # we expose two sros container interfaces # to host namespace by using host interfaces style # docs: https://containerlab.srlinux.dev/manual/network/#host-links - endpoints : [ \"sros:eth1\" , \"host:sr-eth1\" ] - endpoints : [ \"sros:eth2\" , \"host:sr-eth2\" ] This will effectively make two veth pairs. Let us consider the first veth pair where one end of a it will be placed inside the container' namespace and named eth1 , the other end will stay in the container host root namespace and will be named sros-eth1 . Same picture will be on VM2 with vMX interfaces exposed to a container host. verify host link VM1 \u276f ip l | grep sros-eth 622 : sr-eth1@if623: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 624 : sr-eth2@if625: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default VM2 \u276f ip l | grep vmx-eth 1982 : vmx-eth1@if1983: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 1984 : vmx-eth2@if1985: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default vxlan tunneling # At this moment there is no connectivity between the routers, as the datapath is not ready. What we need to add is the VxLAN tunnels that will stitch SR OS container with vMX. We do this by provisioning VxLAN tunnels that will stitch the interfaces of our routers. Logically we make our interface appear to be connected in a point-to-point fashion. To make these tunnels we leverage containerlab' tools vxlan create command, that will create the VxLAN tunnel and the necessary redirection rules to forward traffic back-and-forth to a relevant host interface. All we need is to provide the VMs address and choose VNI numbers. And do this on both hosts. VM1 \u276f clab tools vxlan create --remote 10 .0.0.20 --id 10 --link sr-eth1 \u276f clab tools vxlan create --remote 10 .0.0.20 --id 20 --link sr-eth2 VM2 \u276f clab tools vxlan create --remote 10 .0.0.18 --id 10 --link vmx-eth1 \u276f clab tools vxlan create --remote 10 .0.0.18 --id 20 --link vmx-eth2 The above set of commands will create the necessary VxLAN tunnels and the datapath is ready. At this moment, the connectivity diagrams becomes complete and can be depicted as follows: Configuration # Once the datapath is in place, we proceed with the configuration of a simple LACP use case, where both SR OS and vMX routers have their pair of interfaces aggregated into a LAG and form an LACP neighborship. SR OS configure lag \"lag-aggr\" admin-state enable configure lag \"lag-aggr\" mode hybrid configure lag \"lag-aggr\" lacp mode active configure lag \"lag-aggr\" port 1/1/c1/1 configure lag \"lag-aggr\" port 1/1/c2/1 configure port 1/1/c1 admin-state enable configure port 1/1/c1 connector breakout c1-100g configure port 1/1/c1/1 admin-state enable configure port 1/1/c1/1 ethernet configure port 1/1/c1/1 ethernet mode hybrid configure port 1/1/c2 admin-state enable configure port 1/1/c2 connector breakout c1-100g configure port 1/1/c2/1 admin-state enable configure port 1/1/c2/1 ethernet mode hybrid configure router \"Base\" interface \"toVMX\" port lag-aggr:0 configure router \"Base\" interface \"toVMX\" ipv4 primary address 192.168.1.1 prefix-length 24 vMX set interfaces ge-0/0/0 gigether-options 802.3ad ae0 set interfaces ge-0/0/1 gigether-options 802.3ad ae0 set interfaces ae0 aggregated-ether-options minimum-links 1 set interfaces ae0 aggregated-ether-options link-speed 1g set interfaces ae0 aggregated-ether-options lacp active set interfaces ae0 unit 0 family inet address 192.168.1.2/24 Verification # To verify that LACP protocol works the following commands can be issued on both routers to display information about the aggregated interface and LACP status: SR OS # verifying operational status of LAG interface A:admin@sros# show lag \"lag-aggr\" =============================================================================== Lag Data =============================================================================== Lag-id Adm Opr Weighted Threshold Up-Count MC Act/Stdby name ------------------------------------------------------------------------------- 65 up up No 0 2 N/A lag-aggr =============================================================================== # show LACP statistics. Both incoming and trasmitted counters will increase A:admin@sros# show lag \"lag-aggr\" lacp-statistics =============================================================================== LAG LACP Statistics =============================================================================== LAG-id Port-id Tx Rx Rx Error Rx Illegal (Pdus) (Pdus) (Pdus) (Pdus) ------------------------------------------------------------------------------- 65 1/1/c1/1 78642 77394 0 0 65 1/1/c2/1 78644 77396 0 0 ------------------------------------------------------------------------------- Totals 157286 154790 0 0 =============================================================================== vMX admin@vmx> show interfaces ae0 brief Physical interface: ae0, Enabled, Physical link is Up Link-level type: Ethernet, MTU: 1514, Speed: 2Gbps, Loopback: Disabled, Source filtering: Disabled, Flow control: Disabled Device flags : Present Running Interface flags: SNMP-Traps Internal: 0x4000 Logical interface ae0.0 Flags: Up SNMP-Traps 0x4004000 Encapsulation: ENET2 inet 192.168.1.2/24 multiservice admin@vmx> show lacp interfaces Aggregated interface: ae0 LACP state: Role Exp Def Dist Col Syn Aggr Timeout Activity ge-0/0/0 Actor No No Yes Yes Yes Yes Fast Active ge-0/0/0 Partner No No Yes Yes Yes Yes Fast Active ge-0/0/1 Actor No No Yes Yes Yes Yes Fast Active ge-0/0/1 Partner No No Yes Yes Yes Yes Fast Active LACP protocol: Receive State Transmit State Mux State ge-0/0/0 Current Fast periodic Collecting distributing ge-0/0/1 Current Fast periodic Collecting distributing admin@vmx> show lacp statistics interfaces ae0 Aggregated interface: ae0 LACP Statistics: LACP Rx LACP Tx Unknown Rx Illegal Rx ge-0/0/0 78104 77469 0 0 ge-0/0/1 78106 77471 0 0 After the control plane verfification let's verify that the dataplane is working by pinging the IP address of the remote interface (issued from SR OS node in the example): A:admin@sros# ping 192.168.1.2 PING 192.168.1.2 56 data bytes 64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=13.5ms. 64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=2.61ms. ping aborted by user ---- 192.168.1.2 PING Statistics ---- 2 packets transmitted, 2 packets received, 0.00% packet loss round-trip min = 2.61ms, avg = 8.04ms, max = 13.5ms, stddev = 0.000ms Great! Additionally users can capture the traffic from any of the interfaces involved in the datapath. To see the VxLAN encapsulation the VM's outgoing interfaces should be used. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Multi-node labs"},{"location":"lab-examples/multinode/#description","text":"This lab demonstrates how containerlab can deploy labs on different machines and stitch the interfaces of the running nodes via VxLAN tunnels. With such approach users are allowed to spread the load between multiple VMs and still have the nodes connected via p2p links as if they were sitting on the same virtual machine. For the sake of the demonstration the topology used in this lab consists of just two virtualized routers packaged in a container format - Nokia SR OS and Juniper vMX. Although the routers are running on different VMs, they logically form a back-to-back connection over a pair of interfaces aggregated in a logical bundle. Upon succesful lab deployment and configuration, the routers will be able to exchange LACP frames, thus proving a transparent L2 connectivity and will be able to ping each other.","title":"Description"},{"location":"lab-examples/multinode/#deployment","text":"Since this lab is of a multi-node nature, a user needs to have two machines/VMs and perform lab deployment process on each of them. The lab directory has topology files named vxlan-sros.clab.yml and vxlan-vmx.clab.yml which are meant to be deployed on VM1 and VM2 accordingly. The following command will deploy a lab on a specified host: VM1 (SROS) clab dep -t vxlan-sros.clab.yml VM2 (VMX) clab dep -t vxlan-vmx.clab.yml","title":"Deployment"},{"location":"lab-examples/multinode/#host-links","text":"Both topology files leverage host link feature which allows a container to have its interface to be connected to a container host namespace. Once the topology is created you will have one side of the veth link visible in the root namespace by the names specified in topo file. For example, vxlan-sros.clab.yml file has the following links section: links : # we expose two sros container interfaces # to host namespace by using host interfaces style # docs: https://containerlab.srlinux.dev/manual/network/#host-links - endpoints : [ \"sros:eth1\" , \"host:sr-eth1\" ] - endpoints : [ \"sros:eth2\" , \"host:sr-eth2\" ] This will effectively make two veth pairs. Let us consider the first veth pair where one end of a it will be placed inside the container' namespace and named eth1 , the other end will stay in the container host root namespace and will be named sros-eth1 . Same picture will be on VM2 with vMX interfaces exposed to a container host. verify host link VM1 \u276f ip l | grep sros-eth 622 : sr-eth1@if623: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 624 : sr-eth2@if625: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default VM2 \u276f ip l | grep vmx-eth 1982 : vmx-eth1@if1983: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 1984 : vmx-eth2@if1985: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default","title":"host links"},{"location":"lab-examples/multinode/#vxlan-tunneling","text":"At this moment there is no connectivity between the routers, as the datapath is not ready. What we need to add is the VxLAN tunnels that will stitch SR OS container with vMX. We do this by provisioning VxLAN tunnels that will stitch the interfaces of our routers. Logically we make our interface appear to be connected in a point-to-point fashion. To make these tunnels we leverage containerlab' tools vxlan create command, that will create the VxLAN tunnel and the necessary redirection rules to forward traffic back-and-forth to a relevant host interface. All we need is to provide the VMs address and choose VNI numbers. And do this on both hosts. VM1 \u276f clab tools vxlan create --remote 10 .0.0.20 --id 10 --link sr-eth1 \u276f clab tools vxlan create --remote 10 .0.0.20 --id 20 --link sr-eth2 VM2 \u276f clab tools vxlan create --remote 10 .0.0.18 --id 10 --link vmx-eth1 \u276f clab tools vxlan create --remote 10 .0.0.18 --id 20 --link vmx-eth2 The above set of commands will create the necessary VxLAN tunnels and the datapath is ready. At this moment, the connectivity diagrams becomes complete and can be depicted as follows:","title":"vxlan tunneling"},{"location":"lab-examples/multinode/#configuration","text":"Once the datapath is in place, we proceed with the configuration of a simple LACP use case, where both SR OS and vMX routers have their pair of interfaces aggregated into a LAG and form an LACP neighborship. SR OS configure lag \"lag-aggr\" admin-state enable configure lag \"lag-aggr\" mode hybrid configure lag \"lag-aggr\" lacp mode active configure lag \"lag-aggr\" port 1/1/c1/1 configure lag \"lag-aggr\" port 1/1/c2/1 configure port 1/1/c1 admin-state enable configure port 1/1/c1 connector breakout c1-100g configure port 1/1/c1/1 admin-state enable configure port 1/1/c1/1 ethernet configure port 1/1/c1/1 ethernet mode hybrid configure port 1/1/c2 admin-state enable configure port 1/1/c2 connector breakout c1-100g configure port 1/1/c2/1 admin-state enable configure port 1/1/c2/1 ethernet mode hybrid configure router \"Base\" interface \"toVMX\" port lag-aggr:0 configure router \"Base\" interface \"toVMX\" ipv4 primary address 192.168.1.1 prefix-length 24 vMX set interfaces ge-0/0/0 gigether-options 802.3ad ae0 set interfaces ge-0/0/1 gigether-options 802.3ad ae0 set interfaces ae0 aggregated-ether-options minimum-links 1 set interfaces ae0 aggregated-ether-options link-speed 1g set interfaces ae0 aggregated-ether-options lacp active set interfaces ae0 unit 0 family inet address 192.168.1.2/24","title":"Configuration"},{"location":"lab-examples/multinode/#verification","text":"To verify that LACP protocol works the following commands can be issued on both routers to display information about the aggregated interface and LACP status: SR OS # verifying operational status of LAG interface A:admin@sros# show lag \"lag-aggr\" =============================================================================== Lag Data =============================================================================== Lag-id Adm Opr Weighted Threshold Up-Count MC Act/Stdby name ------------------------------------------------------------------------------- 65 up up No 0 2 N/A lag-aggr =============================================================================== # show LACP statistics. Both incoming and trasmitted counters will increase A:admin@sros# show lag \"lag-aggr\" lacp-statistics =============================================================================== LAG LACP Statistics =============================================================================== LAG-id Port-id Tx Rx Rx Error Rx Illegal (Pdus) (Pdus) (Pdus) (Pdus) ------------------------------------------------------------------------------- 65 1/1/c1/1 78642 77394 0 0 65 1/1/c2/1 78644 77396 0 0 ------------------------------------------------------------------------------- Totals 157286 154790 0 0 =============================================================================== vMX admin@vmx> show interfaces ae0 brief Physical interface: ae0, Enabled, Physical link is Up Link-level type: Ethernet, MTU: 1514, Speed: 2Gbps, Loopback: Disabled, Source filtering: Disabled, Flow control: Disabled Device flags : Present Running Interface flags: SNMP-Traps Internal: 0x4000 Logical interface ae0.0 Flags: Up SNMP-Traps 0x4004000 Encapsulation: ENET2 inet 192.168.1.2/24 multiservice admin@vmx> show lacp interfaces Aggregated interface: ae0 LACP state: Role Exp Def Dist Col Syn Aggr Timeout Activity ge-0/0/0 Actor No No Yes Yes Yes Yes Fast Active ge-0/0/0 Partner No No Yes Yes Yes Yes Fast Active ge-0/0/1 Actor No No Yes Yes Yes Yes Fast Active ge-0/0/1 Partner No No Yes Yes Yes Yes Fast Active LACP protocol: Receive State Transmit State Mux State ge-0/0/0 Current Fast periodic Collecting distributing ge-0/0/1 Current Fast periodic Collecting distributing admin@vmx> show lacp statistics interfaces ae0 Aggregated interface: ae0 LACP Statistics: LACP Rx LACP Tx Unknown Rx Illegal Rx ge-0/0/0 78104 77469 0 0 ge-0/0/1 78106 77471 0 0 After the control plane verfification let's verify that the dataplane is working by pinging the IP address of the remote interface (issued from SR OS node in the example): A:admin@sros# ping 192.168.1.2 PING 192.168.1.2 56 data bytes 64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=13.5ms. 64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=2.61ms. ping aborted by user ---- 192.168.1.2 PING Statistics ---- 2 packets transmitted, 2 packets received, 0.00% packet loss round-trip min = 2.61ms, avg = 8.04ms, max = 13.5ms, stddev = 0.000ms Great! Additionally users can capture the traffic from any of the interfaces involved in the datapath. To see the VxLAN encapsulation the VM's outgoing interfaces should be used. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Verification"},{"location":"lab-examples/single-srl/","text":"Description a single Nokia SR Linux node Components Nokia SR Linux Resource requirements 1 2 2 GB Topology file srl01.clab.yml Name srl01 Description # A lab consists of a single SR Linux container equipped with a single interface - its management interface. No other network/data interfaces are created. The SR Linux's mgmt interface is connected to the containerlab docker network that is created as part of the lab deployment process. The mgmt interface of SRL will get IPv4/6 address information via DHCP service provided by docker daemon. Use cases # This lightweight lab enables the users to perform the following exercises: get familiar with SR Linux architecture explore SR Linux extensible CLI navigate the SR Linux YANG tree play with gNMI 2 and JSON-RPC programmable interfaces write/debug/manage custom apps built for SR Linux NDK Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9 Check out gnmic gNMI client to interact with SR Linux gNMI server. \u21a9","title":"Single SR Linux node"},{"location":"lab-examples/single-srl/#description","text":"A lab consists of a single SR Linux container equipped with a single interface - its management interface. No other network/data interfaces are created. The SR Linux's mgmt interface is connected to the containerlab docker network that is created as part of the lab deployment process. The mgmt interface of SRL will get IPv4/6 address information via DHCP service provided by docker daemon.","title":"Description"},{"location":"lab-examples/single-srl/#use-cases","text":"This lightweight lab enables the users to perform the following exercises: get familiar with SR Linux architecture explore SR Linux extensible CLI navigate the SR Linux YANG tree play with gNMI 2 and JSON-RPC programmable interfaces write/debug/manage custom apps built for SR Linux NDK Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9 Check out gnmic gNMI client to interact with SR Linux gNMI server. \u21a9","title":"Use cases"},{"location":"lab-examples/srl-ceos/","text":"Description A Nokia SR Linux connected back-to-back with Arista cEOS Components Nokia SR Linux , Arista cEOS Resource requirements 1 2 2 GB Topology file srlceos01.clab.yml Name srlceos01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , ceos:4.25.0F , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Arista cEOS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the containerlab docker network. Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Arista cEOS operating systems. BGP # This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and Arista cEOS. Both nodes exchange NLRI with their loopback prefix making it reachable. Configuration # Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlceos01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 # configure BGP set / network-instance default protocols bgp admin-state enable set / network-instance default protocols bgp router-id 10 .10.10.1 set / network-instance default protocols bgp autonomous-system 65001 set / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable set / network-instance default protocols bgp group ibgp export-policy export-lo set / network-instance default protocols bgp neighbor 192 .168.1.2 admin-state enable set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-group ibgp set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-as 65001 # create export policy set / routing-policy policy export-lo statement 10 match protocol local set / routing-policy policy export-lo statement 10 action accept # commit config commit now ceos Get into cEOS CLI with docker exec -it clab-srlceos01-ceos Cli and start configuration # enter configuration mode configure ip routing # configure loopback and data interfaces interface Ethernet1 no switchport ip address 192 .168.1.2/24 exit interface Loopback0 ip address 10 .10.10.2/32 exit # configure BGP router bgp 65001 router-id 10 .10.10.2 neighbor 192 .168.1.1 remote-as 65001 network 10 .10.10.2/32 exit Verification # Once BGP peering is established, the routes can be seen in GRT of both nodes: srl A:srl# show network-instance default route-table ipv4-unicast summary | grep bgp | 10 .10.10.2/32 | 0 | true | bgp | 0 | 170 | 192 .168.1.2 ( indirect ) | None | ceos ceos>show ip route VRF: default Codes: C - connected, S - static, K - kernel, O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1 , E2 - OSPF external type 2 , N1 - OSPF NSSA external type 1 , N2 - OSPF NSSA external type2, B - BGP, B I - iBGP, B E - eBGP, R - RIP, I L1 - IS-IS level 1 , I L2 - IS-IS level 2 , O3 - OSPFv3, A B - BGP Aggregate, A O - OSPF Summary, NG - Nexthop Group Static Route, V - VXLAN Control Service, DH - DHCP client installed default route, M - Martian, DP - Dynamic Policy Route, L - VRF Leaked, RC - Route Cache Route Gateway of last resort: K 0 .0.0.0/0 [ 40 /0 ] via 172 .20.20.1, Management0 B I 10 .10.10.1/32 [ 200 /0 ] via 192 .168.1.1, Ethernet1 C 10 .10.10.2/32 is directly connected, Loopback0 C 172 .20.20.0/24 is directly connected, Management0 C 192 .168.1.0/24 is directly connected, Ethernet1 Data plane confirms that routes have been programmed to FIB: A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=3.47 ms Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Arista cEOS"},{"location":"lab-examples/srl-ceos/#description","text":"A lab consists of an SR Linux node connected with Arista cEOS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the containerlab docker network.","title":"Description"},{"location":"lab-examples/srl-ceos/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Arista cEOS operating systems.","title":"Use cases"},{"location":"lab-examples/srl-ceos/#bgp","text":"This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and Arista cEOS. Both nodes exchange NLRI with their loopback prefix making it reachable.","title":"BGP"},{"location":"lab-examples/srl-ceos/#configuration","text":"Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlceos01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 # configure BGP set / network-instance default protocols bgp admin-state enable set / network-instance default protocols bgp router-id 10 .10.10.1 set / network-instance default protocols bgp autonomous-system 65001 set / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable set / network-instance default protocols bgp group ibgp export-policy export-lo set / network-instance default protocols bgp neighbor 192 .168.1.2 admin-state enable set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-group ibgp set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-as 65001 # create export policy set / routing-policy policy export-lo statement 10 match protocol local set / routing-policy policy export-lo statement 10 action accept # commit config commit now ceos Get into cEOS CLI with docker exec -it clab-srlceos01-ceos Cli and start configuration # enter configuration mode configure ip routing # configure loopback and data interfaces interface Ethernet1 no switchport ip address 192 .168.1.2/24 exit interface Loopback0 ip address 10 .10.10.2/32 exit # configure BGP router bgp 65001 router-id 10 .10.10.2 neighbor 192 .168.1.1 remote-as 65001 network 10 .10.10.2/32 exit","title":"Configuration"},{"location":"lab-examples/srl-ceos/#verification","text":"Once BGP peering is established, the routes can be seen in GRT of both nodes: srl A:srl# show network-instance default route-table ipv4-unicast summary | grep bgp | 10 .10.10.2/32 | 0 | true | bgp | 0 | 170 | 192 .168.1.2 ( indirect ) | None | ceos ceos>show ip route VRF: default Codes: C - connected, S - static, K - kernel, O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1 , E2 - OSPF external type 2 , N1 - OSPF NSSA external type 1 , N2 - OSPF NSSA external type2, B - BGP, B I - iBGP, B E - eBGP, R - RIP, I L1 - IS-IS level 1 , I L2 - IS-IS level 2 , O3 - OSPFv3, A B - BGP Aggregate, A O - OSPF Summary, NG - Nexthop Group Static Route, V - VXLAN Control Service, DH - DHCP client installed default route, M - Martian, DP - Dynamic Policy Route, L - VRF Leaked, RC - Route Cache Route Gateway of last resort: K 0 .0.0.0/0 [ 40 /0 ] via 172 .20.20.1, Management0 B I 10 .10.10.1/32 [ 200 /0 ] via 192 .168.1.1, Ethernet1 C 10 .10.10.2/32 is directly connected, Loopback0 C 172 .20.20.0/24 is directly connected, Management0 C 192 .168.1.0/24 is directly connected, Ethernet1 Data plane confirms that routes have been programmed to FIB: A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=3.47 ms Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Verification"},{"location":"lab-examples/srl-crpd/","text":"Description A Nokia SR Linux connected back-to-back with Juniper cRPD Components Nokia SR Linux , Juniper cRPD Resource requirements 1 2 2 GB Topology file srlcrpd01.clab.yml Name srlcrpd01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , crpd:20.2R1.10 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Juniper cRPD via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper cRPD network operating systems. OSPF # Configuration # Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable OSPF on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlcrpd01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 # configure OSPF set / network-instance default router-id 10 .10.10.1 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 set / network-instance default protocols ospf instance main admin-state enable set / network-instance default protocols ospf instance main version ospf-v2 set / network-instance default protocols ospf instance main area 0 .0.0.0 interface ethernet-1/1.0 interface-type point-to-point set / network-instance default protocols ospf instance main area 0 .0.0.0 interface ethernet-1/1.0 # commit config commit now crpd cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process bash shell and configure interfaces: docker exec -it clab-srlcrpd01-crpd bash # configure linux interfaces ip addr add 192 .168.1.2/24 dev eth1 ip addr add 10 .10.10.2/32 dev lo Then launch the CLI and continue configuration docker exec -it clab-srlcrpd01-crpd cli : # enter configuration mode configure set routing-options router-id 10 .10.10.2 set protocols ospf area 0 .0.0.0 interface eth1 interface-type p2p set protocols ospf area 0 .0.0.0 interface lo.0 interface-type nbma # commit configuration commit Verificaton # After the configuration is done on both nodes, verify the control plane by checking the route tables on both ends and ensuring dataplane was programmed as well by pinging the remote loopback srl # control plane verification A:srl# / show network-instance default route-table ipv4-unicast summary | grep ospf | 10 .10.10.2/32 | 0 | true | ospfv2 | 1 | 10 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | # data plane verification A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms crpd # control plane verification root@crpd> show route | match OSPF 10 .10.10.1/32 * [ OSPF/10 ] 00 :01:24, metric 1 224 .0.0.5/32 * [ OSPF/10 ] 00 :05:49, metric 1 IS-IS # Configuration # Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable IS-IS on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlcrpd01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 # configure IS-IS set / network-instance default router-id 10 .10.10.1 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 set / network-instance default protocols isis instance main admin-state enable set / network-instance default protocols isis instance main net [ 49 .0001.0100.1001.0001.00 ] set / network-instance default protocols isis instance main interface ethernet-1/1.0 admin-state enable set / network-instance default protocols isis instance main interface ethernet-1/1.0 circuit-type point-to-point set / network-instance default protocols isis instance main interface lo0.0 # commit config commit now crpd cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process bash shell and configure interfaces: docker exec -it clab-srlcrpd01-crpd bash # configure linux interfaces ip addr add 192 .168.1.2/24 dev eth1 ip addr add 10 .10.10.2/32 dev lo Then launch the CLI and continue configuration docker exec -it clab-srlcrpd01-crpd cli : # enter configuration mode configure set interfaces lo0 unit 0 family iso address 49 .0001.0100.1001.0002.00 set routing-options router-id 10 .10.10.2 set protocols isis interface all point-to-point set protocols isis interface lo0.0 set protocols isis level 1 wide-metrics-only set protocols isis level 2 wide-metrics-only set protocols isis reference-bandwidth 100g # commit configuration commit srl # control plane verification A:srl# / show network-instance default route-table ipv4-unicast summary | grep isis | 10 .10.10.2/32 | 0 | true | isis | 10 | 18 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | | 172 .20.20.0/24 | 0 | true | isis | 110 | 18 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | # data plane verification A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms crpd # control plane verification root@crpd> show route table inet.0 | match IS-IS 10 .10.10.1/32 * [ IS-IS/18 ] 00 :00:13, metric 100 Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Juniper cRPD"},{"location":"lab-examples/srl-crpd/#description","text":"A lab consists of an SR Linux node connected with Juniper cRPD via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network.","title":"Description"},{"location":"lab-examples/srl-crpd/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper cRPD network operating systems.","title":"Use cases"},{"location":"lab-examples/srl-crpd/#ospf","text":"","title":"OSPF"},{"location":"lab-examples/srl-crpd/#configuration","text":"Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable OSPF on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlcrpd01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 # configure OSPF set / network-instance default router-id 10 .10.10.1 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 set / network-instance default protocols ospf instance main admin-state enable set / network-instance default protocols ospf instance main version ospf-v2 set / network-instance default protocols ospf instance main area 0 .0.0.0 interface ethernet-1/1.0 interface-type point-to-point set / network-instance default protocols ospf instance main area 0 .0.0.0 interface ethernet-1/1.0 # commit config commit now crpd cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process bash shell and configure interfaces: docker exec -it clab-srlcrpd01-crpd bash # configure linux interfaces ip addr add 192 .168.1.2/24 dev eth1 ip addr add 10 .10.10.2/32 dev lo Then launch the CLI and continue configuration docker exec -it clab-srlcrpd01-crpd cli : # enter configuration mode configure set routing-options router-id 10 .10.10.2 set protocols ospf area 0 .0.0.0 interface eth1 interface-type p2p set protocols ospf area 0 .0.0.0 interface lo.0 interface-type nbma # commit configuration commit","title":"Configuration"},{"location":"lab-examples/srl-crpd/#verificaton","text":"After the configuration is done on both nodes, verify the control plane by checking the route tables on both ends and ensuring dataplane was programmed as well by pinging the remote loopback srl # control plane verification A:srl# / show network-instance default route-table ipv4-unicast summary | grep ospf | 10 .10.10.2/32 | 0 | true | ospfv2 | 1 | 10 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | # data plane verification A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms crpd # control plane verification root@crpd> show route | match OSPF 10 .10.10.1/32 * [ OSPF/10 ] 00 :01:24, metric 1 224 .0.0.5/32 * [ OSPF/10 ] 00 :05:49, metric 1","title":"Verificaton"},{"location":"lab-examples/srl-crpd/#is-is","text":"","title":"IS-IS"},{"location":"lab-examples/srl-crpd/#configuration_1","text":"Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable IS-IS on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlcrpd01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 # configure IS-IS set / network-instance default router-id 10 .10.10.1 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 set / network-instance default protocols isis instance main admin-state enable set / network-instance default protocols isis instance main net [ 49 .0001.0100.1001.0001.00 ] set / network-instance default protocols isis instance main interface ethernet-1/1.0 admin-state enable set / network-instance default protocols isis instance main interface ethernet-1/1.0 circuit-type point-to-point set / network-instance default protocols isis instance main interface lo0.0 # commit config commit now crpd cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process bash shell and configure interfaces: docker exec -it clab-srlcrpd01-crpd bash # configure linux interfaces ip addr add 192 .168.1.2/24 dev eth1 ip addr add 10 .10.10.2/32 dev lo Then launch the CLI and continue configuration docker exec -it clab-srlcrpd01-crpd cli : # enter configuration mode configure set interfaces lo0 unit 0 family iso address 49 .0001.0100.1001.0002.00 set routing-options router-id 10 .10.10.2 set protocols isis interface all point-to-point set protocols isis interface lo0.0 set protocols isis level 1 wide-metrics-only set protocols isis level 2 wide-metrics-only set protocols isis reference-bandwidth 100g # commit configuration commit srl # control plane verification A:srl# / show network-instance default route-table ipv4-unicast summary | grep isis | 10 .10.10.2/32 | 0 | true | isis | 10 | 18 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | | 172 .20.20.0/24 | 0 | true | isis | 110 | 18 | 192 .168.1.2 ( direct ) | ethernet-1/1.0 | # data plane verification A:srl# ping 10.10.10.2 network-instance default Using network instance default PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms crpd # control plane verification root@crpd> show route table inet.0 | match IS-IS 10 .10.10.1/32 * [ IS-IS/18 ] 00 :00:13, metric 100 Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Configuration"},{"location":"lab-examples/srl-frr/","text":"Description A Nokia SR Linux connected back-to-back FRR router Components Nokia SR Linux , FRR Resource requirements 1 2 2 GB Topology file srlfrr01.clab.yml Name srlfrr01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , frrouting/frr:v7.5.0 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with FRR router via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Use cases # This lab allows users to launch basic control plane interoperability scenarios between Nokia SR Linux and FRR network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. There you will find the config files to demonstrate a classic iBGP peering use case: daemons : frr daemons config that is bind mounted to the frr container to trigger the start of the relevant FRR services frr.cfg : vtysh config lines to configure a basic iBGP peering srl.cfg : sr_cli config lines to configure a basic iBGP peering Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and FRR"},{"location":"lab-examples/srl-frr/#description","text":"A lab consists of an SR Linux node connected with FRR router via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network.","title":"Description"},{"location":"lab-examples/srl-frr/#use-cases","text":"This lab allows users to launch basic control plane interoperability scenarios between Nokia SR Linux and FRR network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. There you will find the config files to demonstrate a classic iBGP peering use case: daemons : frr daemons config that is bind mounted to the frr container to trigger the start of the relevant FRR services frr.cfg : vtysh config lines to configure a basic iBGP peering srl.cfg : sr_cli config lines to configure a basic iBGP peering Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/srl-sonic/","text":"Description A Nokia SR Linux connected back-to-back with SONiC-VS Components Nokia SR Linux , SONiC Resource requirements 1 2 2 GB Topology file sonic01.clab.yml Name sonic01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , docker-sonic-vs:202012 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Azure SONiC via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the containerlab docker network. Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and SONiC operating systems. BGP # This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and SONiC. Both nodes exchange NLRI with their loopback prefix making it reachable. Configuration # Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlsonic01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 # configure BGP set / network-instance default protocols bgp admin-state enable set / network-instance default protocols bgp router-id 10 .10.10.1 set / network-instance default protocols bgp autonomous-system 65001 set / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable set / network-instance default protocols bgp group ibgp export-policy export-lo set / network-instance default protocols bgp neighbor 192 .168.1.2 admin-state enable set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-group ibgp set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-as 65001 # create export policy set / routing-policy policy export-lo statement 10 match protocol local set / routing-policy policy export-lo statement 10 action accept # commit config commit now sonic Get into sonic container shell with docker exec -it clab-srlsonic01-sonic bash and configure the so-called front-panel ports. Since we defined only one data interface for our sonic/srl nodes, we need to confgure a single port: config interface ip add Ethernet0 192 .168.1.2/24 config interface startup Ethernet0 Now when data interface has been configured, enter in the FRR shell to configure BGP by typing vtysh command inside the sonic container. # enter configuration mode configure # configure BGP router bgp 65001 bgp router-id 10 .10.10.2 neighbor 192 .168.1.1 remote-as 65001 address-family ipv4 unicast network 10 .10.10.2/32 exit-address-family exit access-list all seq 5 permit any Verification # Once BGP peering is established, the routes can be seen in GRT of both nodes: srl A:srl# / show network-instance default route-table ipv4-unicast summary | grep bgp | 10 .10.10.2/32 | 0 | true | bgp | 0 | 170 | 192 .168.1.2 ( indirect ) | None | sonic sonic# sh ip route Codes: K - kernel route, C - connected, S - static, R - RIP, O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP, T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP, F - PBR, f - OpenFabric, > - selected route, * - FIB route, q - queued route, r - rejected route K>* 0 .0.0.0/0 [ 0 /0 ] via 172 .20.20.1, eth0, 00 :20:55 B>* 10 .10.10.1/32 [ 200 /0 ] via 192 .168.1.1, Ethernet0, 00 :01:51 C>* 172 .20.20.0/24 is directly connected, eth0, 00 :20:55 B 192 .168.1.0/24 [ 200 /0 ] via 192 .168.1.0 inactive, 00 :01:51 C>* 192 .168.1.0/24 is directly connected, Ethernet0, 00 :03:50 Data plane confirms that routes have been programmed to FIB: sonic# ping 10.10.10.1 PING 10.10.10.1 (10.10.10.1) 56(84) bytes of data. 64 bytes from 10.10.10.1: icmp_seq=1 ttl=64 time=2.28 ms 64 bytes from 10.10.10.1: icmp_seq=2 ttl=64 time=2.84 ms Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and SONiC"},{"location":"lab-examples/srl-sonic/#description","text":"A lab consists of an SR Linux node connected with Azure SONiC via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the containerlab docker network.","title":"Description"},{"location":"lab-examples/srl-sonic/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and SONiC operating systems.","title":"Use cases"},{"location":"lab-examples/srl-sonic/#bgp","text":"This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and SONiC. Both nodes exchange NLRI with their loopback prefix making it reachable.","title":"BGP"},{"location":"lab-examples/srl-sonic/#configuration","text":"Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes. srl Get into SR Linux CLI with docker exec -it clab-srlsonic01-srl sr_cli and start configuration # enter candidate datastore enter candidate # configure loopback and data interfaces set / interface ethernet-1/1 admin-state enable set / interface ethernet-1/1 subinterface 0 admin-state enable set / interface ethernet-1/1 subinterface 0 ipv4 address 192 .168.1.1/24 set / interface lo0 subinterface 0 admin-state enable set / interface lo0 subinterface 0 ipv4 address 10 .10.10.1/32 set / network-instance default interface ethernet-1/1.0 set / network-instance default interface lo0.0 # configure BGP set / network-instance default protocols bgp admin-state enable set / network-instance default protocols bgp router-id 10 .10.10.1 set / network-instance default protocols bgp autonomous-system 65001 set / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable set / network-instance default protocols bgp group ibgp export-policy export-lo set / network-instance default protocols bgp neighbor 192 .168.1.2 admin-state enable set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-group ibgp set / network-instance default protocols bgp neighbor 192 .168.1.2 peer-as 65001 # create export policy set / routing-policy policy export-lo statement 10 match protocol local set / routing-policy policy export-lo statement 10 action accept # commit config commit now sonic Get into sonic container shell with docker exec -it clab-srlsonic01-sonic bash and configure the so-called front-panel ports. Since we defined only one data interface for our sonic/srl nodes, we need to confgure a single port: config interface ip add Ethernet0 192 .168.1.2/24 config interface startup Ethernet0 Now when data interface has been configured, enter in the FRR shell to configure BGP by typing vtysh command inside the sonic container. # enter configuration mode configure # configure BGP router bgp 65001 bgp router-id 10 .10.10.2 neighbor 192 .168.1.1 remote-as 65001 address-family ipv4 unicast network 10 .10.10.2/32 exit-address-family exit access-list all seq 5 permit any","title":"Configuration"},{"location":"lab-examples/srl-sonic/#verification","text":"Once BGP peering is established, the routes can be seen in GRT of both nodes: srl A:srl# / show network-instance default route-table ipv4-unicast summary | grep bgp | 10 .10.10.2/32 | 0 | true | bgp | 0 | 170 | 192 .168.1.2 ( indirect ) | None | sonic sonic# sh ip route Codes: K - kernel route, C - connected, S - static, R - RIP, O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP, T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP, F - PBR, f - OpenFabric, > - selected route, * - FIB route, q - queued route, r - rejected route K>* 0 .0.0.0/0 [ 0 /0 ] via 172 .20.20.1, eth0, 00 :20:55 B>* 10 .10.10.1/32 [ 200 /0 ] via 192 .168.1.1, Ethernet0, 00 :01:51 C>* 172 .20.20.0/24 is directly connected, eth0, 00 :20:55 B 192 .168.1.0/24 [ 200 /0 ] via 192 .168.1.0 inactive, 00 :01:51 C>* 192 .168.1.0/24 is directly connected, Ethernet0, 00 :03:50 Data plane confirms that routes have been programmed to FIB: sonic# ping 10.10.10.1 PING 10.10.10.1 (10.10.10.1) 56(84) bytes of data. 64 bytes from 10.10.10.1: icmp_seq=1 ttl=64 time=2.28 ms 64 bytes from 10.10.10.1: icmp_seq=2 ttl=64 time=2.84 ms Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Verification"},{"location":"lab-examples/tls-cert/","text":"Description Securing gNMI with containerlab generated certificates Components Nokia SR OS Resource requirements 1 2 6 GB Topology file cert01.clab.yml Version information 2 containerlab:0.12.0 , vr-sros:21.2.R1 , docker-ce:19.03.13 , vrnetlab:0.2.3 3 , gnmic:0.9.0 Description # Nowadays more and more protocols require a secured transport layer for their operation where TLS is king. Creating a Certificate Authority, public/private keys, certificate signing requests and signing those was a mundane task that most network engineers tried to avoid... But thanks to the opensource projects like cfssl it is now less painful to overcome the difficulties of bootstrapping the PKI infra at least in the lab setting. Containerlab embeds parts of cfssl to expose what we consider a critical set of commands that enable our users to quickly set up TLS enabled transports. This lab demonstrates how containerlab' helper commands instantly create the necessary certificates for CA and the SR OS router to enable TLS-secured gNMI communication. Lab deployment # Before we start generating certificates, let's deploy this simple lab which consists of a single Nokia SR OS node with no data interfaces whatsoever. clab dep -t ~/cert01.clab.yml Write down the IP address that container engine assigned to our node, as we will use it in the certificate phase. +---+----------------+--------------+--------------------------+---------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+----------------+--------------+--------------------------+---------+-------+---------+----------------+----------------------+ | 1 | clab-cert01-sr | 183c82e1a033 | vrnetlab/vr-sros:21.2.R1 | vr-sros | | running | 172.20.20.2/24 | 2001:172:20:20::2/80 | +---+----------------+--------------+--------------------------+---------+-------+---------+----------------+----------------------+ Certificate generation # As promised, containerlab aims to provide a necessary tooling for users to enable TLS transport. In short, we need to create a CA which will sign the certificate of the SR OS node that we will also create. For that we will leverage the following containerlab commands: tools cert ca create - creates a Certificate Authority tools cert sign - creates certificate/key for a host and signs the certificate with CA Create CA # First we need to create a Certificate Authority that will be able to sign a node's certificate. Leveraging the default values that ca create command embeds, we can be as short as this: # create CA certificate and key in the current working dir containerlab tools cert ca create As a result of this command we will have ca.pem and ca-key.pem files generate in our current working directory. That is all it takes to create a CA. Create and sign node certificate # Next is the node certificate that we need to create and sign with the CA created before. Again, this is pretty simple, we just need to specify the DNS names and IP addresses we want this certificate to be valid for. Since containerlab creates persistent DNS names for the fully qualified node names, we know that DNS name of our router is clab-cert01-sr , which follow the pattern of clab-<lab-name>-<node-name> . We will also make our certificate to be valid for the IP address of the node. To get the IP address of the node refer to the summary table which containerlab provides when the lab deployment finishes. In our case the IP was 172.20.20.2 . Knowing the DNS and IP of the node we can create the certificate and key and immediately sign it with the Certificate Authority created earlier. All in one command! containerlab tools cert sign --ca-cert ca.pem --ca-key ca-key.pem \\ --hosts clab-cert01-sr,172.20.20.2 INFO [ 0000 ] Creating and signing certificate: Hosts =[ \"clab-cert01-sr\" \"172.20.20.2\" ] , CN = containerlab.srlinux.dev, C = Internet, L = Server, O = Containerlab, OU = Containerlab Tools Here we leveraged tools cert sign command that firstly inits the CA by using the its files ca.pem and ca-key.pem and then creates a node certificate for the DNS and IP names provided via hosts flag. Now, in our working directory we have the signed node's certificate with the file names cert.pem , cert-key.pem and CA cert and key from the previous step. Two short commands and you are good to go and configure SR OS to use them. Configuring SR OS # Transferring certificate and key # At a minimum we need to transfer the node certificate and key. An extra mile would be to also transfer the CA files to the node, but we will not do that in this lab. We will transfer the certificate files with SCP, but you can choose any other means: scp cert-key.pem admin@clab-cert01-sr:cf3:/ scp cert.pem admin@clab-cert01-sr:cf3:/ Importing certificate and key # SR OS needs the certificates to be imported after they are copied to the flash card. For that we need to switch to use the Classic CLI notation with // command prefix: //admin certificate import type cert input cf3:/cert.pem output cert.pem format pem //admin certificate import type key input cf3:/cert-key.pem output cert-key.pem format pem When certificates are imported, they are copied to a system system-pki directory on the flash card: [/] A:admin@sr# //file dir system-pki Directory of cf3:\\system-pki 03/26/2021 08:50p <DIR> ./ 03/26/2021 08:50p <DIR> ../ 03/26/2021 08:51p 1256 cert-key.pem 03/26/2021 08:50p 1095 cert.pem 2 File(s) 2351 bytes. 2 Dir(s) 683569152 bytes free. This command verifies that our two files - node' certificate and a matching private key - have been imported successfully. Certificate profile # Next step is to create a certificate profile that will bring the imported certificate file and a its private key under a single logical construct. /configure system security tls cert-profile sr-cert-prof entry 1 certificate-file cert.pem /configure system security tls cert-profile sr-cert-prof entry 1 key-file cert-key.pem /configure system security tls cert-profile sr-cert-prof admin-state enable Ciphers list # Proceed with creating a ciphers list that SR OS will use when negotiating TLS with. We choose a single cipher, though many are available on SR OS to match your client capabilities. /configure system security tls server-cipher-list \"ciphers\" cipher 1 name tls-rsa-with3des-ede-cbc-sha Server TLS profile # Finishing step is configuring the specific SR OS construct called \"server-tls-profile\". It sets which TLS profile, ciphers (and optionally CRL) to use for a specific TLS server configuration. /configure system security tls server-tls-profile sr-server-tls-prof cert-profile \"sr-cert-prof\" admin-state enable /configure system security tls server-tls-profile sr-server-tls-prof Configuring secured gRPC # Now when TLS objects are all created, we can make gRPC services on SR OS make use of the TLS. To do that, we override the default unsecured gRPC that vr-sros uses with a one that uses the tls-server-profile we created earlier: /configure system grpc tls-server-profile \"sr-server-tls-prof\" commit gRPC config before (pr)[/configure system grpc] A:admin@sr# info admin-state enable allow-unsecure-connection gnmi { auto-config-save true } rib-api { admin-state enable } gRPC config after *(pr)[/configure system grpc] A:admin@sr# info admin-state enable tls-server-profile \"sr-server-tls-prof\" gnmi { auto-config-save true } rib-api { admin-state enable } Now gRPC services will require TLS to be used by the clients, let's verify it. Verification # We will use gnmic CLI to issue gNMI RPCs to check if TLS is now really enforced and used. First, let's use the DNS name that our SR OS node an entry in /etc/hosts for 4 . gnmic -a clab-cert01-sr -u admin -p admin --tls-ca ca.pem capabilities gNMI version: 0.7.0 supported models: - nokia-conf, Nokia, 21.2.R1 - nokia-state, Nokia, 21.2.R1 - nokia-li-state, Nokia, 21.2.R1 - nokia-li-conf, Nokia, 21.2.R1 supported encodings: - JSON - BYTES - PROTO Note here, that we use the --tls-ca flag of gnmic to make sure that we verify the server's (router's) certificate by checking it with a CA certificate. If you remember, when we created the router' certificate we specified not only its DNS name, but also the IP address. This allows us to use management IP address with gNMI and still being able to verify the router's certificate: gnmic -a 172.20.20.2 -u admin -p admin --tls-ca ca.pem capabilities gNMI version: 0.7.0 <SNIP> Feel free to examine the pcap I captured with containerlab wireshark integration that shows the flow of TCP handshake with TLS negotiation for the same gNMI Capabilities request. Summary # Pretty neat, right? With just the two commands ( tools cert ca create and tools cert sign ) we managed to perform a lot of actions in the background which resulted in a signed CA and node certificates. Those certificates we can now use for any protocol that requires TLS and the certificates are verifiable and legit. Resource requirements are provisional. Consult with the installation guides for additional information. Memory deduplication techniques like UKMS might help with RAM consumption. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9 Version of our fork - hellt/vrnetlab with which the container image of this VM was generated. \u21a9 the /etc/hosts entry is created by containerlab when it deploys the nodes. \u21a9","title":"Securing gNMI with TLS"},{"location":"lab-examples/tls-cert/#description","text":"Nowadays more and more protocols require a secured transport layer for their operation where TLS is king. Creating a Certificate Authority, public/private keys, certificate signing requests and signing those was a mundane task that most network engineers tried to avoid... But thanks to the opensource projects like cfssl it is now less painful to overcome the difficulties of bootstrapping the PKI infra at least in the lab setting. Containerlab embeds parts of cfssl to expose what we consider a critical set of commands that enable our users to quickly set up TLS enabled transports. This lab demonstrates how containerlab' helper commands instantly create the necessary certificates for CA and the SR OS router to enable TLS-secured gNMI communication.","title":"Description"},{"location":"lab-examples/tls-cert/#lab-deployment","text":"Before we start generating certificates, let's deploy this simple lab which consists of a single Nokia SR OS node with no data interfaces whatsoever. clab dep -t ~/cert01.clab.yml Write down the IP address that container engine assigned to our node, as we will use it in the certificate phase. +---+----------------+--------------+--------------------------+---------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+----------------+--------------+--------------------------+---------+-------+---------+----------------+----------------------+ | 1 | clab-cert01-sr | 183c82e1a033 | vrnetlab/vr-sros:21.2.R1 | vr-sros | | running | 172.20.20.2/24 | 2001:172:20:20::2/80 | +---+----------------+--------------+--------------------------+---------+-------+---------+----------------+----------------------+","title":"Lab deployment"},{"location":"lab-examples/tls-cert/#certificate-generation","text":"As promised, containerlab aims to provide a necessary tooling for users to enable TLS transport. In short, we need to create a CA which will sign the certificate of the SR OS node that we will also create. For that we will leverage the following containerlab commands: tools cert ca create - creates a Certificate Authority tools cert sign - creates certificate/key for a host and signs the certificate with CA","title":"Certificate generation"},{"location":"lab-examples/tls-cert/#create-ca","text":"First we need to create a Certificate Authority that will be able to sign a node's certificate. Leveraging the default values that ca create command embeds, we can be as short as this: # create CA certificate and key in the current working dir containerlab tools cert ca create As a result of this command we will have ca.pem and ca-key.pem files generate in our current working directory. That is all it takes to create a CA.","title":"Create CA"},{"location":"lab-examples/tls-cert/#create-and-sign-node-certificate","text":"Next is the node certificate that we need to create and sign with the CA created before. Again, this is pretty simple, we just need to specify the DNS names and IP addresses we want this certificate to be valid for. Since containerlab creates persistent DNS names for the fully qualified node names, we know that DNS name of our router is clab-cert01-sr , which follow the pattern of clab-<lab-name>-<node-name> . We will also make our certificate to be valid for the IP address of the node. To get the IP address of the node refer to the summary table which containerlab provides when the lab deployment finishes. In our case the IP was 172.20.20.2 . Knowing the DNS and IP of the node we can create the certificate and key and immediately sign it with the Certificate Authority created earlier. All in one command! containerlab tools cert sign --ca-cert ca.pem --ca-key ca-key.pem \\ --hosts clab-cert01-sr,172.20.20.2 INFO [ 0000 ] Creating and signing certificate: Hosts =[ \"clab-cert01-sr\" \"172.20.20.2\" ] , CN = containerlab.srlinux.dev, C = Internet, L = Server, O = Containerlab, OU = Containerlab Tools Here we leveraged tools cert sign command that firstly inits the CA by using the its files ca.pem and ca-key.pem and then creates a node certificate for the DNS and IP names provided via hosts flag. Now, in our working directory we have the signed node's certificate with the file names cert.pem , cert-key.pem and CA cert and key from the previous step. Two short commands and you are good to go and configure SR OS to use them.","title":"Create and sign node certificate"},{"location":"lab-examples/tls-cert/#configuring-sr-os","text":"","title":"Configuring SR OS"},{"location":"lab-examples/tls-cert/#transferring-certificate-and-key","text":"At a minimum we need to transfer the node certificate and key. An extra mile would be to also transfer the CA files to the node, but we will not do that in this lab. We will transfer the certificate files with SCP, but you can choose any other means: scp cert-key.pem admin@clab-cert01-sr:cf3:/ scp cert.pem admin@clab-cert01-sr:cf3:/","title":"Transferring certificate and key"},{"location":"lab-examples/tls-cert/#importing-certificate-and-key","text":"SR OS needs the certificates to be imported after they are copied to the flash card. For that we need to switch to use the Classic CLI notation with // command prefix: //admin certificate import type cert input cf3:/cert.pem output cert.pem format pem //admin certificate import type key input cf3:/cert-key.pem output cert-key.pem format pem When certificates are imported, they are copied to a system system-pki directory on the flash card: [/] A:admin@sr# //file dir system-pki Directory of cf3:\\system-pki 03/26/2021 08:50p <DIR> ./ 03/26/2021 08:50p <DIR> ../ 03/26/2021 08:51p 1256 cert-key.pem 03/26/2021 08:50p 1095 cert.pem 2 File(s) 2351 bytes. 2 Dir(s) 683569152 bytes free. This command verifies that our two files - node' certificate and a matching private key - have been imported successfully.","title":"Importing certificate and key"},{"location":"lab-examples/tls-cert/#certificate-profile","text":"Next step is to create a certificate profile that will bring the imported certificate file and a its private key under a single logical construct. /configure system security tls cert-profile sr-cert-prof entry 1 certificate-file cert.pem /configure system security tls cert-profile sr-cert-prof entry 1 key-file cert-key.pem /configure system security tls cert-profile sr-cert-prof admin-state enable","title":"Certificate profile"},{"location":"lab-examples/tls-cert/#ciphers-list","text":"Proceed with creating a ciphers list that SR OS will use when negotiating TLS with. We choose a single cipher, though many are available on SR OS to match your client capabilities. /configure system security tls server-cipher-list \"ciphers\" cipher 1 name tls-rsa-with3des-ede-cbc-sha","title":"Ciphers list"},{"location":"lab-examples/tls-cert/#server-tls-profile","text":"Finishing step is configuring the specific SR OS construct called \"server-tls-profile\". It sets which TLS profile, ciphers (and optionally CRL) to use for a specific TLS server configuration. /configure system security tls server-tls-profile sr-server-tls-prof cert-profile \"sr-cert-prof\" admin-state enable /configure system security tls server-tls-profile sr-server-tls-prof","title":"Server TLS profile"},{"location":"lab-examples/tls-cert/#configuring-secured-grpc","text":"Now when TLS objects are all created, we can make gRPC services on SR OS make use of the TLS. To do that, we override the default unsecured gRPC that vr-sros uses with a one that uses the tls-server-profile we created earlier: /configure system grpc tls-server-profile \"sr-server-tls-prof\" commit gRPC config before (pr)[/configure system grpc] A:admin@sr# info admin-state enable allow-unsecure-connection gnmi { auto-config-save true } rib-api { admin-state enable } gRPC config after *(pr)[/configure system grpc] A:admin@sr# info admin-state enable tls-server-profile \"sr-server-tls-prof\" gnmi { auto-config-save true } rib-api { admin-state enable } Now gRPC services will require TLS to be used by the clients, let's verify it.","title":"Configuring secured gRPC"},{"location":"lab-examples/tls-cert/#verification","text":"We will use gnmic CLI to issue gNMI RPCs to check if TLS is now really enforced and used. First, let's use the DNS name that our SR OS node an entry in /etc/hosts for 4 . gnmic -a clab-cert01-sr -u admin -p admin --tls-ca ca.pem capabilities gNMI version: 0.7.0 supported models: - nokia-conf, Nokia, 21.2.R1 - nokia-state, Nokia, 21.2.R1 - nokia-li-state, Nokia, 21.2.R1 - nokia-li-conf, Nokia, 21.2.R1 supported encodings: - JSON - BYTES - PROTO Note here, that we use the --tls-ca flag of gnmic to make sure that we verify the server's (router's) certificate by checking it with a CA certificate. If you remember, when we created the router' certificate we specified not only its DNS name, but also the IP address. This allows us to use management IP address with gNMI and still being able to verify the router's certificate: gnmic -a 172.20.20.2 -u admin -p admin --tls-ca ca.pem capabilities gNMI version: 0.7.0 <SNIP> Feel free to examine the pcap I captured with containerlab wireshark integration that shows the flow of TCP handshake with TLS negotiation for the same gNMI Capabilities request.","title":"Verification"},{"location":"lab-examples/tls-cert/#summary","text":"Pretty neat, right? With just the two commands ( tools cert ca create and tools cert sign ) we managed to perform a lot of actions in the background which resulted in a signed CA and node certificates. Those certificates we can now use for any protocol that requires TLS and the certificates are verifiable and legit. Resource requirements are provisional. Consult with the installation guides for additional information. Memory deduplication techniques like UKMS might help with RAM consumption. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9 Version of our fork - hellt/vrnetlab with which the container image of this VM was generated. \u21a9 the /etc/hosts entry is created by containerlab when it deploys the nodes. \u21a9","title":"Summary"},{"location":"lab-examples/two-srls/","text":"Description Two Nokia SR Linux nodes Components Nokia SR Linux Resource requirements 1 2 2 GB Topology file srl02.clab.yml Name srl02 Validated versions 2 containerlab v0.8.2 , srlinux:20.6.2-332 Description # A lab consists of two SR Linux nodes connected with each other via a point-to-point link over e1-1 interfaces. Both nodes are also connected with their management interfaces to the clab docker network. Configuration # The nodes of this lab have been provided with a startup configuration by means of config directive in the topo definition file. The startup configuration adds loopback and interfaces addressing as per the diagram above. Once the lab is started, the nodes will be able to ping each other via configured interfaces: A:srl1# ping 192.168.0.1 network-instance default Using network instance default PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data. 64 bytes from 192.168.0.1: icmp_seq=1 ttl=64 time=5.17 ms Use cases # This lab, besides having the same objectives as srl01 lab, also enables the following scenarios: get to know protocols and services configuration verify basic control plane and data plane operations explore SR Linux state datastore for the paths which reflect control plane operation metrics or dataplane counters Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9 versions of respective container images or software that was used to create the lab. \u21a9","title":"Two SR Linux nodes"},{"location":"lab-examples/two-srls/#description","text":"A lab consists of two SR Linux nodes connected with each other via a point-to-point link over e1-1 interfaces. Both nodes are also connected with their management interfaces to the clab docker network.","title":"Description"},{"location":"lab-examples/two-srls/#configuration","text":"The nodes of this lab have been provided with a startup configuration by means of config directive in the topo definition file. The startup configuration adds loopback and interfaces addressing as per the diagram above. Once the lab is started, the nodes will be able to ping each other via configured interfaces: A:srl1# ping 192.168.0.1 network-instance default Using network instance default PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data. 64 bytes from 192.168.0.1: icmp_seq=1 ttl=64 time=5.17 ms","title":"Configuration"},{"location":"lab-examples/two-srls/#use-cases","text":"This lab, besides having the same objectives as srl01 lab, also enables the following scenarios: get to know protocols and services configuration verify basic control plane and data plane operations explore SR Linux state datastore for the paths which reflect control plane operation metrics or dataplane counters Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9 versions of respective container images or software that was used to create the lab. \u21a9","title":"Use cases"},{"location":"lab-examples/vr-sros/","text":"Description A Nokia SR Linux connected back-to-back with Nokia SR OS Components Nokia SR Linux , Nokia SR OS Resource requirements 1 2 5 GB Topology file vr01.clab.yml Name vr01 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , vr-sros:20.10.R1 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Nokia SR OS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Nokia SR OS VM is launched as a container, using vrnetlab integration . Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Nokia SR OS network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Nokia SR OS"},{"location":"lab-examples/vr-sros/#description","text":"A lab consists of an SR Linux node connected with Nokia SR OS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Nokia SR OS VM is launched as a container, using vrnetlab integration .","title":"Description"},{"location":"lab-examples/vr-sros/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Nokia SR OS network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/vr-vmx/","text":"Description A Nokia SR Linux connected back-to-back with Juniper vMX Components Nokia SR Linux , Juniper vMX Resource requirements 1 2 8 GB Topology file vr02.clab.yml Name vr02 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , vr-vmx:20.2R1.10 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Juniper vMX via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Juniper vMX VM is launched as a container, using vrnetlab integration . Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper vMX network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Juniper vMX"},{"location":"lab-examples/vr-vmx/#description","text":"A lab consists of an SR Linux node connected with Juniper vMX via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Juniper vMX VM is launched as a container, using vrnetlab integration .","title":"Description"},{"location":"lab-examples/vr-vmx/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper vMX network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/vr-xrv/","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRv Components Nokia SR Linux , Cisco XRv Resource requirements 1 1 3 GB Topology file vr03.clab.yml Name vr03 Version information 2 containerlab:0.9.0 , srlinux:20.6.3-145 , vr-xrv:6.1.2 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Cisco XRv via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Cisco XRv VM is launched as a container, using vrnetlab integration . Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Cisco XRv"},{"location":"lab-examples/vr-xrv/#description","text":"A lab consists of an SR Linux node connected with Cisco XRv via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Cisco XRv VM is launched as a container, using vrnetlab integration .","title":"Description"},{"location":"lab-examples/vr-xrv/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/vr-xrv9k/","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRv9k Components Nokia SR Linux , Cisco XRv9k Resource requirements 1 2 12 GB Topology file vr04.clab.yml Name vr04 Version information 2 containerlab:0.9.5 , srlinux:20.6.3-145 , vr-xrv9k:7.2.1 , docker-ce:19.03.13 Description # A lab consists of an SR Linux node connected with Cisco XRv9k via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Cisco XRv9k VM is launched as a container, using vrnetlab integration . Use cases # This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv9k network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Nokia SR Linux and Cisco XRv9k"},{"location":"lab-examples/vr-xrv9k/#description","text":"A lab consists of an SR Linux node connected with Cisco XRv9k via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the clab docker network. Cisco XRv9k VM is launched as a container, using vrnetlab integration .","title":"Description"},{"location":"lab-examples/vr-xrv9k/#use-cases","text":"This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv9k network operating systems. The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. Resource requirements are provisional. Consult with the installation guides for additional information. \u21a9 The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process. \u21a9","title":"Use cases"},{"location":"lab-examples/wan/","text":"Description WAN emulating topology Components Nokia SR Linux Resource requirements 1 2 3 GB Topology file srl03.clab.yml Name srl03 Description # Nokia SR Linux while focusing on the data center deployments in the first releases, will also be suitable for WAN deployments. In this lab users presented with a small WAN topology of four interconnected SR Linux nodes with multiple p2p interfaces between them. Use cases # The WAN-centric scenarios can be tested with this lab: Link aggregation WAN protocols and features Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"WAN topology"},{"location":"lab-examples/wan/#description","text":"Nokia SR Linux while focusing on the data center deployments in the first releases, will also be suitable for WAN deployments. In this lab users presented with a small WAN topology of four interconnected SR Linux nodes with multiple p2p interfaces between them.","title":"Description"},{"location":"lab-examples/wan/#use-cases","text":"The WAN-centric scenarios can be tested with this lab: Link aggregation WAN protocols and features Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information. \u21a9","title":"Use cases"},{"location":"manual/cert/","text":"As more and more services move to \"secure by default\" behavior, it becomes important to simplify the PKI/TLS infrastructure provisioning in the lab environments. Containerlab embeds parts of cfssl project to automate certificate generation and provisioning. For SR Linux nodes containerlab creates Certificate Authority and generates signed cert and key for each node of a lab. This makes SR Linux node to boot up with TLS profiles correctly configured and enable operation of a secured management protocol - gNMI. For other nodes the automated TLS pipeline is not provided yet and can be addressed by contributors. Apart from automated pipeline for certificate provisioning, containerlab exposes the following commands that can create a CA and node's cert/key: tools cert ca create - creates a Certificate Authority tools cert sign - creates certificate/key for a host and signs the certificate with CA With these two commands users can easily create CA node certificates and secure the transport channel of various protocols. This lab from our demonstrates how with containerlab help one can easily create certificates and configure Nokia SR OS to use it for secured gNMI communication.","title":"Certificate management"},{"location":"manual/conf-artifacts/","text":"When containerlab deploys a lab it creates a Lab Directory in the current working directory . This directory is used to keep all the necessary files that are needed to run/configure the nodes. We call these files configuration artifacts . Things like: Root CA certificate and node' TLS certificate and private keys node config file (if applicable) node-specific files and directories that are required to launch the container license files if needed all these artifacts will be available under a Lab Directory. Identifying a lab directory # The lab directory name follows the clab-<lab_name> template. Thus, if the name of your lab is srl02 you will find the clab-srl02 directory created in the current working directory. \u276f ls -lah clab-srl02 total 4.0K drwxr-xr-x 5 root root 40 Dec 1 22:11 . drwxr-xr-x 23 root root 4.0K Dec 1 22:11 .. drwxr-xr-x 5 root root 42 Dec 1 22:11 ca drwxr-xr-x 3 root root 79 Dec 1 22:11 srl1 drwxr-xr-x 3 root root 79 Dec 1 22:11 srl2 The contents of this directory will contain kind-specific files and directories. Containerlab will name directories after the node names and will only created those if they are needed. For instance, by default any node of kind linux will not have it's own directory under the Lab Directory. Persistance of a lab directory # When a user first deploy a lab, the Lab Directory gets created. Depending on a node's kind, this directory might act as a persistent storage area for a node. A common case is having the configuration file saved when the changes are made to the node via management interfaces. Below is an example of the srl1 node directory contents. It keeps a directory that is mounted to containers configuration path, as well as stores additional files needed to launch and configure the node. ~/clab/clab-srl02 \u276f ls -lah srl1 drwxrwxrwx+ 6 1002 1002 87 Dec 1 22:11 config -rw-r--r-- 1 root root 2.8K Dec 1 22:11 license.key -rw-r--r-- 1 root root 4.4K Dec 1 22:11 srlinux.conf -rw-r--r-- 1 root root 233 Dec 1 22:11 topology.clab.yml When a user destroys a lab without providing the --cleanup flag to the destroy command, the Lab Directory does not get deleted. This means that every configuration artefact will be kept on disk. Moreover, when the user will deploy the same lab, containerlab will reuse the configuration artifacts if possible, which will, for example, start the nodes with the config files saved from the previous lab run. To be able to deploy a lab without reusing existing configuration artefact use the --reconfigure flag with deploy command. With that setting, containerlab will first delete the Lab Directory and then will start the deployment process.","title":"Configuration artifacts"},{"location":"manual/conf-artifacts/#identifying-a-lab-directory","text":"The lab directory name follows the clab-<lab_name> template. Thus, if the name of your lab is srl02 you will find the clab-srl02 directory created in the current working directory. \u276f ls -lah clab-srl02 total 4.0K drwxr-xr-x 5 root root 40 Dec 1 22:11 . drwxr-xr-x 23 root root 4.0K Dec 1 22:11 .. drwxr-xr-x 5 root root 42 Dec 1 22:11 ca drwxr-xr-x 3 root root 79 Dec 1 22:11 srl1 drwxr-xr-x 3 root root 79 Dec 1 22:11 srl2 The contents of this directory will contain kind-specific files and directories. Containerlab will name directories after the node names and will only created those if they are needed. For instance, by default any node of kind linux will not have it's own directory under the Lab Directory.","title":"Identifying a lab directory"},{"location":"manual/conf-artifacts/#persistance-of-a-lab-directory","text":"When a user first deploy a lab, the Lab Directory gets created. Depending on a node's kind, this directory might act as a persistent storage area for a node. A common case is having the configuration file saved when the changes are made to the node via management interfaces. Below is an example of the srl1 node directory contents. It keeps a directory that is mounted to containers configuration path, as well as stores additional files needed to launch and configure the node. ~/clab/clab-srl02 \u276f ls -lah srl1 drwxrwxrwx+ 6 1002 1002 87 Dec 1 22:11 config -rw-r--r-- 1 root root 2.8K Dec 1 22:11 license.key -rw-r--r-- 1 root root 4.4K Dec 1 22:11 srlinux.conf -rw-r--r-- 1 root root 233 Dec 1 22:11 topology.clab.yml When a user destroys a lab without providing the --cleanup flag to the destroy command, the Lab Directory does not get deleted. This means that every configuration artefact will be kept on disk. Moreover, when the user will deploy the same lab, containerlab will reuse the configuration artifacts if possible, which will, for example, start the nodes with the config files saved from the previous lab run. To be able to deploy a lab without reusing existing configuration artefact use the --reconfigure flag with deploy command. With that setting, containerlab will first delete the Lab Directory and then will start the deployment process.","title":"Persistance of a lab directory"},{"location":"manual/inventory/","text":"To accommodate for smooth transition from lab deployment to subsequent automation activities, containerlab generates inventory files for different automation tools. Ansible # Ansible inventory is generated automatically for every lab. The inventory file can be found in the lab directory under the ansible-inventory.yml name. Lab nodes are grouped under their kinds in the inventory so that the users can selectively choose the right group of nodes in the playbooks. topology file name : ansible topology : nodes : r1 : kind : crpd image : crpd:latest r2 : kind : ceos image : ceos:latest r3 : kind : ceos image : ceos:latest grafana : kind : linux image : grafana/grafana:7.4.3 generated ansible inventory all : children : crpd : hosts : clab-ansible-r1 : ansible_host : <mgmt-ipv4-address> ceos : hosts : clab-ansible-r2 : ansible_host : <mgmt-ipv4-address> clab-ansible-r3 : ansible_host : <mgmt-ipv4-address> linux : hosts : clab-ansible-grafana : ansible_host : <mgmt-ipv4-address>","title":"Inventory"},{"location":"manual/inventory/#ansible","text":"Ansible inventory is generated automatically for every lab. The inventory file can be found in the lab directory under the ansible-inventory.yml name. Lab nodes are grouped under their kinds in the inventory so that the users can selectively choose the right group of nodes in the playbooks. topology file name : ansible topology : nodes : r1 : kind : crpd image : crpd:latest r2 : kind : ceos image : ceos:latest r3 : kind : ceos image : ceos:latest grafana : kind : linux image : grafana/grafana:7.4.3 generated ansible inventory all : children : crpd : hosts : clab-ansible-r1 : ansible_host : <mgmt-ipv4-address> ceos : hosts : clab-ansible-r2 : ansible_host : <mgmt-ipv4-address> clab-ansible-r3 : ansible_host : <mgmt-ipv4-address> linux : hosts : clab-ansible-grafana : ansible_host : <mgmt-ipv4-address>","title":"Ansible"},{"location":"manual/multi-node/","text":"Multi-node labs # Containerlab is a perfect tool of choice when all the lab components/nodes fit into one VM or bare metal server. Unfortunately, sometimes it is hard to satisfy this requirement and fit a big and sophisticated lab on a single host. Although containerlab is not (yet) capable of deploying topologies over a number of container hosts, we have embedded some capabilities that can help you to workaround the single-host resources constraint. Exposing services # Sometimes all that is needed is to make certain services running inside the nodes launched with containerlab availalbe to a system running outside of the container host. For example, you might have an already running telemetry stack somewhere in your lab and you want to use it with the routing systems deployed with containerlab. In that case, the simple solution would be to expose the nodes' ports which are used to collect telemetry information. Take a look the following example where two nodes are defined in the topology file and get their gNMI port exposed to a host under a user-defined host-port. name : telemetry topology : nodes : ceos : kind : ceos image : ceos:latest ports : # host port 57401 is mapped to port 57400 of ceos node - 57401:57400 srl : kind : srl image : srl:latest license : lic.txt ports : - 57402:57400 links : - endpoints : [ \"ceos:eth1\" , \"srl:e1-1\" ] Once the container's ports/services are exposed to a host under host-port, the telemetry collector running outside of the container host system can reach each node gNMI service. If container host has IP address of $IP , then telemetry collector can reach ceos telemetry service by $IP:57401 address and srl gNMI service will be reachable via $IP:57402 . Exposing management network # Exposing services on a per-port basis as shown above is a quick and easy way to make a certain service available via a host port, likely being the most common way of exposing services with containerlab. Unfortunately, not every use case can be covered with such approach. Imagine if you want to integrate an NMS system running elsewhere with a lab you launched with containerlab. Typically you would need to expose the entire management network for an NMS to start managing the nodes with management protocols required. In this scenario you wouldn't get far with exposing services via host-ports, as NMS would expect to have IP connectivity with the node it is about to adopt for managing. For integration tasks like this containerlab users can levelrage static routing towards containerlab management network . Consider the following diagram: This solution requires to set up roting between the host which runs the NMS and the container host that has containerlab nodes inside. Since containers are always attached to a common management network, we can make this network reachable by installing, for example, a static route on the NMS host. This will provision the datapath between the NMS and the containerlab management network. By default, containerlab management network is addressed with 172.20.20./0 IPv4 address, but this can be easily changed to accomodate for network environment. Bridging # Previous examples were aiming management network access, but what if we need to rather connect a network interfaces of a certain node with a system running outside of the container host? An example for such connectivity requirement could be a traffic generator connected to a containerized node port. In this case we can leverage the bridge kind 1 that containerlab offers to connect container' interface to a pre-created bridge and slice the network with VLANs to create a L2 connectivity between the ports: VxLAN Tunneling # Sometimes VLAN bridging is not possible, for example when the other end of the virtual wire is reachable via routing, not bridging. We have developed a semi-automated solution for this case as well. The idea is to create unicast VxLAN tunnels between the VMs hosting nodes requiring connectivity. Refer to the multinode lab that goes deep in details on how to create this tunneling and explains the technicalities of such dataplane. Both regular linux bridge and ovs-bridge kinds can be used, depending on the requirements. \u21a9","title":"Multi-node labs"},{"location":"manual/multi-node/#multi-node-labs","text":"Containerlab is a perfect tool of choice when all the lab components/nodes fit into one VM or bare metal server. Unfortunately, sometimes it is hard to satisfy this requirement and fit a big and sophisticated lab on a single host. Although containerlab is not (yet) capable of deploying topologies over a number of container hosts, we have embedded some capabilities that can help you to workaround the single-host resources constraint.","title":"Multi-node labs"},{"location":"manual/multi-node/#exposing-services","text":"Sometimes all that is needed is to make certain services running inside the nodes launched with containerlab availalbe to a system running outside of the container host. For example, you might have an already running telemetry stack somewhere in your lab and you want to use it with the routing systems deployed with containerlab. In that case, the simple solution would be to expose the nodes' ports which are used to collect telemetry information. Take a look the following example where two nodes are defined in the topology file and get their gNMI port exposed to a host under a user-defined host-port. name : telemetry topology : nodes : ceos : kind : ceos image : ceos:latest ports : # host port 57401 is mapped to port 57400 of ceos node - 57401:57400 srl : kind : srl image : srl:latest license : lic.txt ports : - 57402:57400 links : - endpoints : [ \"ceos:eth1\" , \"srl:e1-1\" ] Once the container's ports/services are exposed to a host under host-port, the telemetry collector running outside of the container host system can reach each node gNMI service. If container host has IP address of $IP , then telemetry collector can reach ceos telemetry service by $IP:57401 address and srl gNMI service will be reachable via $IP:57402 .","title":"Exposing services"},{"location":"manual/multi-node/#exposing-management-network","text":"Exposing services on a per-port basis as shown above is a quick and easy way to make a certain service available via a host port, likely being the most common way of exposing services with containerlab. Unfortunately, not every use case can be covered with such approach. Imagine if you want to integrate an NMS system running elsewhere with a lab you launched with containerlab. Typically you would need to expose the entire management network for an NMS to start managing the nodes with management protocols required. In this scenario you wouldn't get far with exposing services via host-ports, as NMS would expect to have IP connectivity with the node it is about to adopt for managing. For integration tasks like this containerlab users can levelrage static routing towards containerlab management network . Consider the following diagram: This solution requires to set up roting between the host which runs the NMS and the container host that has containerlab nodes inside. Since containers are always attached to a common management network, we can make this network reachable by installing, for example, a static route on the NMS host. This will provision the datapath between the NMS and the containerlab management network. By default, containerlab management network is addressed with 172.20.20./0 IPv4 address, but this can be easily changed to accomodate for network environment.","title":"Exposing management network"},{"location":"manual/multi-node/#bridging","text":"Previous examples were aiming management network access, but what if we need to rather connect a network interfaces of a certain node with a system running outside of the container host? An example for such connectivity requirement could be a traffic generator connected to a containerized node port. In this case we can leverage the bridge kind 1 that containerlab offers to connect container' interface to a pre-created bridge and slice the network with VLANs to create a L2 connectivity between the ports:","title":"Bridging"},{"location":"manual/multi-node/#vxlan-tunneling","text":"Sometimes VLAN bridging is not possible, for example when the other end of the virtual wire is reachable via routing, not bridging. We have developed a semi-automated solution for this case as well. The idea is to create unicast VxLAN tunnels between the VMs hosting nodes requiring connectivity. Refer to the multinode lab that goes deep in details on how to create this tunneling and explains the technicalities of such dataplane. Both regular linux bridge and ovs-bridge kinds can be used, depending on the requirements. \u21a9","title":"VxLAN Tunneling"},{"location":"manual/network/","text":"One of the most important tasks in the process of building container based labs is to create a virtual wiring between the containers and the host. That is one of the problems that containerlab was designed to solve. In this document we will discuss the networking concepts that containerlab employs to provide the following connectivity scenarios: Make containers available from the lab host Interconnect containers to create network topologies of users choice Management network # As governed by the well-established container networking principles containers are able to get network connectivity using various drivers/methods. The most common networking driver that is enabled by default for docker-managed containers is the bridge driver . The bridge driver connects containers to a linux bridge interface named docker0 on most linux operating systems. The containers are then able to communicate with each other and the host via this virtual switch (bridge interface). In containerlab we follow a similar approach: containers launched by containerlab will be attached with their interface to a containerlab-managed docker network. It's best to be explained by an example which we will base on a two nodes lab from our catalog: name : srl02 topology : kinds : srl : type : ixr6 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] As seen from the topology definition file, the lab consists of the two SR Linux nodes which are interconnected via a single point-to-point link. The diagram above shows that these two nodes are not only interconnected between themselves, but also connected to a bridge interface on the lab host. This is driven by the containerlab default management network settings. default settings # When no information about the management network is provided within the topo definition file, containerlab will do the following create, if not already created, a docker network named clab configure the IPv4/6 addressing pertaining to this docker network Info We often refer to clab docker network simply as management network since its the network to which management interfaces of the containerized NOS'es are connected. The addressing information that containerlab will use on this network: IPv4: subnet 172.20.20.0/24, gateway 172.20.20.1 IPv6: subnet 2001:172:20:20::/80, gateway 2001:172:20:20::1 This management network will be configured with MTU value matching the value of a docker0 host interface to match docker configuration on the system. This option is configurable . With these defaults in place, the two containers from this lab will get connected to that management network and will be able to communicate using the IP addresses allocated by docker daemon. The addresses that docker carves out for each container are presented to a user once the lab deployment finishes or can be queried any time after: # addressing information is available once the lab deployment completes \u276f containerlab deploy -t srl02.clab.yml # deployment log omitted for brevity +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | | 2 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ # addresses can also be fetched afterwards with `inspect` command \u276f containerlab inspect -a +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Lab Name | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | srl02 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | | 2 | srl02 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ The output above shows that srl1 container has been assigned 172.20.20.3/24 / 2001:172:20:20::3/80 IPv4/6 address. We can ensure this by querying the srl1 management interfaces address info: \u276f docker exec clab-srl02-srl1 ip address show dummy-mgmt0 6 : dummy-mgmt0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 2a:66:2b:09:2e:4d brd ff:ff:ff:ff:ff:ff inet 172 .20.20.3/24 brd 172 .20.20.255 scope global dummy-mgmt0 valid_lft forever preferred_lft forever inet6 2001 :172:20:20::3/80 scope global valid_lft forever preferred_lft forever Now it's possible to reach the assigned IP address from the lab host as well as from other containers connected to this management network. # ping srl1 management interface from srl2 \u276f docker exec -it clab-srl02-srl2 sr_cli \"ping 172.20.20.3 network-instance mgmt\" x -> $176 x48 Using network instance mgmt PING 172 .20.20.3 ( 172 .20.20.3 ) 56 ( 84 ) bytes of data. 64 bytes from 172 .20.20.3: icmp_seq = 1 ttl = 64 time = 2 .43 ms Note If you run multiple labs without changing the default management settings, the containers of those labs will end up connecting to the same management network with their management interface. configuring management network # Most of the time there is no need to change the defaults for management network configuration, but sometimes it is needed. For example, it might be that the default network ranges are overlapping with existing addressing scheme on the lab host or it might be desirable to have predefined management IP addresses. For such cases the users need to add the mgmt container at the top level of their topology definition file: name : srl02 mgmt : network : custom_mgmt # management network name ipv4_subnet : 172.100.100.0/24 # ipv4 range ipv6_subnet : 2001:172:100:100::/80 # ipv6 range (optional) topology : # the rest of the file is omitted for brevity With this settings in place container will get their IP addresses from the specified ranges accordingly. user-defined addresses # By default container runtime will assign the management IP addresses for the containers. But sometimes it's useful to have a user-defined addressing in the management network. For such cases users can define the desired IPv4/6 addresses on a per-node basis: mgmt : network : fixedips ipv4_subnet : 172.100.100.0/24 ipv6_subnet : 2001:172:100:100::/80 topology : nodes : n1 : kind : srl mgmt_ipv4 : 172.100.100.11 # set ipv4 address on management network mgmt_ipv6 : 2001:172:100:100::11 # set ipv6 address on management network Users can specify either IPv4 or IPv6 or both addresses, if one of the addresses is omitted, it will be assigned by container runtime in an arbitrary fashion. Note If user-defined IP addresses are needed, they must be provided for all containers attached to a given network to avoid address collision. IPv4/6 addresses set on a node level must be from the management network range. MTU # The MTU of the management network defaults to an MTU value of docker0 interface, but it can be set to a user defined value: mgmt : network : clab_mgmt mtu : 2100 # set mtu of the management network to 2100 This will result in every interface connected to that network to inherit this MTU value. default docker network # To make clab nodes to start in the default docker network bridge which uses the docker0 bridge interface, users need to mention this explicitly in the configuration: mgmt : network : bridge Since bridge network is created by default by docker, using its name in the configuration will make nodes to connect to this network. connection details # When containerlab needs to create the management network it asks the docker daemon to do this. Docker will fullfil the request and will create a network with the underlying linux bridge interface backing it. The bridge interface name is generated by the docker daemon, but it is easy to find it: # list existing docker networks # notice the presence of the `clab` network with a `bridge` driver \u276f docker network ls NETWORK ID NAME DRIVER SCOPE 5d60b6ec8420 bridge bridge local d2169a14e334 clab bridge local 58ec5037122a host host local 4c1491a09a1a none null local # the underlying linux bridge interface name follows the `br-<first_12_chars_of_docker_network_id> pattern # to find the network ID use: \u276f docker network inspect clab -f {{ .ID }} | head -c 12 d2169a14e334 # now the name is known and its easy to show bridge state \u276f brctl show br-d2169a14e334 bridge name bridge id STP enabled interfaces br-d2169a14e334 8000 .0242fe382b74 no vetha57b950 vethe9da10a As explained in the beginning of this article, containers will connect to this docker network. This connection is carried out by the veth devices created and attached with one end to bridge interface in the lab host and the other end in the container namespace. This is illustrated by the bridge output above and the diagram at the beginning the of the article. Point-to-point links # Management network is used to provide management access to the NOS containers, it does not carry control or dataplane traffic. In containerlab we create additional point-to-point links between the containers to provide the datapath between the lab nodes. The above diagram shows how links are created in the topology definition file. In this example, the datapath consists of the two virtual point-to-point wires between SR Linux and cEOS containers. These links are created on-demand by containerlab itself. The p2p links are provided by the veth device pairs where each end of the veth pair is attached to a respective container. The MTU on these veth links is set to 65000, so a regular 9212 MTU on the network links shouldn't be a problem. host links # It is also possible to interconnect container' data inteface not with other container or add it to a bridge , but to attach it to a host's root namespace. This is, for example, needed to create a L2 connectivity between containerlab nodes running on different VMs (aka multi-node labs). This \"host-connectivity\" is achieved by using a reserved node name - host - referenced in the endpoints section. Consider the following example where an SR Linux container has its only data interface connected to a hosts root namespace via veth interface: name : host topology : nodes : srl : kind : srl image : srlinux:20.6.3-145 license : license.key config : test-srl-config.json links : - endpoints : [ \"srl:e1-1\" , \"host:srl_e1-1\" ] With this topology definition, we will have a veth interface with its one end in the container' namespace and its other end in the host namespace. The host will have the interface named srl_e1-1 once the lab deployed: ip link # SNIP 433 : srl_e1-1@if434: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether b2:80:e9:60:c7:9d brd ff:ff:ff:ff:ff:ff link-netns clab-srl01-srl","title":"Network wiring concepts"},{"location":"manual/network/#management-network","text":"As governed by the well-established container networking principles containers are able to get network connectivity using various drivers/methods. The most common networking driver that is enabled by default for docker-managed containers is the bridge driver . The bridge driver connects containers to a linux bridge interface named docker0 on most linux operating systems. The containers are then able to communicate with each other and the host via this virtual switch (bridge interface). In containerlab we follow a similar approach: containers launched by containerlab will be attached with their interface to a containerlab-managed docker network. It's best to be explained by an example which we will base on a two nodes lab from our catalog: name : srl02 topology : kinds : srl : type : ixr6 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl links : - endpoints : [ \"srl1:e1-1\" , \"srl2:e1-1\" ] As seen from the topology definition file, the lab consists of the two SR Linux nodes which are interconnected via a single point-to-point link. The diagram above shows that these two nodes are not only interconnected between themselves, but also connected to a bridge interface on the lab host. This is driven by the containerlab default management network settings.","title":"Management network"},{"location":"manual/network/#default-settings","text":"When no information about the management network is provided within the topo definition file, containerlab will do the following create, if not already created, a docker network named clab configure the IPv4/6 addressing pertaining to this docker network Info We often refer to clab docker network simply as management network since its the network to which management interfaces of the containerized NOS'es are connected. The addressing information that containerlab will use on this network: IPv4: subnet 172.20.20.0/24, gateway 172.20.20.1 IPv6: subnet 2001:172:20:20::/80, gateway 2001:172:20:20::1 This management network will be configured with MTU value matching the value of a docker0 host interface to match docker configuration on the system. This option is configurable . With these defaults in place, the two containers from this lab will get connected to that management network and will be able to communicate using the IP addresses allocated by docker daemon. The addresses that docker carves out for each container are presented to a user once the lab deployment finishes or can be queried any time after: # addressing information is available once the lab deployment completes \u276f containerlab deploy -t srl02.clab.yml # deployment log omitted for brevity +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | | 2 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | +---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ # addresses can also be fetched afterwards with `inspect` command \u276f containerlab inspect -a +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | # | Lab Name | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ | 1 | srl02 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl | | running | 172 .20.20.3/24 | 2001 :172:20:20::3/80 | | 2 | srl02 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl | | running | 172 .20.20.2/24 | 2001 :172:20:20::2/80 | +---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+ The output above shows that srl1 container has been assigned 172.20.20.3/24 / 2001:172:20:20::3/80 IPv4/6 address. We can ensure this by querying the srl1 management interfaces address info: \u276f docker exec clab-srl02-srl1 ip address show dummy-mgmt0 6 : dummy-mgmt0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 2a:66:2b:09:2e:4d brd ff:ff:ff:ff:ff:ff inet 172 .20.20.3/24 brd 172 .20.20.255 scope global dummy-mgmt0 valid_lft forever preferred_lft forever inet6 2001 :172:20:20::3/80 scope global valid_lft forever preferred_lft forever Now it's possible to reach the assigned IP address from the lab host as well as from other containers connected to this management network. # ping srl1 management interface from srl2 \u276f docker exec -it clab-srl02-srl2 sr_cli \"ping 172.20.20.3 network-instance mgmt\" x -> $176 x48 Using network instance mgmt PING 172 .20.20.3 ( 172 .20.20.3 ) 56 ( 84 ) bytes of data. 64 bytes from 172 .20.20.3: icmp_seq = 1 ttl = 64 time = 2 .43 ms Note If you run multiple labs without changing the default management settings, the containers of those labs will end up connecting to the same management network with their management interface.","title":"default settings"},{"location":"manual/network/#configuring-management-network","text":"Most of the time there is no need to change the defaults for management network configuration, but sometimes it is needed. For example, it might be that the default network ranges are overlapping with existing addressing scheme on the lab host or it might be desirable to have predefined management IP addresses. For such cases the users need to add the mgmt container at the top level of their topology definition file: name : srl02 mgmt : network : custom_mgmt # management network name ipv4_subnet : 172.100.100.0/24 # ipv4 range ipv6_subnet : 2001:172:100:100::/80 # ipv6 range (optional) topology : # the rest of the file is omitted for brevity With this settings in place container will get their IP addresses from the specified ranges accordingly.","title":"configuring management network"},{"location":"manual/network/#user-defined-addresses","text":"By default container runtime will assign the management IP addresses for the containers. But sometimes it's useful to have a user-defined addressing in the management network. For such cases users can define the desired IPv4/6 addresses on a per-node basis: mgmt : network : fixedips ipv4_subnet : 172.100.100.0/24 ipv6_subnet : 2001:172:100:100::/80 topology : nodes : n1 : kind : srl mgmt_ipv4 : 172.100.100.11 # set ipv4 address on management network mgmt_ipv6 : 2001:172:100:100::11 # set ipv6 address on management network Users can specify either IPv4 or IPv6 or both addresses, if one of the addresses is omitted, it will be assigned by container runtime in an arbitrary fashion. Note If user-defined IP addresses are needed, they must be provided for all containers attached to a given network to avoid address collision. IPv4/6 addresses set on a node level must be from the management network range.","title":"user-defined addresses"},{"location":"manual/network/#mtu","text":"The MTU of the management network defaults to an MTU value of docker0 interface, but it can be set to a user defined value: mgmt : network : clab_mgmt mtu : 2100 # set mtu of the management network to 2100 This will result in every interface connected to that network to inherit this MTU value.","title":"MTU"},{"location":"manual/network/#default-docker-network","text":"To make clab nodes to start in the default docker network bridge which uses the docker0 bridge interface, users need to mention this explicitly in the configuration: mgmt : network : bridge Since bridge network is created by default by docker, using its name in the configuration will make nodes to connect to this network.","title":"default docker network"},{"location":"manual/network/#connection-details","text":"When containerlab needs to create the management network it asks the docker daemon to do this. Docker will fullfil the request and will create a network with the underlying linux bridge interface backing it. The bridge interface name is generated by the docker daemon, but it is easy to find it: # list existing docker networks # notice the presence of the `clab` network with a `bridge` driver \u276f docker network ls NETWORK ID NAME DRIVER SCOPE 5d60b6ec8420 bridge bridge local d2169a14e334 clab bridge local 58ec5037122a host host local 4c1491a09a1a none null local # the underlying linux bridge interface name follows the `br-<first_12_chars_of_docker_network_id> pattern # to find the network ID use: \u276f docker network inspect clab -f {{ .ID }} | head -c 12 d2169a14e334 # now the name is known and its easy to show bridge state \u276f brctl show br-d2169a14e334 bridge name bridge id STP enabled interfaces br-d2169a14e334 8000 .0242fe382b74 no vetha57b950 vethe9da10a As explained in the beginning of this article, containers will connect to this docker network. This connection is carried out by the veth devices created and attached with one end to bridge interface in the lab host and the other end in the container namespace. This is illustrated by the bridge output above and the diagram at the beginning the of the article.","title":"connection details"},{"location":"manual/network/#point-to-point-links","text":"Management network is used to provide management access to the NOS containers, it does not carry control or dataplane traffic. In containerlab we create additional point-to-point links between the containers to provide the datapath between the lab nodes. The above diagram shows how links are created in the topology definition file. In this example, the datapath consists of the two virtual point-to-point wires between SR Linux and cEOS containers. These links are created on-demand by containerlab itself. The p2p links are provided by the veth device pairs where each end of the veth pair is attached to a respective container. The MTU on these veth links is set to 65000, so a regular 9212 MTU on the network links shouldn't be a problem.","title":"Point-to-point links"},{"location":"manual/network/#host-links","text":"It is also possible to interconnect container' data inteface not with other container or add it to a bridge , but to attach it to a host's root namespace. This is, for example, needed to create a L2 connectivity between containerlab nodes running on different VMs (aka multi-node labs). This \"host-connectivity\" is achieved by using a reserved node name - host - referenced in the endpoints section. Consider the following example where an SR Linux container has its only data interface connected to a hosts root namespace via veth interface: name : host topology : nodes : srl : kind : srl image : srlinux:20.6.3-145 license : license.key config : test-srl-config.json links : - endpoints : [ \"srl:e1-1\" , \"host:srl_e1-1\" ] With this topology definition, we will have a veth interface with its one end in the container' namespace and its other end in the host namespace. The host will have the interface named srl_e1-1 once the lab deployed: ip link # SNIP 433 : srl_e1-1@if434: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether b2:80:e9:60:c7:9d brd ff:ff:ff:ff:ff:ff link-netns clab-srl01-srl","title":"host links"},{"location":"manual/nodes/","text":"Node object is one of the containerlab' pillars. Essentially, it is nodes and links what constitute the lab topology. To let users build flexible and customizable labs the nodes are meant to be configurable. The node configuration is part of the topology definition file and may consist of the following fields that we explain in details below. # part of topology definition file topology : nodes : node1 : # node name kind : srl type : ixrd2 image : srlinux license : license.key config : /root/mylab/node1.cfg binds : - /usr/local/bin/gobgp:/root/gobgp - /root/files:/root/files:ro ports : - 80:8080 - 55555:43555/udp - 55554:43554/tcp user : test env : ENV1 : VAL1 cmd : /bin/bash script.sh kind # The kind property selects which kind this node is of. Kinds are essentially a way of telling containerlab how to treat the nodes properties considering the specific flavor of the node. We dedicated a separate section to discuss kinds in details. Note Kind must be defined either by setting the kind for a node specifically (as in the example above), or by setting the default kind: topology : defaults : kind : srl nodes : node1 : # kind value of `srl` is inherited from defaults section type # With type the user sets a type of the node. Types work in combination with the kinds, such as the type value of ixrd2 sets the chassis type for SR Linux node, thus this value only makes sense to nodes of kind srl . Other nodes might treat type field differently, that will depend on the kind of the node. The type values and effects defined in the documentation for a specific kind. image # The common image attribute sets the container image name that will be used to start the node. The image name should be provided in a well-known format of repository(:tag) . We use <repository> image name throughout the docs articles. This means that the image with <repository>:latest name will be looked up. A user will need to add the latest tag if they want to use the same loose-tag naming: # tagging srlinux:20.6.1-286 as srlinux:latest # after this change its possible to use `srlinux:latest` or `srlinux` image name docker tag srlinux:20.6.1-286 srlinux:latest license # Some containerized NOSes require a license to operate. With license property a user sets a path to a license file that a node will use. The license file will then be mounted to the container by the path that is defined by the kind/type of the node. config # For the specific kinds its possible to pass a path to a config template file that a node will use. The template engine is Go template . The srlconfig.tpl template is used by default for Nokia SR Linux nodes and can be used to create configuration templates for SR Linux nodes. Supported for: Nokia SR Linux. binds # In order to expose host files to the containerized nodes a user can leverage the bind mount capability. Provide a list of binds instructions under the binds container of the node configuration. The string format of those binding instructions follow the same rules as the --volume parameter of the docker/podman CLI. binds : # mount a file from a host to a container (implicit RW mode) - /usr/local/bin/gobgp:/root/gobgp # mount a directory from a host to a container in RO mode - /root/files:/root/files:ro ports # To bind the ports between the lab host and the containers the users can populate the ports object inside the node: ports : - 80:8080 # tcp port 80 of the host is mapped to port 8080 of the container - 55555:43555/udp - 55554:43554/tcp The list of port bindings consists of strings in the same format that is acceptable by docker run command's -p/--export flag . This option is only configurable under the node level. env # To add environment variables to a node use the env container that can be added at defaults , kind and node levels. The variables values are merged when the same vars are defined on multiple levels with nodes level being the most specific. topology : defaults : env : ENV1 : 3 # ENV1=3 will be set if its not set on kind or node level ENV2 : glob # ENV2=glob will be set for all nodes kinds : srl : env : ENV1 : 2 # ENV1=2 will be set to if its not set on node level ENV3 : kind # ENV3=kind will be set for all nodes of srl kind nodes : node1 : env : ENV1 : 1 # ENV1=1 will be set for node1 user # To set a user which will be used to run a containerized process use the user configuration option. Can be defined at node , kind and global levels. topology : defaults : user : alice # alice user will be used for all nodes unless set on kind or node levels kinds : srl : user : bob # bob user will be used for nodes of kind srl unless it is set on node level nodes : node1 : user : clab # clab user will be used for node1 cmd # It is possible to set/override the command of the container image with cmd configuration option. It accepts the \"shell\" form and can be set on all levels. topology : defaults : cmd : bash cmd.sh kinds : srl : cmd : bash cmd2.sh nodes : node1 : cmd : bash cmd3.sh labels # To add container labels to a node use the labels container that can be added at defaults , kind and node levels. The label values are merged when the same vars are defined on multiple levels with nodes level being the most specific. Consider the following example, where labels are defined on different levels to show value propagation. topology : defaults : labels : label1 : value1 label2 : value2 kinds : srl : env : label1 : kind_value1 label3 : value3 nodes : node1 : env : label1 : node_value1 As a result of such label distribution, node1 will have the following labels: label1: node_value1 # most specific label wins label2: value2 # inherited from defaults section label2: value3 # inherited from kinds section mgmt_ipv4 # To make a node to boot with a user-specified management IPv4 address, the mgmt_ipv4 setting can be used. Note, that the static management IP address should be part of the subnet that is used within the lab. Read more about user-defined management addresses here . nodes : r1 : kind : srl mgmt_ipv4 : 172.20.20.100 mgmt_ipv6 # To make a node to boot with a user-specified management IPv4 address, the mgmt_ipv6 setting can be used. Note, that the static management IP address should be part of the subnet that is used within the lab. Read more about user-defined management addresses here . nodes : r1 : kind : srl mgmt_ipv6 : 2001:172:20:20::100 publish # Container lab integrates with mysocket.io service to allow for private, Internet-reachable tunnels created for ports of containerlab nodes. This enables effortless access sharing with cusomters/partners/colleagues. This integration is extensively covered on Publish ports page. name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be published - tcp/57400 # tcp port 57400 will be published - http/8080 # http port 8080 will be published network-mode # By default containerlab nodes use bridge-mode driver - nodes are created with their first interface connected to a docker network (management network). It is possible to override this behavior and set the network mode to the value of host . # example node definition with host networking mode my-node : image : alpine:3 network-mode : host The network-mode configuration option set to host will launch the node in the host networking mode .","title":"Nodes"},{"location":"manual/nodes/#kind","text":"The kind property selects which kind this node is of. Kinds are essentially a way of telling containerlab how to treat the nodes properties considering the specific flavor of the node. We dedicated a separate section to discuss kinds in details. Note Kind must be defined either by setting the kind for a node specifically (as in the example above), or by setting the default kind: topology : defaults : kind : srl nodes : node1 : # kind value of `srl` is inherited from defaults section","title":"kind"},{"location":"manual/nodes/#type","text":"With type the user sets a type of the node. Types work in combination with the kinds, such as the type value of ixrd2 sets the chassis type for SR Linux node, thus this value only makes sense to nodes of kind srl . Other nodes might treat type field differently, that will depend on the kind of the node. The type values and effects defined in the documentation for a specific kind.","title":"type"},{"location":"manual/nodes/#image","text":"The common image attribute sets the container image name that will be used to start the node. The image name should be provided in a well-known format of repository(:tag) . We use <repository> image name throughout the docs articles. This means that the image with <repository>:latest name will be looked up. A user will need to add the latest tag if they want to use the same loose-tag naming: # tagging srlinux:20.6.1-286 as srlinux:latest # after this change its possible to use `srlinux:latest` or `srlinux` image name docker tag srlinux:20.6.1-286 srlinux:latest","title":"image"},{"location":"manual/nodes/#license","text":"Some containerized NOSes require a license to operate. With license property a user sets a path to a license file that a node will use. The license file will then be mounted to the container by the path that is defined by the kind/type of the node.","title":"license"},{"location":"manual/nodes/#config","text":"For the specific kinds its possible to pass a path to a config template file that a node will use. The template engine is Go template . The srlconfig.tpl template is used by default for Nokia SR Linux nodes and can be used to create configuration templates for SR Linux nodes. Supported for: Nokia SR Linux.","title":"config"},{"location":"manual/nodes/#binds","text":"In order to expose host files to the containerized nodes a user can leverage the bind mount capability. Provide a list of binds instructions under the binds container of the node configuration. The string format of those binding instructions follow the same rules as the --volume parameter of the docker/podman CLI. binds : # mount a file from a host to a container (implicit RW mode) - /usr/local/bin/gobgp:/root/gobgp # mount a directory from a host to a container in RO mode - /root/files:/root/files:ro","title":"binds"},{"location":"manual/nodes/#ports","text":"To bind the ports between the lab host and the containers the users can populate the ports object inside the node: ports : - 80:8080 # tcp port 80 of the host is mapped to port 8080 of the container - 55555:43555/udp - 55554:43554/tcp The list of port bindings consists of strings in the same format that is acceptable by docker run command's -p/--export flag . This option is only configurable under the node level.","title":"ports"},{"location":"manual/nodes/#env","text":"To add environment variables to a node use the env container that can be added at defaults , kind and node levels. The variables values are merged when the same vars are defined on multiple levels with nodes level being the most specific. topology : defaults : env : ENV1 : 3 # ENV1=3 will be set if its not set on kind or node level ENV2 : glob # ENV2=glob will be set for all nodes kinds : srl : env : ENV1 : 2 # ENV1=2 will be set to if its not set on node level ENV3 : kind # ENV3=kind will be set for all nodes of srl kind nodes : node1 : env : ENV1 : 1 # ENV1=1 will be set for node1","title":"env"},{"location":"manual/nodes/#user","text":"To set a user which will be used to run a containerized process use the user configuration option. Can be defined at node , kind and global levels. topology : defaults : user : alice # alice user will be used for all nodes unless set on kind or node levels kinds : srl : user : bob # bob user will be used for nodes of kind srl unless it is set on node level nodes : node1 : user : clab # clab user will be used for node1","title":"user"},{"location":"manual/nodes/#cmd","text":"It is possible to set/override the command of the container image with cmd configuration option. It accepts the \"shell\" form and can be set on all levels. topology : defaults : cmd : bash cmd.sh kinds : srl : cmd : bash cmd2.sh nodes : node1 : cmd : bash cmd3.sh","title":"cmd"},{"location":"manual/nodes/#labels","text":"To add container labels to a node use the labels container that can be added at defaults , kind and node levels. The label values are merged when the same vars are defined on multiple levels with nodes level being the most specific. Consider the following example, where labels are defined on different levels to show value propagation. topology : defaults : labels : label1 : value1 label2 : value2 kinds : srl : env : label1 : kind_value1 label3 : value3 nodes : node1 : env : label1 : node_value1 As a result of such label distribution, node1 will have the following labels: label1: node_value1 # most specific label wins label2: value2 # inherited from defaults section label2: value3 # inherited from kinds section","title":"labels"},{"location":"manual/nodes/#mgmt_ipv4","text":"To make a node to boot with a user-specified management IPv4 address, the mgmt_ipv4 setting can be used. Note, that the static management IP address should be part of the subnet that is used within the lab. Read more about user-defined management addresses here . nodes : r1 : kind : srl mgmt_ipv4 : 172.20.20.100","title":"mgmt_ipv4"},{"location":"manual/nodes/#mgmt_ipv6","text":"To make a node to boot with a user-specified management IPv4 address, the mgmt_ipv6 setting can be used. Note, that the static management IP address should be part of the subnet that is used within the lab. Read more about user-defined management addresses here . nodes : r1 : kind : srl mgmt_ipv6 : 2001:172:20:20::100","title":"mgmt_ipv6"},{"location":"manual/nodes/#publish","text":"Container lab integrates with mysocket.io service to allow for private, Internet-reachable tunnels created for ports of containerlab nodes. This enables effortless access sharing with cusomters/partners/colleagues. This integration is extensively covered on Publish ports page. name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be published - tcp/57400 # tcp port 57400 will be published - http/8080 # http port 8080 will be published","title":"publish"},{"location":"manual/nodes/#network-mode","text":"By default containerlab nodes use bridge-mode driver - nodes are created with their first interface connected to a docker network (management network). It is possible to override this behavior and set the network mode to the value of host . # example node definition with host networking mode my-node : image : alpine:3 network-mode : host The network-mode configuration option set to host will launch the node in the host networking mode .","title":"network-mode"},{"location":"manual/published-ports/","text":"Labs are typically deployed in the isolated environments, such as company's internal network, cloud region or even a laptop. The lab nodes can happily talk to each other and, if needed, can reach Internet in the outbound direction. But sometimes it is really needed to let your lab nodes be reachable over Internet securely and privately in the inbound direction. There are many use cases that warrant such publishing , some of the most common are: create a lab in your environment and share it with a customer/colleague on-demand make an interactive demo/training where nodes are shared with an audience for hands-on experience share a private lab with someone to collaborate or troubleshoot expose management interfaces (gNMI, NETCONF, SNMP) to test integration with collectors deployed outside of your lab environment Check out the short video demonstrating the integration: Containerlab made all of these use cases possible by integrating with mysocket.io service. Mysocket.io provides personal and secure tunnels for https/https/tls/tcp ports over global anycast 1 network spanning US, Europe and Asia. To make a certain port of a node available via mysocket.io tunnel provide a publish container under the node/kind/default section of the topology: name : demo topology : nodes : r1 : kind : srl publish : # tcp port 22 will be published and accessible to anyone - tcp/22 # tcp port 57400 will be published for a specific user only - tls/57400/user@domain.com # http service running over 10200 will be published # for any authenticated user within gmail domain - http/10200/gmail.com Registration # Tunnels set up by mysocket.io are associated with a user who set them, thus users are required to register within the service. Luckily, the registration is a split second process carried out via a web portal . All it takes is an email and a password. Acquiring a token # To authenticate with mysocket.io service a user needs to acquire the token by logging into the service. A helper command mysocketio login has been added to containerlab to help with that: # Login with password entered from the prompt containerlab tools mysocketio login -e myemail@dot.com Password: INFO [ 0000 ] Written mysocketio token to a file /root/containerlab/.mysocketio_token The acquired token will be saved under .mysocketio_token filename in the current working directory. Info The token is valid for 5 hours, once the token expires, the already established tunnels will continue to work, but to establish new tunnels a new token must be provided. Specify what to share # To indicate which ports to publish a users needs to add the publish section under the node/kind or default level of the topology definition file . In the example below, we are publishing SSH and gNMI services of r1 node: name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be exposed - tcp/57400 # tcp port 57400 will be exposed The publish section holds a list of <type>/<port-number>[/<allowed-domains-and-email> strings, where <type> must be one of the supported mysocket.io socket type 2 - http/https/tls/tcp <port> must be a single valid port value <allowed-domains-and-email> an optional element restricting access to published ports for a list of users' emails or domains. Read more about in the Identity Aware tunnels section. Note For a regular mysocketio account the maximum number of tunnels is limited to: - tcp based tunnels: 5 - http based tunnels: 10 If >5 tcp tunnels are required users should launch a VM in a lab, expose it's SSH service and use this VM as a jumpbox. Add mysocketio node # Containerlab integrates with mysocket.io service by leveraging mysocketctl application packaged in a container format. In order for the ports indicated in the publish block to be published, a user needs to add a mysocketio node to the topology. The complete topology file could look like this: name : publish topology : nodes : r1 : kind : srl image : srlinux:20.6.3-145 license : license.key publish : - tcp/22 # tcp port 22 will be exposed grafana : kind : linux image : grafana/grafana:7.4.3 publish : - http/3000 # grafana' default http port will be published # adding mysocketio container which has mysocketctl client inside mysocketio : kind : mysocketio image : ghcr.io/hellt/mysocketctl:0.2.0 binds : - .mysocketio_token:/root/.mysocketio_token # bind mount API token The mysocketio node is a tiny linux container with mysocketctl client installed. Containerlab uses this node to create the sockets and start the tunnels as per publish block instructions. Pay specific attention to binds section defined for mysocketio node. With this section we provide a path to the API token that we acquired before launching the lab. This token is used to authenticate with mysocketio API service. Explore published ports # When a user launches a lab with published ports it will be presented with a summary table after the lab deployment process finishes: +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ | 1 | clab-sock-r1 | 9cefd6cdb239 | srlinux:20.6.3-145 | srl | | running | 172.20.20.2/24 | 2001:172:20:20::2/80 | | 2 | clab-sock-mysocketctl | 8f5385beb97e | ghcr.io/hellt/mysocketctl:0.1.0 | mysocketio | | running | 172.20.20.3/24 | 2001:172:20:20::3/80 | +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ Published ports: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 SOCKET ID \u2502 DNS NAME \u2502 PORT(S) \u2502 TYPE \u2502 CLOUD AUTH \u2502 NAME \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 444ed853-d3b6-448c-8f0a-6854b3578848 \u2502 wild-water-9221.edge.mysocket.io \u2502 80, 443 \u2502 http \u2502 false \u2502 clab-grafana-http-3000 \u2502 \u2502 287e5962-29ac-4ca1-8e01-e0333d399070 \u2502 falling-wave-5735.edge.mysocket.io \u2502 54506 \u2502 tcp \u2502 false \u2502 clab-r1-tcp-22 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The Published ports table lists the published ports and their corresponding DNS names. Looking at the NAME column users can quickly discover which tunnel corresponds to which node-port. The socket name follows the clab-<node-name>-<type>-<port> pattern. To access the published port, users need to combine the DNS name and the Port to derive the full address. For the exposed SSH port, for example, the ssh client can use the following command to access remote SSH service: ssh user@falling-wave-5735.edge.mysocket.io -p 54506 Warning When a lab with published ports start, containerlab first removes all previously established tunnels. This means that any manually set up tunnels for this account will get removed. Identity aware tunnels # In the previous examples the published ports were created in a way that makes them accessible to anyone on the Internet who knows the exact domain name and port of a respective tunnel. Although being convenient, this approach is not secure, since there is no control over who can access the ports you published. If additional security is needed, containerlab users should define the published ports using Identity Awareness 3 feature of mysocketio. With Identity aware sockets users are allowed to specify a list of email addresses or domains which will have access to a certain port. Consider the following snippet: topology : nodes : leaf1 : publish : - tcp/22/dodin.roman@gmail.com,contractor@somewhere.com leaf2 : publish : - tcp/22 grafana : publish : - http/3000/gmail.com,nokia.com,colleague@somedomain.com Within the same publish block it is possible to provide a list of comma separated emails and/or domains which will be allowed to access the published port in question. Authentication is carried out by OAuth via Google, GitHub, Facebook and Mysocket.io providers. When accessing a secured tunnel, a browser page is opened asking to authenticate: Once authenticated via any of the available providers the sessions will establish. TCP/TLS # With Identity Aware sockets used for SSH 4 service, a client must have mysocketctl client installed and use the following command to establish a connection: ssh <ssh-username>@<mysocket-tunnel-address> \\ -o 'ProxyCommand=mysocketctl client tls --host %h' As with HTTP services, a browser page will appear asking to proceed with authentication. Upon successful authentication, the SSH session will establish. Troubleshooting # To check the health status of the established tunnels execute the following command to check the logs created on mysocketio container: docker exec -it <mysocketio-node-name> /bin/sh -c \"cat socket*\" This command will display all the logs for the published ports. If something is not right, you will see the errors in the log. https://mysocket.readthedocs.io/en/latest/about/about.html#build-on-a-global-anycast-network \u21a9 https://mysocket.readthedocs.io/en/latest/about/about.html#features \u21a9 Identity aware HTTP and TCP tunnels are available. \u21a9 Read more about Identity Aware sockets for TCP in the official blog. \u21a9","title":"Publish ports"},{"location":"manual/published-ports/#registration","text":"Tunnels set up by mysocket.io are associated with a user who set them, thus users are required to register within the service. Luckily, the registration is a split second process carried out via a web portal . All it takes is an email and a password.","title":"Registration"},{"location":"manual/published-ports/#acquiring-a-token","text":"To authenticate with mysocket.io service a user needs to acquire the token by logging into the service. A helper command mysocketio login has been added to containerlab to help with that: # Login with password entered from the prompt containerlab tools mysocketio login -e myemail@dot.com Password: INFO [ 0000 ] Written mysocketio token to a file /root/containerlab/.mysocketio_token The acquired token will be saved under .mysocketio_token filename in the current working directory. Info The token is valid for 5 hours, once the token expires, the already established tunnels will continue to work, but to establish new tunnels a new token must be provided.","title":"Acquiring a token"},{"location":"manual/published-ports/#specify-what-to-share","text":"To indicate which ports to publish a users needs to add the publish section under the node/kind or default level of the topology definition file . In the example below, we are publishing SSH and gNMI services of r1 node: name : demo topology : nodes : r1 : kind : srl publish : - tcp/22 # tcp port 22 will be exposed - tcp/57400 # tcp port 57400 will be exposed The publish section holds a list of <type>/<port-number>[/<allowed-domains-and-email> strings, where <type> must be one of the supported mysocket.io socket type 2 - http/https/tls/tcp <port> must be a single valid port value <allowed-domains-and-email> an optional element restricting access to published ports for a list of users' emails or domains. Read more about in the Identity Aware tunnels section. Note For a regular mysocketio account the maximum number of tunnels is limited to: - tcp based tunnels: 5 - http based tunnels: 10 If >5 tcp tunnels are required users should launch a VM in a lab, expose it's SSH service and use this VM as a jumpbox.","title":"Specify what to share"},{"location":"manual/published-ports/#add-mysocketio-node","text":"Containerlab integrates with mysocket.io service by leveraging mysocketctl application packaged in a container format. In order for the ports indicated in the publish block to be published, a user needs to add a mysocketio node to the topology. The complete topology file could look like this: name : publish topology : nodes : r1 : kind : srl image : srlinux:20.6.3-145 license : license.key publish : - tcp/22 # tcp port 22 will be exposed grafana : kind : linux image : grafana/grafana:7.4.3 publish : - http/3000 # grafana' default http port will be published # adding mysocketio container which has mysocketctl client inside mysocketio : kind : mysocketio image : ghcr.io/hellt/mysocketctl:0.2.0 binds : - .mysocketio_token:/root/.mysocketio_token # bind mount API token The mysocketio node is a tiny linux container with mysocketctl client installed. Containerlab uses this node to create the sockets and start the tunnels as per publish block instructions. Pay specific attention to binds section defined for mysocketio node. With this section we provide a path to the API token that we acquired before launching the lab. This token is used to authenticate with mysocketio API service.","title":"Add mysocketio node"},{"location":"manual/published-ports/#explore-published-ports","text":"When a user launches a lab with published ports it will be presented with a summary table after the lab deployment process finishes: +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ | # | Name | Container ID | Image | Kind | Group | State | IPv4 Address | IPv6 Address | +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ | 1 | clab-sock-r1 | 9cefd6cdb239 | srlinux:20.6.3-145 | srl | | running | 172.20.20.2/24 | 2001:172:20:20::2/80 | | 2 | clab-sock-mysocketctl | 8f5385beb97e | ghcr.io/hellt/mysocketctl:0.1.0 | mysocketio | | running | 172.20.20.3/24 | 2001:172:20:20::3/80 | +---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+ Published ports: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 SOCKET ID \u2502 DNS NAME \u2502 PORT(S) \u2502 TYPE \u2502 CLOUD AUTH \u2502 NAME \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 444ed853-d3b6-448c-8f0a-6854b3578848 \u2502 wild-water-9221.edge.mysocket.io \u2502 80, 443 \u2502 http \u2502 false \u2502 clab-grafana-http-3000 \u2502 \u2502 287e5962-29ac-4ca1-8e01-e0333d399070 \u2502 falling-wave-5735.edge.mysocket.io \u2502 54506 \u2502 tcp \u2502 false \u2502 clab-r1-tcp-22 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The Published ports table lists the published ports and their corresponding DNS names. Looking at the NAME column users can quickly discover which tunnel corresponds to which node-port. The socket name follows the clab-<node-name>-<type>-<port> pattern. To access the published port, users need to combine the DNS name and the Port to derive the full address. For the exposed SSH port, for example, the ssh client can use the following command to access remote SSH service: ssh user@falling-wave-5735.edge.mysocket.io -p 54506 Warning When a lab with published ports start, containerlab first removes all previously established tunnels. This means that any manually set up tunnels for this account will get removed.","title":"Explore published ports"},{"location":"manual/published-ports/#identity-aware-tunnels","text":"In the previous examples the published ports were created in a way that makes them accessible to anyone on the Internet who knows the exact domain name and port of a respective tunnel. Although being convenient, this approach is not secure, since there is no control over who can access the ports you published. If additional security is needed, containerlab users should define the published ports using Identity Awareness 3 feature of mysocketio. With Identity aware sockets users are allowed to specify a list of email addresses or domains which will have access to a certain port. Consider the following snippet: topology : nodes : leaf1 : publish : - tcp/22/dodin.roman@gmail.com,contractor@somewhere.com leaf2 : publish : - tcp/22 grafana : publish : - http/3000/gmail.com,nokia.com,colleague@somedomain.com Within the same publish block it is possible to provide a list of comma separated emails and/or domains which will be allowed to access the published port in question. Authentication is carried out by OAuth via Google, GitHub, Facebook and Mysocket.io providers. When accessing a secured tunnel, a browser page is opened asking to authenticate: Once authenticated via any of the available providers the sessions will establish.","title":"Identity aware tunnels"},{"location":"manual/published-ports/#tcptls","text":"With Identity Aware sockets used for SSH 4 service, a client must have mysocketctl client installed and use the following command to establish a connection: ssh <ssh-username>@<mysocket-tunnel-address> \\ -o 'ProxyCommand=mysocketctl client tls --host %h' As with HTTP services, a browser page will appear asking to proceed with authentication. Upon successful authentication, the SSH session will establish.","title":"TCP/TLS"},{"location":"manual/published-ports/#troubleshooting","text":"To check the health status of the established tunnels execute the following command to check the logs created on mysocketio container: docker exec -it <mysocketio-node-name> /bin/sh -c \"cat socket*\" This command will display all the logs for the published ports. If something is not right, you will see the errors in the log. https://mysocket.readthedocs.io/en/latest/about/about.html#build-on-a-global-anycast-network \u21a9 https://mysocket.readthedocs.io/en/latest/about/about.html#features \u21a9 Identity aware HTTP and TCP tunnels are available. \u21a9 Read more about Identity Aware sockets for TCP in the official blog. \u21a9","title":"Troubleshooting"},{"location":"manual/topo-def-file/","text":"Containerlab builds labs based on the topology information that users pass to it. This topology information is expressed as a code contained in the topology definition file which structure is the prime focus of this document. Topology definition components # The topology definition file is a configuration file expressed in YAML and has a name pattern of *.clab.yml 1 . In this document we take a pre-packaged Nokia SR Linux and Arista cEOS lab and explain the topology definition structure using its definition file srlceos01.clab.yml which is pasted below: name : srlceos01 topology : nodes : srl : kind : srl image : srlinux:20.6.3-145 license : license.key ceos : kind : ceos image : ceos:4.25.0F links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] This topology results in the two nodes being started up and interconnected with each other using a single point-po-point interface: Let's touch on the key components of the topology definition file used in this example. Name # The topology must have a name associated with it. The name is used to distinct one topology from another, to allow multiple topologies to be deployed on the same host without clashes. name : srlceos01 Its user's responsibility to give labs unique names if they plan to run multiple labs. The name is a free-formed string, though it is better not to use dashes ( - ) as they are used to separate lab names from node names. When containerlab starts the containers, their names will be generated using the following pattern: clab-{{lab-name}}-{{node-name}} . The lab name here is used to make the container's names unique between two different labs even if the nodes are named the same. Topology # The topology object inside the topology definition is the core element of the file. Under the topology element you will find all the main building blocks of a topology such as nodes , kinds , defaults and links . Nodes # As with every other topology the nodes are in the center of things. With nodes we define which lab elements we want to run, in what configuration and flavor. Let's zoom into the two nodes we have defined in our topology: topology : nodes : srl : # this is a name of the 1st node kind : srl type : ixrd2 image : srlinux:20.6.3-145 license : license.key ceos : # this is a name of the 2nd node kind : ceos image : ceos:4.25.0F We defined individual nodes under the topology.nodes container. The name of the node is the key under which it is defined. Following the example, our two nodes are named srl and ceos respectively. Each node can have multiple configuration properties which make containerlab quite a flexible tool. The srl node in our example is defined with the a few node-specific properties: srl : kind : srl type : ixrd2 image : srlinux license : license.key Refer to the node configuration document to meet all other options a node can have. Links # Although it is totally fine to define a node without any links (like in this lab ) most of the time we interconnect the nodes to make datapaths. One of containerlab purposes is to make the interconnection of nodes simple. Links are defined under the topology.links container in the following manner: # nodes configuration omitted for clarity topology : nodes : srl : ceos : links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] - endpoints : [ \"srl:e1-2\" , \"ceos:eth2\" ] As you see, the topology.links element is a list of individual links. The link itself is expressed as pair of endpoints . This might sound complicated, lets use a graphical explanation: As demonstrated on a diagram above, the links between the containers are the point-to-point links which are defined by a pair of interfaces. The link defined as: endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] will result in a creation of a p2p link between the node named srl and its e1-1 interface and the node named ceos and its eth1 interface. The p2p link is realized with a veth pair. Kinds # Kinds define the behavior and the nature of a node, it says if the node is a specific containerized Network OS, virtualized router or something else. We go into details of kinds in its own document section , so here we will discuss what happens when kinds section appears in the topology definition: topology : kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl srl3 : kind : srl In the example above the topology.kinds element has srl kind referenced. With this, we set some values for the properties of the srl kind. A configuration like that says that nodes of srl kind will also inherit the properties (type, image, license) defined on the kind level . Essentially, what kinds section allows us to do is to shorten the lab definition in cases when we have a number of nodes of a same kind. All the nodes ( srl1 , srl2 , srl3 ) will have the same values for their type , image and license properties. Consider how the topology would have looked like without setting the kinds object: topology : nodes : srl1 : kind : srl type : ixrd2 image : srlinux license : license.key srl2 : kind : srl type : ixrd2 image : srlinux license : license.key srl3 : kind : srl type : ixrd2 image : srlinux license : license.key A lot of unnecessary repetition which is eliminated when we set srl kind properties on kind level. Defaults # kinds set the values for the properties of a specific kind, whereas with the defaults container it is possible to set values globally. For example, to set the environment variable for all the nodes of a topology: topology : defaults : env : MYENV : VALUE nodes : srl1 : srl2 : srl3 : Now every node in this topology will have environment variable MYENV set to VALUE . if the filename has .clab.yml or -clab.yml suffix, the YAML file will have autocompletion and linting support in VSCode editor. \u21a9","title":"Topology definition"},{"location":"manual/topo-def-file/#topology-definition-components","text":"The topology definition file is a configuration file expressed in YAML and has a name pattern of *.clab.yml 1 . In this document we take a pre-packaged Nokia SR Linux and Arista cEOS lab and explain the topology definition structure using its definition file srlceos01.clab.yml which is pasted below: name : srlceos01 topology : nodes : srl : kind : srl image : srlinux:20.6.3-145 license : license.key ceos : kind : ceos image : ceos:4.25.0F links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] This topology results in the two nodes being started up and interconnected with each other using a single point-po-point interface: Let's touch on the key components of the topology definition file used in this example.","title":"Topology definition components"},{"location":"manual/topo-def-file/#name","text":"The topology must have a name associated with it. The name is used to distinct one topology from another, to allow multiple topologies to be deployed on the same host without clashes. name : srlceos01 Its user's responsibility to give labs unique names if they plan to run multiple labs. The name is a free-formed string, though it is better not to use dashes ( - ) as they are used to separate lab names from node names. When containerlab starts the containers, their names will be generated using the following pattern: clab-{{lab-name}}-{{node-name}} . The lab name here is used to make the container's names unique between two different labs even if the nodes are named the same.","title":"Name"},{"location":"manual/topo-def-file/#topology","text":"The topology object inside the topology definition is the core element of the file. Under the topology element you will find all the main building blocks of a topology such as nodes , kinds , defaults and links .","title":"Topology"},{"location":"manual/topo-def-file/#nodes","text":"As with every other topology the nodes are in the center of things. With nodes we define which lab elements we want to run, in what configuration and flavor. Let's zoom into the two nodes we have defined in our topology: topology : nodes : srl : # this is a name of the 1st node kind : srl type : ixrd2 image : srlinux:20.6.3-145 license : license.key ceos : # this is a name of the 2nd node kind : ceos image : ceos:4.25.0F We defined individual nodes under the topology.nodes container. The name of the node is the key under which it is defined. Following the example, our two nodes are named srl and ceos respectively. Each node can have multiple configuration properties which make containerlab quite a flexible tool. The srl node in our example is defined with the a few node-specific properties: srl : kind : srl type : ixrd2 image : srlinux license : license.key Refer to the node configuration document to meet all other options a node can have.","title":"Nodes"},{"location":"manual/topo-def-file/#links","text":"Although it is totally fine to define a node without any links (like in this lab ) most of the time we interconnect the nodes to make datapaths. One of containerlab purposes is to make the interconnection of nodes simple. Links are defined under the topology.links container in the following manner: # nodes configuration omitted for clarity topology : nodes : srl : ceos : links : - endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] - endpoints : [ \"srl:e1-2\" , \"ceos:eth2\" ] As you see, the topology.links element is a list of individual links. The link itself is expressed as pair of endpoints . This might sound complicated, lets use a graphical explanation: As demonstrated on a diagram above, the links between the containers are the point-to-point links which are defined by a pair of interfaces. The link defined as: endpoints : [ \"srl:e1-1\" , \"ceos:eth1\" ] will result in a creation of a p2p link between the node named srl and its e1-1 interface and the node named ceos and its eth1 interface. The p2p link is realized with a veth pair.","title":"Links"},{"location":"manual/topo-def-file/#kinds","text":"Kinds define the behavior and the nature of a node, it says if the node is a specific containerized Network OS, virtualized router or something else. We go into details of kinds in its own document section , so here we will discuss what happens when kinds section appears in the topology definition: topology : kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl srl3 : kind : srl In the example above the topology.kinds element has srl kind referenced. With this, we set some values for the properties of the srl kind. A configuration like that says that nodes of srl kind will also inherit the properties (type, image, license) defined on the kind level . Essentially, what kinds section allows us to do is to shorten the lab definition in cases when we have a number of nodes of a same kind. All the nodes ( srl1 , srl2 , srl3 ) will have the same values for their type , image and license properties. Consider how the topology would have looked like without setting the kinds object: topology : nodes : srl1 : kind : srl type : ixrd2 image : srlinux license : license.key srl2 : kind : srl type : ixrd2 image : srlinux license : license.key srl3 : kind : srl type : ixrd2 image : srlinux license : license.key A lot of unnecessary repetition which is eliminated when we set srl kind properties on kind level.","title":"Kinds"},{"location":"manual/topo-def-file/#defaults","text":"kinds set the values for the properties of a specific kind, whereas with the defaults container it is possible to set values globally. For example, to set the environment variable for all the nodes of a topology: topology : defaults : env : MYENV : VALUE nodes : srl1 : srl2 : srl3 : Now every node in this topology will have environment variable MYENV set to VALUE . if the filename has .clab.yml or -clab.yml suffix, the YAML file will have autocompletion and linting support in VSCode editor. \u21a9","title":"Defaults"},{"location":"manual/vrnetlab/","text":"Containerlab focuses on containers, but there are many routing products which are only shipped in a virtual machine packaging. Leaving containerlab users without ability to create topologies with both containerized and VM-based routing systems would have been a shame. Keeping this requirement in mind from the very beginning, we added kinds like bridge / ovs-bridge , that allows to, ehm, bridge your containerized topology with other resources available via a bridged network. For example, a VM based router: Although this approach has many pros, it doesn't allow users to define the VM based nodes in the same topology file. But not anymore, with vrnetlab integration containerlab is capable of launching topologies with VM-based routers defined in the same topology file. Vrnetlab # Vrnetlab essentially allows to package a regular VM inside a container and makes it runnable and accessible as if it was a container image. To make this work, vrnetlab provides a set of scripts that will build the container image out of a user provided VM disk. This enables containerlab to build topologies which consist both of native containerized NOSes and VMs: Warning Make sure, that the VM that containerlab runs on have Nested virtualization enabled to support vrnetlab based containers. Compatibility matrix # To make vrnetlab images to work with container-based networking in containerlab we needed to fork vrnetlab project and implement the necessary improvements. This means that VM-based routers that you intend to run with containerlab should be built with hellt/vrnetlab project, and not with the upstream vrnetlab. Containerlab depends on hellt/vrnetlab project and sometimes features added in containerlab must be implemented in vrnetlab (and vice-versa). This leads to a cross-dependency between these projects. The following table provides a link between the version combinations that were validated: containerlab 3 vrnetlab 4 Notes 0.10.4 0.1.0-cl Initial release. Images: sros, vmx, xrv, xrv9k 0.11.0 0.2.0 added vr-veos , support for boot-delay , SR OS will have a static route to docker network, improved XRv startup chances -- 0.2.1 added timeout for SR OS images to allow eth interfaces to appear in the container namespace. Other images are not touched. -- 0.2.2 fixed serial (telnet) access to SR OS nodes -- 0.2.3 set default cpu/ram for SR OS images Building vrnetlab images # To build a vrnetlab image compatible with containerlab users first need to ensure that the versions of both projects follow compatibility matrix . Clone hellt/vrnetlab and checkout to a version compatible with containerlab release: git clone https://github.com/hellt/vrnetlab && cd vrnetlab # assuming we are running containerlab 0.11.0, # the latest compatible vrnetlab version is 0.2.3 # at the moment of this writing git checkout v0.2.3 Enter the directory for the image of interest cd sros Follow the build instructions from the README.md file in the image directory Supported VM products # The images that work with containerlab will appear in the supported list gradually, as we implement the necessary integration. Product Kind Demo lab Notes Nokia SR OS vr-sros SRL & SR OS When building SR OS vrnetlab image for use with containerlab, do not provide the license during the image build process. The license shall be provided in the containerlab topology definition file 1 . Juniper vMX vr-vmx SRL & vMX Cisco XRv vr-xrv SRL & XRv Cisco XRv9k vr-xrv9k SRL & XRv9k Arista vEOS vr-veos Connection modes # Containerlab offers several ways VM based routers can be connected with the rest of the docker workloads. By default, vrnetlab integrated routers will use tc backend 2 which doesn't require any additional packages to be installed on the containerhost and supoprts transparent passage of LACP frames. Any other datapaths? Althout tc based datapath should cover all the needed connectivity requirements, if other, bridge-like, datapaths are needed, Containerlab offers OpenvSwitch and Linux bridge modes. Users can plug in those datapaths by specifying CONNECTION_MODE env variable: # the env variable can also be set in the defaults section name : myTopo topology : nodes : sr1 : kind : vr-sros image : vrnetlab/vr-sros:20.10.R1 env : CONNECTION_MODE : bridge # use `ovs` for openvswitch datapath Boot delay # Simultaneous boot of many qemu nodes may stress the underlying system, which sometimes render in a boot loop or system halt. If the container host doesn't have enough capacity to bear the simultaneous boot of many qemu nodes it is still possible to successfully run them by scheduling their boot time. Delaying the boot process of certain nodes by a user defined time will allow nodes to boot successfully while \"gradually\" load the system. The boot delay can be set with BOOT_DELAY environment varialbe that supported vr-xxxx kinds will recognize. Consider the following example where the first SR OS nodes will boot immediately, whereas the second node will sleep for 30 seconds and then start the boot process: name : bootdelay topology : nodes : sr1 : kind : vr-sros image : vr-sros:21.2.R1 license : license-sros21.txt sr2 : kind : vr-sros image : vr-sros:21.2.R1 license : license-sros21.txt env : # boot delay in seconds BOOT_DELAY : 30 Memory optimization # Typically a lab consists of a few types of VMs which are spawned and inteconnected with each other. Consider a fictious lab that consists of 5 interconnected routers, 1 router uses VM image X and 4 routers are using VM image Y. Effectively we run just two types of VMs in that lab, and thus we can implement memory deduplication technique that drastically reduces the memory footprint of a lab. In Linux this can be achieved with technologies like UKSM/KSM. Refer to this article that explains the methodology and provides steps to get UKSM working on Ubuntu/Fedora systems. see this example lab with a license path provided in the topology definition file \u21a9 pros and cons of different datapaths were examined here \u21a9 to install a certain version of containerlab, use the instructions from installation doc. \u21a9 to have a guaranteed compatibility checkout to the mentined tag and build the images. \u21a9","title":"VM based routers integration"},{"location":"manual/vrnetlab/#vrnetlab","text":"Vrnetlab essentially allows to package a regular VM inside a container and makes it runnable and accessible as if it was a container image. To make this work, vrnetlab provides a set of scripts that will build the container image out of a user provided VM disk. This enables containerlab to build topologies which consist both of native containerized NOSes and VMs: Warning Make sure, that the VM that containerlab runs on have Nested virtualization enabled to support vrnetlab based containers.","title":"Vrnetlab"},{"location":"manual/vrnetlab/#compatibility-matrix","text":"To make vrnetlab images to work with container-based networking in containerlab we needed to fork vrnetlab project and implement the necessary improvements. This means that VM-based routers that you intend to run with containerlab should be built with hellt/vrnetlab project, and not with the upstream vrnetlab. Containerlab depends on hellt/vrnetlab project and sometimes features added in containerlab must be implemented in vrnetlab (and vice-versa). This leads to a cross-dependency between these projects. The following table provides a link between the version combinations that were validated: containerlab 3 vrnetlab 4 Notes 0.10.4 0.1.0-cl Initial release. Images: sros, vmx, xrv, xrv9k 0.11.0 0.2.0 added vr-veos , support for boot-delay , SR OS will have a static route to docker network, improved XRv startup chances -- 0.2.1 added timeout for SR OS images to allow eth interfaces to appear in the container namespace. Other images are not touched. -- 0.2.2 fixed serial (telnet) access to SR OS nodes -- 0.2.3 set default cpu/ram for SR OS images","title":"Compatibility matrix"},{"location":"manual/vrnetlab/#building-vrnetlab-images","text":"To build a vrnetlab image compatible with containerlab users first need to ensure that the versions of both projects follow compatibility matrix . Clone hellt/vrnetlab and checkout to a version compatible with containerlab release: git clone https://github.com/hellt/vrnetlab && cd vrnetlab # assuming we are running containerlab 0.11.0, # the latest compatible vrnetlab version is 0.2.3 # at the moment of this writing git checkout v0.2.3 Enter the directory for the image of interest cd sros Follow the build instructions from the README.md file in the image directory","title":"Building vrnetlab images"},{"location":"manual/vrnetlab/#supported-vm-products","text":"The images that work with containerlab will appear in the supported list gradually, as we implement the necessary integration. Product Kind Demo lab Notes Nokia SR OS vr-sros SRL & SR OS When building SR OS vrnetlab image for use with containerlab, do not provide the license during the image build process. The license shall be provided in the containerlab topology definition file 1 . Juniper vMX vr-vmx SRL & vMX Cisco XRv vr-xrv SRL & XRv Cisco XRv9k vr-xrv9k SRL & XRv9k Arista vEOS vr-veos","title":"Supported VM products"},{"location":"manual/vrnetlab/#connection-modes","text":"Containerlab offers several ways VM based routers can be connected with the rest of the docker workloads. By default, vrnetlab integrated routers will use tc backend 2 which doesn't require any additional packages to be installed on the containerhost and supoprts transparent passage of LACP frames. Any other datapaths? Althout tc based datapath should cover all the needed connectivity requirements, if other, bridge-like, datapaths are needed, Containerlab offers OpenvSwitch and Linux bridge modes. Users can plug in those datapaths by specifying CONNECTION_MODE env variable: # the env variable can also be set in the defaults section name : myTopo topology : nodes : sr1 : kind : vr-sros image : vrnetlab/vr-sros:20.10.R1 env : CONNECTION_MODE : bridge # use `ovs` for openvswitch datapath","title":"Connection modes"},{"location":"manual/vrnetlab/#boot-delay","text":"Simultaneous boot of many qemu nodes may stress the underlying system, which sometimes render in a boot loop or system halt. If the container host doesn't have enough capacity to bear the simultaneous boot of many qemu nodes it is still possible to successfully run them by scheduling their boot time. Delaying the boot process of certain nodes by a user defined time will allow nodes to boot successfully while \"gradually\" load the system. The boot delay can be set with BOOT_DELAY environment varialbe that supported vr-xxxx kinds will recognize. Consider the following example where the first SR OS nodes will boot immediately, whereas the second node will sleep for 30 seconds and then start the boot process: name : bootdelay topology : nodes : sr1 : kind : vr-sros image : vr-sros:21.2.R1 license : license-sros21.txt sr2 : kind : vr-sros image : vr-sros:21.2.R1 license : license-sros21.txt env : # boot delay in seconds BOOT_DELAY : 30","title":"Boot delay"},{"location":"manual/vrnetlab/#memory-optimization","text":"Typically a lab consists of a few types of VMs which are spawned and inteconnected with each other. Consider a fictious lab that consists of 5 interconnected routers, 1 router uses VM image X and 4 routers are using VM image Y. Effectively we run just two types of VMs in that lab, and thus we can implement memory deduplication technique that drastically reduces the memory footprint of a lab. In Linux this can be achieved with technologies like UKSM/KSM. Refer to this article that explains the methodology and provides steps to get UKSM working on Ubuntu/Fedora systems. see this example lab with a license path provided in the topology definition file \u21a9 pros and cons of different datapaths were examined here \u21a9 to install a certain version of containerlab, use the instructions from installation doc. \u21a9 to have a guaranteed compatibility checkout to the mentined tag and build the images. \u21a9","title":"Memory optimization"},{"location":"manual/wireshark/","text":"Every lab must have a packet capturing abilities, without it data plane verification becomes unnecessary complicated. Containerlab is no exception and capturing packets is something you can and should do with the labs launched by containerlab. Consider the following lab topology which highlights the typical points of packet capture. Since containerlab leverages linux network devices, users are free to use whatever tool of choice to sniff from any of them. This article will provide examples for tcpdump and wireshark tools. Packet capture, namespaces and interfaces # Capturing the packets from an interface requires having that interface name and it's network namespace (netns). And that's it. Keep in mind, that containers employ network isolation by the means of network namespaces. As depicted above, each container has its own network namespace which is named exactly the same. This makes it trivial to pinpoint which namespace to use. If containerlab at the end of a lab deploy reports that it created the containers with the names clab-lab1-srl clab-lab1-ceos clab-lab1-linux then the namespaces for each of those containers will be named the same (clab-lab1-srl, etc). To list the interfaces (links) of a given container leverage the ip utility: # where $netns_name is the container name of a node ip netns exec $netns_name ip link Capturing with tcpdump/wireshark # Now when it is clear which netns names corresponds to which container and which interfaces are available inside the given lab node, its extremely easy to start capturing traffic. local capture # From the containerlab host to capture from any interface inside a container simply use: # where $lab_node_name is the name of the container, which is also the name of the network namespace # and $if_name is the interface name inside the container netns ip netns exec $lab_node_name tcpdump -nni $if_name remote capture # If you want to start capture from a remote machine, then add ssh command to the mix: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -nni $if_name \" Capturing remotely with tcpdump makes little sense, but it makes all the difference when wireshark is concerned. Wireshark normally is not installed on the containerlab host, but it more often than not installed on the users machine/laptop. Thus it is possible to use remote capture capability to let wireshark receive the traffic from the remote containerlab node: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | wireshark -k -i - This will start the capture from a given interface and redirect the received flow to the wireshark input. Note Windows users should use WSL and invoke the command similar to the following: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | /mnt/c/Program \\ Files/Wireshark/wireshark.exe -k -i - Examples # Lets take the first diagram of this article and see which commands are used to sniff from the highlighted interfaces. In the examples below the wireshark will be used as a sniffing tool and the following naming simplifications and conventions used: $clab_host - address of the containerlab host clab-pcap-srl , clab-pcap-ceos , clab-pcap-linux - container names of the SRL, cEOS and Linux nodes accordingly. SR Linux [1], [4] SR Linux linecard interfaces are named as e<linecard_num>-<port_num> which translates to ethernet-<linecard_num>/<port_num> name inside the NOS itself. So to capture from ethernet-1/1 interface the following command should be used: ssh $clab_host \"ip netns exec $clab -pcap-srl tcpdump -U -nni e1-1 -w -\" | wireshark -k -i - The management interface on the SR Linux container is named mgmt0 , so the relevant command will look like: ssh $clab_host \"ip netns exec $clab -pcap-srl tcpdump -U -nni mgmt0 -w -\" | wireshark -k -i - cEOS [2] Similarly to SR Linux example, to capture the data interface of cEOS is no different. Just pick the right interface: ssh $clab_host \"ip netns exec $clab -pcap-ceos tcpdump -U -nni eth1 -w -\" | wireshark -k -i - Linux container [3] A bare linux container is no different, its interfaces are named ethX where eth0 is the interface connected to the containerlab management network. So to capture from the first data link we will use eth1 interface: ssh $clab_host \"ip netns exec $clab -pcap-linux tcpdump -U -nni eth1 -w -\" | wireshark -k -i - management bridge [5] It is also possible to listen for all management traffic that traverses the containerlab's management network. To do that you firstly need to find out the name of the linux bridge and then capture from it: ssh $clab_host \"tcpdump -U -nni brXXXXXX -w -\" | wireshark -k -i - Note that in this case you do not need to drill into the network namespace, since management bridge is in the default netns. To simplify wireshark remote capturing process users can create a tiny bash script that will save some typing: #!/bin/sh # call this script as `bash script_name.sh <container-name> <interface-name>` ssh <containerlab_address> \"ip netns exec $1 tcpdump -U -nni $2 -w -\" | wireshark -k -i -","title":"Packet capture & Wireshark"},{"location":"manual/wireshark/#packet-capture-namespaces-and-interfaces","text":"Capturing the packets from an interface requires having that interface name and it's network namespace (netns). And that's it. Keep in mind, that containers employ network isolation by the means of network namespaces. As depicted above, each container has its own network namespace which is named exactly the same. This makes it trivial to pinpoint which namespace to use. If containerlab at the end of a lab deploy reports that it created the containers with the names clab-lab1-srl clab-lab1-ceos clab-lab1-linux then the namespaces for each of those containers will be named the same (clab-lab1-srl, etc). To list the interfaces (links) of a given container leverage the ip utility: # where $netns_name is the container name of a node ip netns exec $netns_name ip link","title":"Packet capture, namespaces and interfaces"},{"location":"manual/wireshark/#capturing-with-tcpdumpwireshark","text":"Now when it is clear which netns names corresponds to which container and which interfaces are available inside the given lab node, its extremely easy to start capturing traffic.","title":"Capturing with tcpdump/wireshark"},{"location":"manual/wireshark/#local-capture","text":"From the containerlab host to capture from any interface inside a container simply use: # where $lab_node_name is the name of the container, which is also the name of the network namespace # and $if_name is the interface name inside the container netns ip netns exec $lab_node_name tcpdump -nni $if_name","title":"local capture"},{"location":"manual/wireshark/#remote-capture","text":"If you want to start capture from a remote machine, then add ssh command to the mix: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -nni $if_name \" Capturing remotely with tcpdump makes little sense, but it makes all the difference when wireshark is concerned. Wireshark normally is not installed on the containerlab host, but it more often than not installed on the users machine/laptop. Thus it is possible to use remote capture capability to let wireshark receive the traffic from the remote containerlab node: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | wireshark -k -i - This will start the capture from a given interface and redirect the received flow to the wireshark input. Note Windows users should use WSL and invoke the command similar to the following: ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | /mnt/c/Program \\ Files/Wireshark/wireshark.exe -k -i -","title":"remote capture"},{"location":"manual/wireshark/#examples","text":"Lets take the first diagram of this article and see which commands are used to sniff from the highlighted interfaces. In the examples below the wireshark will be used as a sniffing tool and the following naming simplifications and conventions used: $clab_host - address of the containerlab host clab-pcap-srl , clab-pcap-ceos , clab-pcap-linux - container names of the SRL, cEOS and Linux nodes accordingly. SR Linux [1], [4] SR Linux linecard interfaces are named as e<linecard_num>-<port_num> which translates to ethernet-<linecard_num>/<port_num> name inside the NOS itself. So to capture from ethernet-1/1 interface the following command should be used: ssh $clab_host \"ip netns exec $clab -pcap-srl tcpdump -U -nni e1-1 -w -\" | wireshark -k -i - The management interface on the SR Linux container is named mgmt0 , so the relevant command will look like: ssh $clab_host \"ip netns exec $clab -pcap-srl tcpdump -U -nni mgmt0 -w -\" | wireshark -k -i - cEOS [2] Similarly to SR Linux example, to capture the data interface of cEOS is no different. Just pick the right interface: ssh $clab_host \"ip netns exec $clab -pcap-ceos tcpdump -U -nni eth1 -w -\" | wireshark -k -i - Linux container [3] A bare linux container is no different, its interfaces are named ethX where eth0 is the interface connected to the containerlab management network. So to capture from the first data link we will use eth1 interface: ssh $clab_host \"ip netns exec $clab -pcap-linux tcpdump -U -nni eth1 -w -\" | wireshark -k -i - management bridge [5] It is also possible to listen for all management traffic that traverses the containerlab's management network. To do that you firstly need to find out the name of the linux bridge and then capture from it: ssh $clab_host \"tcpdump -U -nni brXXXXXX -w -\" | wireshark -k -i - Note that in this case you do not need to drill into the network namespace, since management bridge is in the default netns. To simplify wireshark remote capturing process users can create a tiny bash script that will save some typing: #!/bin/sh # call this script as `bash script_name.sh <container-name> <interface-name>` ssh <containerlab_address> \"ip netns exec $1 tcpdump -U -nni $2 -w -\" | wireshark -k -i -","title":"Examples"},{"location":"manual/kinds/bridge/","text":"Linux bridge # Containerlab can connect its nodes to a Linux bridge instead of interconnecting the nodes directly. This connectivity option is enabled with bridge kind and opens a variety of integrations that containerlab labs can have with workloads of other types. For example, by connecting a lab node to a bridge we can: allow a node to talk to any workloads (VMs, containers, baremetals) which are connected to that bridge let a node to reach networks which are available via that bridge scale out containerlab labs by running separate labs in different hosts and get network reachibility between them wiring nodes' data interfaces via a broadcast domain (linux bridge) and use vlans to making dynamic connections Using bridge kind # Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file , the bridge needs to be created first. Once the bridge is created, it needs to be referenced as a node inside the topology file: # topology documentation: http://containerlab.srlinux.dev/lab-examples/ext-bridge/ name : br01 topology : kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl srl3 : kind : srl # note, that the bridge br-clab must be created manually br-clab : kind : bridge links : - endpoints : [ \"srl1:e1-1\" , \"br-clab:eth1\" ] - endpoints : [ \"srl2:e1-1\" , \"br-clab:eth2\" ] - endpoints : [ \"srl3:e1-1\" , \"br-clab:eth3\" ] In the example above, node br-clab of kind bridge tells containerlab to identify it as a linux bridge and look for a bridge named br-clab . When connecting other nodes to a bridge, the bridge endpoint must be present in the links section. Note When choosing names of the interfaces that need to be connected to the bridge make sure that these names are not clashing with existing interfaces. In the example above we named interfaces eth1 , eth2 , eth3 accordingly and ensured that none of these interfaces existed before in the root netns. As a result of such topology definition, you will see bridge br-clab with three interfaces attached to it: bridge name bridge id STP enabled interfaces br-clab 8000.6281eb7133d2 no eth1 eth2 eth3 Chec out \"External bridge\" lab for a ready-made example on how to use bridges.","title":"bridge - Linux bridge"},{"location":"manual/kinds/bridge/#linux-bridge","text":"Containerlab can connect its nodes to a Linux bridge instead of interconnecting the nodes directly. This connectivity option is enabled with bridge kind and opens a variety of integrations that containerlab labs can have with workloads of other types. For example, by connecting a lab node to a bridge we can: allow a node to talk to any workloads (VMs, containers, baremetals) which are connected to that bridge let a node to reach networks which are available via that bridge scale out containerlab labs by running separate labs in different hosts and get network reachibility between them wiring nodes' data interfaces via a broadcast domain (linux bridge) and use vlans to making dynamic connections","title":"Linux bridge"},{"location":"manual/kinds/bridge/#using-bridge-kind","text":"Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file , the bridge needs to be created first. Once the bridge is created, it needs to be referenced as a node inside the topology file: # topology documentation: http://containerlab.srlinux.dev/lab-examples/ext-bridge/ name : br01 topology : kinds : srl : type : ixrd2 image : srlinux license : license.key nodes : srl1 : kind : srl srl2 : kind : srl srl3 : kind : srl # note, that the bridge br-clab must be created manually br-clab : kind : bridge links : - endpoints : [ \"srl1:e1-1\" , \"br-clab:eth1\" ] - endpoints : [ \"srl2:e1-1\" , \"br-clab:eth2\" ] - endpoints : [ \"srl3:e1-1\" , \"br-clab:eth3\" ] In the example above, node br-clab of kind bridge tells containerlab to identify it as a linux bridge and look for a bridge named br-clab . When connecting other nodes to a bridge, the bridge endpoint must be present in the links section. Note When choosing names of the interfaces that need to be connected to the bridge make sure that these names are not clashing with existing interfaces. In the example above we named interfaces eth1 , eth2 , eth3 accordingly and ensured that none of these interfaces existed before in the root netns. As a result of such topology definition, you will see bridge br-clab with three interfaces attached to it: bridge name bridge id STP enabled interfaces br-clab 8000.6281eb7133d2 no eth1 eth2 eth3 Chec out \"External bridge\" lab for a ready-made example on how to use bridges.","title":"Using bridge kind"},{"location":"manual/kinds/ceos/","text":"Arista cEOS # Arista cEOS is identified with ceos kind in the topology file . The ceos kind defines a supported feature set and a startup procedure of a ceos node. cEOS nodes launched with containerlab comes up with their management interface eth0 configured with IPv4/6 addresses as assigned by docker hostname assigned to the node name gNMI, Netconf and eAPI services enabled admin user created with password admin Managing ceos nodes # Arista cEOS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running ceos container: docker exec -it <container-name/id> bash CLI to connect to the ceos CLI docker exec -it <container-name/id> Cli NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf gNMI gNMI server is running over port 6030 in non-secure mode using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address>:6030 --insecure \\ -u admin -p admin \\ capabilities Info Default user credentials: admin:admin Interfaces mapping # ceos container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface When containerlab launches ceos node, it will set IPv4/6 addresses as assigned by docker to the eth0 interface and ceos node will boot with that addresses configure. Data interfaces eth1+ need to be configured with IP addressing manually. ceos interfaces output This output demonstrates the IP addressing of the linux interfaces of ceos node. bash-4.2# ip address 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/24 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever <SNIP> 5877: eth0@if5878: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.2/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2001:172:20:20::2/80 scope global valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe14:1402/64 scope link valid_lft forever preferred_lft forever This output shows how the linux interfaces are mapped into the ceos OS. ceos>sh ip int br Address Interface IP Address Status Protocol MTU Owner ----------------- -------------------- ------------ -------------- ---------- ------- Management0 172.20.20.2/24 up up 1500 ceos>sh ipv6 int br Interface Status MTU IPv6 Address Addr State Addr Source --------------- ------------ ---------- -------------------------------- ---------------- ----------- Ma0 up 1500 fe80::42:acff:fe14:1402/64 up link local 2001:172:20:20::2/80 up config As you see, the management interface Ma0 inherits the IP address that docker assigned to ceos container management interface. Features and options # Node configuration # cEOS nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of ceos kind with a basic config or to provide a custom config file that will be used as a startup config instead. Default node configuration # When a node is defined without config statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : ceos topology : nodes : ceos : kind : ceos The generated config will be saved by the path clab-<lab_name>/<node-name>/flash/startup-config . Using the example topology presented above, the exact path to the config will be clab-ceos/ceos/flash/startup-config . User defined config # It is possible to make ceos nodes to boot up with a user-defined config instead of a built-in one. With a config property a user sets the path to the config file that will be mounted to a container and used as a startup config: name : ceos_lab topology : nodes : ceos : kind : ceos config : myconfig.conf With such topology file containerlab is instructed to take a file myconfig.conf from the current working directory, copy it to the lab directory for that specific node under the /flash/startup-config name and mount that dir to the container. This will result in this config to act as a startup config for the node. Saving configuration # With containerlab save command it's possible to save running cEOS configuration into a file. The configuration will be saved by conf-saved.conf path in the relevant node directory. Container configuration # To start an Arista cEOS node containerlab uses the configuration instructions described in Arista Forums 1 . The exact parameters are outlined below. Startup command /sbin/init systemd.setenv=INTFTYPE=eth systemd.setenv=ETBA=4 systemd.setenv=SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT=1 systemd.setenv=CEOS=1 systemd.setenv=EOS_PLATFORM=ceoslab systemd.setenv=container=docker systemd.setenv=MAPETH0=1 systemd.setenv=MGMT_INTF=eth0 Environment variables CEOS:1 EOS_PLATFORM\":ceoslab container:docker ETBA:1 SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT:1 INTFTYPE:eth MAPETH0:1 MGMT_INTF:eth0 File mounts # When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For ceos kind containerlab creates flash directory for each ceos node and mounts these folders by /mnt/flash paths. \u276f tree clab-srlceos01/ceos clab-srlceos01/ceos \u2514\u2500\u2500 flash \u251c\u2500\u2500 AsuFastPktTransmit.log \u251c\u2500\u2500 debug \u2502 \u2514\u2500\u2500 proc \u2502 \u2514\u2500\u2500 modules \u251c\u2500\u2500 fastpkttx.backup \u251c\u2500\u2500 Fossil \u251c\u2500\u2500 kickstart-config \u251c\u2500\u2500 persist \u2502 \u251c\u2500\u2500 local \u2502 \u251c\u2500\u2500 messages \u2502 \u251c\u2500\u2500 persistentRestartLog \u2502 \u251c\u2500\u2500 secure \u2502 \u2514\u2500\u2500 sys \u251c\u2500\u2500 schedule \u2502 \u2514\u2500\u2500 tech-support \u2502 \u2514\u2500\u2500 ceos_tech-support_2021-01-14.0907.log.gz \u251c\u2500\u2500 SsuRestoreLegacy.log \u251c\u2500\u2500 SsuRestore.log \u2514\u2500\u2500 startup-config 9 directories, 11 files Lab examples # The following labs feature cEOS node: SR Linux and cEOS https://eos.arista.com/ceos-lab-topo/ \u21a9","title":"ceos - Arista cEOS"},{"location":"manual/kinds/ceos/#arista-ceos","text":"Arista cEOS is identified with ceos kind in the topology file . The ceos kind defines a supported feature set and a startup procedure of a ceos node. cEOS nodes launched with containerlab comes up with their management interface eth0 configured with IPv4/6 addresses as assigned by docker hostname assigned to the node name gNMI, Netconf and eAPI services enabled admin user created with password admin","title":"Arista cEOS"},{"location":"manual/kinds/ceos/#managing-ceos-nodes","text":"Arista cEOS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running ceos container: docker exec -it <container-name/id> bash CLI to connect to the ceos CLI docker exec -it <container-name/id> Cli NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf gNMI gNMI server is running over port 6030 in non-secure mode using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address>:6030 --insecure \\ -u admin -p admin \\ capabilities Info Default user credentials: admin:admin","title":"Managing ceos nodes"},{"location":"manual/kinds/ceos/#interfaces-mapping","text":"ceos container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface When containerlab launches ceos node, it will set IPv4/6 addresses as assigned by docker to the eth0 interface and ceos node will boot with that addresses configure. Data interfaces eth1+ need to be configured with IP addressing manually. ceos interfaces output This output demonstrates the IP addressing of the linux interfaces of ceos node. bash-4.2# ip address 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/24 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever <SNIP> 5877: eth0@if5878: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.2/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2001:172:20:20::2/80 scope global valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe14:1402/64 scope link valid_lft forever preferred_lft forever This output shows how the linux interfaces are mapped into the ceos OS. ceos>sh ip int br Address Interface IP Address Status Protocol MTU Owner ----------------- -------------------- ------------ -------------- ---------- ------- Management0 172.20.20.2/24 up up 1500 ceos>sh ipv6 int br Interface Status MTU IPv6 Address Addr State Addr Source --------------- ------------ ---------- -------------------------------- ---------------- ----------- Ma0 up 1500 fe80::42:acff:fe14:1402/64 up link local 2001:172:20:20::2/80 up config As you see, the management interface Ma0 inherits the IP address that docker assigned to ceos container management interface.","title":"Interfaces mapping"},{"location":"manual/kinds/ceos/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/ceos/#node-configuration","text":"cEOS nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of ceos kind with a basic config or to provide a custom config file that will be used as a startup config instead.","title":"Node configuration"},{"location":"manual/kinds/ceos/#default-node-configuration","text":"When a node is defined without config statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : ceos topology : nodes : ceos : kind : ceos The generated config will be saved by the path clab-<lab_name>/<node-name>/flash/startup-config . Using the example topology presented above, the exact path to the config will be clab-ceos/ceos/flash/startup-config .","title":"Default node configuration"},{"location":"manual/kinds/ceos/#user-defined-config","text":"It is possible to make ceos nodes to boot up with a user-defined config instead of a built-in one. With a config property a user sets the path to the config file that will be mounted to a container and used as a startup config: name : ceos_lab topology : nodes : ceos : kind : ceos config : myconfig.conf With such topology file containerlab is instructed to take a file myconfig.conf from the current working directory, copy it to the lab directory for that specific node under the /flash/startup-config name and mount that dir to the container. This will result in this config to act as a startup config for the node.","title":"User defined config"},{"location":"manual/kinds/ceos/#saving-configuration","text":"With containerlab save command it's possible to save running cEOS configuration into a file. The configuration will be saved by conf-saved.conf path in the relevant node directory.","title":"Saving configuration"},{"location":"manual/kinds/ceos/#container-configuration","text":"To start an Arista cEOS node containerlab uses the configuration instructions described in Arista Forums 1 . The exact parameters are outlined below. Startup command /sbin/init systemd.setenv=INTFTYPE=eth systemd.setenv=ETBA=4 systemd.setenv=SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT=1 systemd.setenv=CEOS=1 systemd.setenv=EOS_PLATFORM=ceoslab systemd.setenv=container=docker systemd.setenv=MAPETH0=1 systemd.setenv=MGMT_INTF=eth0 Environment variables CEOS:1 EOS_PLATFORM\":ceoslab container:docker ETBA:1 SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT:1 INTFTYPE:eth MAPETH0:1 MGMT_INTF:eth0","title":"Container configuration"},{"location":"manual/kinds/ceos/#file-mounts","text":"When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For ceos kind containerlab creates flash directory for each ceos node and mounts these folders by /mnt/flash paths. \u276f tree clab-srlceos01/ceos clab-srlceos01/ceos \u2514\u2500\u2500 flash \u251c\u2500\u2500 AsuFastPktTransmit.log \u251c\u2500\u2500 debug \u2502 \u2514\u2500\u2500 proc \u2502 \u2514\u2500\u2500 modules \u251c\u2500\u2500 fastpkttx.backup \u251c\u2500\u2500 Fossil \u251c\u2500\u2500 kickstart-config \u251c\u2500\u2500 persist \u2502 \u251c\u2500\u2500 local \u2502 \u251c\u2500\u2500 messages \u2502 \u251c\u2500\u2500 persistentRestartLog \u2502 \u251c\u2500\u2500 secure \u2502 \u2514\u2500\u2500 sys \u251c\u2500\u2500 schedule \u2502 \u2514\u2500\u2500 tech-support \u2502 \u2514\u2500\u2500 ceos_tech-support_2021-01-14.0907.log.gz \u251c\u2500\u2500 SsuRestoreLegacy.log \u251c\u2500\u2500 SsuRestore.log \u2514\u2500\u2500 startup-config 9 directories, 11 files","title":"File mounts"},{"location":"manual/kinds/ceos/#lab-examples","text":"The following labs feature cEOS node: SR Linux and cEOS https://eos.arista.com/ceos-lab-topo/ \u21a9","title":"Lab examples"},{"location":"manual/kinds/crpd/","text":"Juniper cRPD # Juniper cRPD is identified with crpd kind in the topology file . A kind defines a supported feature set and a startup procedure of a crpd node. cRPD nodes launched with containerlab comes up pre-provisioned with SSH service enabled, root user created and NETCONF enabled. Managing cRPD nodes # Juniper cRPD node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running cRPD container: docker exec -it <container-name/id> bash CLI to connect to the cRPD CLI docker exec -it <container-name/id> cli NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf Info Default user credentials: root:clab123 Interfaces mapping # cRPD container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface When containerlab launches cRPD node, it will assign IPv4/6 address to the eth0 interface. Data interface eth1 needs to be configured with IP addressing manually. cRPD interfaces output This output demonstrates the IP addressing of the linux interfaces of cRPD node. \u276f docker exec -it clab-crpd-crpd bash ===> Containerized Routing Protocols Daemon (CRPD) Copyright (C) 2020, Juniper Networks, Inc. All rights reserved. <=== root@crpd:/# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever <SNIP> 5767: eth0@if5768: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.3/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2001:172:20:20::3/80 scope global nodad valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe14:1403/64 scope link valid_lft forever preferred_lft forever 5770: eth1@if5769: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether b6:d3:63:f1:cb:7b brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::b4d3:63ff:fef1:cb7b/64 scope link valid_lft forever preferred_lft forever This output shows how the linux interfaces are mapped into the cRPD OS. root@crpd> show interfaces routing Interface State Addresses lsi Up tunl0 Up ISO enabled sit0 Up ISO enabled INET6 ::172.20.20.3 INET6 ::127.0.0.1 lo.0 Up ISO enabled INET6 fe80::1 ip6tnl0 Up ISO enabled INET6 fe80::42a:e9ff:fede:a0e3 gretap0 Down ISO enabled gre0 Up ISO enabled eth1 Up ISO enabled INET6 fe80::b4d3:63ff:fef1:cb7b eth0 Up ISO enabled INET 172.20.20.3 INET6 2001:172:20:20::3 INET6 fe80::42:acff:fe14:1403 As you see, the management interface eth0 inherits the IP address that docker assigned to cRPD container. Features and options # Node configuration # cRPD nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of crpd kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead. Default node configuration # When a node is defined without config statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : crpd topology : nodes : crpd : kind : crpd The generated config will be saved by the path clab-<lab_name>/<node-name>/config/juniper.conf . Using the example topology presented above, the exact path to the config will be clab-crpd/crpd/config/juniper.conf . User defined config # It is possible to make cRPD nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container: name : crpd_lab topology : nodes : crpd : kind : crpd config : myconfig.conf With such topology file containerlab is instructed to take a file myconfig.conf from the current working directory, copy it to the lab directory for that specific node under the /config/juniper.conf name and mount that dir to the container. This will result in this config to act as a startup config for the node. Saving configuration # With containerlab save command it's possible to save running cRPD configuration into a file. The configuration will be saved by conf-saved.conf path in the relevant node directory. License # cRPD containers require a license file to have some features to be activated. With a license directive it's possible to provide a path to a license file that will be copied over to the nodes configuration directory by the /config/license.conf path. Container configuration # To launch cRPD, containerlab uses the deployment instructions that are provided in the TechLibrary as well as leveraging some setup steps outlined by Matt Oswalt in this blog post . The SSH service is already enabled for root login, so nothing is needed to be done additionally. The root user is created already with the clab123 password. File mounts # When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For crpd kind containerlab creates config and log directories for each crpd node and mounts these folders by /config and /var/log paths accordingly. \u276f tree clab-crpd/crpd clab-crpd/crpd \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 juniper.conf \u2502 \u251c\u2500\u2500 license \u2502 \u2502 \u2514\u2500\u2500 safenet \u2502 \u2514\u2500\u2500 sshd_config \u2514\u2500\u2500 log \u251c\u2500\u2500 cscript.log \u251c\u2500\u2500 license \u251c\u2500\u2500 messages \u251c\u2500\u2500 mgd-api \u251c\u2500\u2500 na-grpcd \u251c\u2500\u2500 __policy_names_rpdc__ \u2514\u2500\u2500 __policy_names_rpdn__ 4 directories, 9 files Lab examples # The following labs feature cRPD node: SR Linux and cRPD","title":"crpd - Juniper cRPD"},{"location":"manual/kinds/crpd/#juniper-crpd","text":"Juniper cRPD is identified with crpd kind in the topology file . A kind defines a supported feature set and a startup procedure of a crpd node. cRPD nodes launched with containerlab comes up pre-provisioned with SSH service enabled, root user created and NETCONF enabled.","title":"Juniper cRPD"},{"location":"manual/kinds/crpd/#managing-crpd-nodes","text":"Juniper cRPD node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running cRPD container: docker exec -it <container-name/id> bash CLI to connect to the cRPD CLI docker exec -it <container-name/id> cli NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf Info Default user credentials: root:clab123","title":"Managing cRPD nodes"},{"location":"manual/kinds/crpd/#interfaces-mapping","text":"cRPD container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface When containerlab launches cRPD node, it will assign IPv4/6 address to the eth0 interface. Data interface eth1 needs to be configured with IP addressing manually. cRPD interfaces output This output demonstrates the IP addressing of the linux interfaces of cRPD node. \u276f docker exec -it clab-crpd-crpd bash ===> Containerized Routing Protocols Daemon (CRPD) Copyright (C) 2020, Juniper Networks, Inc. All rights reserved. <=== root@crpd:/# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever <SNIP> 5767: eth0@if5768: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.3/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2001:172:20:20::3/80 scope global nodad valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe14:1403/64 scope link valid_lft forever preferred_lft forever 5770: eth1@if5769: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default link/ether b6:d3:63:f1:cb:7b brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::b4d3:63ff:fef1:cb7b/64 scope link valid_lft forever preferred_lft forever This output shows how the linux interfaces are mapped into the cRPD OS. root@crpd> show interfaces routing Interface State Addresses lsi Up tunl0 Up ISO enabled sit0 Up ISO enabled INET6 ::172.20.20.3 INET6 ::127.0.0.1 lo.0 Up ISO enabled INET6 fe80::1 ip6tnl0 Up ISO enabled INET6 fe80::42a:e9ff:fede:a0e3 gretap0 Down ISO enabled gre0 Up ISO enabled eth1 Up ISO enabled INET6 fe80::b4d3:63ff:fef1:cb7b eth0 Up ISO enabled INET 172.20.20.3 INET6 2001:172:20:20::3 INET6 fe80::42:acff:fe14:1403 As you see, the management interface eth0 inherits the IP address that docker assigned to cRPD container.","title":"Interfaces mapping"},{"location":"manual/kinds/crpd/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/crpd/#node-configuration","text":"cRPD nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of crpd kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead.","title":"Node configuration"},{"location":"manual/kinds/crpd/#default-node-configuration","text":"When a node is defined without config statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : crpd topology : nodes : crpd : kind : crpd The generated config will be saved by the path clab-<lab_name>/<node-name>/config/juniper.conf . Using the example topology presented above, the exact path to the config will be clab-crpd/crpd/config/juniper.conf .","title":"Default node configuration"},{"location":"manual/kinds/crpd/#user-defined-config","text":"It is possible to make cRPD nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container: name : crpd_lab topology : nodes : crpd : kind : crpd config : myconfig.conf With such topology file containerlab is instructed to take a file myconfig.conf from the current working directory, copy it to the lab directory for that specific node under the /config/juniper.conf name and mount that dir to the container. This will result in this config to act as a startup config for the node.","title":"User defined config"},{"location":"manual/kinds/crpd/#saving-configuration","text":"With containerlab save command it's possible to save running cRPD configuration into a file. The configuration will be saved by conf-saved.conf path in the relevant node directory.","title":"Saving configuration"},{"location":"manual/kinds/crpd/#license","text":"cRPD containers require a license file to have some features to be activated. With a license directive it's possible to provide a path to a license file that will be copied over to the nodes configuration directory by the /config/license.conf path.","title":"License"},{"location":"manual/kinds/crpd/#container-configuration","text":"To launch cRPD, containerlab uses the deployment instructions that are provided in the TechLibrary as well as leveraging some setup steps outlined by Matt Oswalt in this blog post . The SSH service is already enabled for root login, so nothing is needed to be done additionally. The root user is created already with the clab123 password.","title":"Container configuration"},{"location":"manual/kinds/crpd/#file-mounts","text":"When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For crpd kind containerlab creates config and log directories for each crpd node and mounts these folders by /config and /var/log paths accordingly. \u276f tree clab-crpd/crpd clab-crpd/crpd \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 juniper.conf \u2502 \u251c\u2500\u2500 license \u2502 \u2502 \u2514\u2500\u2500 safenet \u2502 \u2514\u2500\u2500 sshd_config \u2514\u2500\u2500 log \u251c\u2500\u2500 cscript.log \u251c\u2500\u2500 license \u251c\u2500\u2500 messages \u251c\u2500\u2500 mgd-api \u251c\u2500\u2500 na-grpcd \u251c\u2500\u2500 __policy_names_rpdc__ \u2514\u2500\u2500 __policy_names_rpdn__ 4 directories, 9 files","title":"File mounts"},{"location":"manual/kinds/crpd/#lab-examples","text":"The following labs feature cRPD node: SR Linux and cRPD","title":"Lab examples"},{"location":"manual/kinds/kinds/","text":"Kinds # Containerlab launches, wires up and manages container based labs. The steps required to launch a vanilla debian or centos container image aren't at all different. On the other hand, Nokia SR Linux launching procedure is nothing like the one for Arista cEOS. Things like required syscalls, mounted directories, entrypoints and commands to execute are all different for the containerized NOS'es. To let containerlab understand which launching sequence to use, the notion of a kind was introduced. Essentially kinds abstract away the need to understand certain setup peculiarities of different NOS'es. Given the following topology definition file , containerlab is able to know how to launch node1 as an SR Linux container and node2 as a cEOS one because they are associated with the kinds: name : srlceos01 topology : nodes : node1 : kind : srl # node1 is of srl kind type : ixrd2 image : srlinux:20.6.3-145 license : license.key node2 : kind : ceos # node2 is of ceos kind image : ceos:4.25F links : - endpoints : [ \"node1:e1-1\" , \"node2:eth1\" ] Containerlab supports a fixed number of kinds. Within each predefined kind we store the necessary information that is used to launch the container successfully. The following kinds are supported or in the roadmap of containerlab: Name Kind Status Nokia SR Linux srl supported Juniper cRPD crpd supported Arista cEOS ceos supported SONiC sonic supported Nokia SR OS vr-sros supported Juniper vMX vr-vmx supported Cisco XRv9k vr-xrv9k supported Cisco XRv vr-xrv supported Arista vEOS vr-veos supported Linux container linux supported Linux bridge bridge supported OvS bridge ovs-bridge supported mysocketio node mysocketio supported Refer to a specific kind documentation article to see the details about it.","title":"About"},{"location":"manual/kinds/kinds/#kinds","text":"Containerlab launches, wires up and manages container based labs. The steps required to launch a vanilla debian or centos container image aren't at all different. On the other hand, Nokia SR Linux launching procedure is nothing like the one for Arista cEOS. Things like required syscalls, mounted directories, entrypoints and commands to execute are all different for the containerized NOS'es. To let containerlab understand which launching sequence to use, the notion of a kind was introduced. Essentially kinds abstract away the need to understand certain setup peculiarities of different NOS'es. Given the following topology definition file , containerlab is able to know how to launch node1 as an SR Linux container and node2 as a cEOS one because they are associated with the kinds: name : srlceos01 topology : nodes : node1 : kind : srl # node1 is of srl kind type : ixrd2 image : srlinux:20.6.3-145 license : license.key node2 : kind : ceos # node2 is of ceos kind image : ceos:4.25F links : - endpoints : [ \"node1:e1-1\" , \"node2:eth1\" ] Containerlab supports a fixed number of kinds. Within each predefined kind we store the necessary information that is used to launch the container successfully. The following kinds are supported or in the roadmap of containerlab: Name Kind Status Nokia SR Linux srl supported Juniper cRPD crpd supported Arista cEOS ceos supported SONiC sonic supported Nokia SR OS vr-sros supported Juniper vMX vr-vmx supported Cisco XRv9k vr-xrv9k supported Cisco XRv vr-xrv supported Arista vEOS vr-veos supported Linux container linux supported Linux bridge bridge supported OvS bridge ovs-bridge supported mysocketio node mysocketio supported Refer to a specific kind documentation article to see the details about it.","title":"Kinds"},{"location":"manual/kinds/linux/","text":"Linux container # Labs deployed with containerlab are endlessly flexible, mostly because containerlab can spin up and wire regular containers as part of the lab topology. Nowadays more and more workloads are packaged into containers, and containerlab users can nicely integrate them in their labs following a familiar docker' compose-like syntax. As long as the networking domain is considered, the most common use case for bare linux containers is to introduce \"clients\" or traffic generators which are connected to the network nodes or host telemetry/monitoring stacks. But, of course, you are free to choose which container to add into your lab, there is not restriction to that! Using linux containers # As with any other node, the linux container is a node of a specific kind, linux in this case. # a simple topo of two alpine containers connected with each other name : demo topology : n1 : kind : linux image : alpine:latest n2 : kind : linux image : alpine:latest links : - endpoints : [ \"n1:eth1\" , \"n2:eth1\" ] With a topology file like that, the nodes will start and both containers will have eth1 link available. Containerlab tries to deliver the same level of flexibility in container configuration as docker-compose has. With linux containers it is possible to use the following node configuration parameters: image - to set an image source for the container binds - to mount files from the host to a container ports - to expose services running in the container to a host env - to set environment variables user - to set a user that will be used inside the container system cmd - to provide a command that will be executed when the container is started publish - to provide expose container' service via myscoket.io integration","title":"linux - Linux container"},{"location":"manual/kinds/linux/#linux-container","text":"Labs deployed with containerlab are endlessly flexible, mostly because containerlab can spin up and wire regular containers as part of the lab topology. Nowadays more and more workloads are packaged into containers, and containerlab users can nicely integrate them in their labs following a familiar docker' compose-like syntax. As long as the networking domain is considered, the most common use case for bare linux containers is to introduce \"clients\" or traffic generators which are connected to the network nodes or host telemetry/monitoring stacks. But, of course, you are free to choose which container to add into your lab, there is not restriction to that!","title":"Linux container"},{"location":"manual/kinds/linux/#using-linux-containers","text":"As with any other node, the linux container is a node of a specific kind, linux in this case. # a simple topo of two alpine containers connected with each other name : demo topology : n1 : kind : linux image : alpine:latest n2 : kind : linux image : alpine:latest links : - endpoints : [ \"n1:eth1\" , \"n2:eth1\" ] With a topology file like that, the nodes will start and both containers will have eth1 link available. Containerlab tries to deliver the same level of flexibility in container configuration as docker-compose has. With linux containers it is possible to use the following node configuration parameters: image - to set an image source for the container binds - to mount files from the host to a container ports - to expose services running in the container to a host env - to set environment variables user - to set a user that will be used inside the container system cmd - to provide a command that will be executed when the container is started publish - to provide expose container' service via myscoket.io integration","title":"Using linux containers"},{"location":"manual/kinds/ovs-bridge/","text":"Openvswitch bridge # Similar to linux bridge capability, containerlab allows to connect nodes to an Openvswitch (Ovs) bridge. Ovs bridges offers even more connectivity options compared to classic Linux bridge, as well as it allows to create stretched L2 domain by means of tunneled interfaces (vxlan). Using ovs-bridge kind # Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file , the Ovs bridge needs to be created first. Once the bridge is created, it has to be referenced as a node inside the topology file: name : ovs topology : nodes : myovs : kind : ovs-bridge ceos : kind : ceos image : ceos:latest links : - endpoints : [ \"myovs:ovsp1\" , \"srl:eth1\" ] In the example above, node myovs of kind ovs-bridge tells containerlab to identify it as a Ovs bridge and look for a bridge named myovs . When connecting other nodes to a bridge, the bridge endpoint must be present in the links section. Note When choosing names of the interfaces that need to be connected to the bridge make sure that these names are not clashing with existing interfaces. In the example above we attach a single interfaces named ovsp1 to the Ovs bridge with a name myovs . Before that, we ensured that no other interfaces named ovsp1 existed. As a result of such topology definition, you will see the Ovs bridge br-clab with three interfaces attached to it: \u276f ovs-vsctl show 918a3466-4368-4167-9162-f2cf80a0c106 Bridge myovs Port myovs Interface myovs type: internal Port eth1 Interface ovsp1 ovs_version: \"2.13.1\"","title":"ovs-bridge - Openvswitch bridge"},{"location":"manual/kinds/ovs-bridge/#openvswitch-bridge","text":"Similar to linux bridge capability, containerlab allows to connect nodes to an Openvswitch (Ovs) bridge. Ovs bridges offers even more connectivity options compared to classic Linux bridge, as well as it allows to create stretched L2 domain by means of tunneled interfaces (vxlan).","title":"Openvswitch bridge"},{"location":"manual/kinds/ovs-bridge/#using-ovs-bridge-kind","text":"Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file , the Ovs bridge needs to be created first. Once the bridge is created, it has to be referenced as a node inside the topology file: name : ovs topology : nodes : myovs : kind : ovs-bridge ceos : kind : ceos image : ceos:latest links : - endpoints : [ \"myovs:ovsp1\" , \"srl:eth1\" ] In the example above, node myovs of kind ovs-bridge tells containerlab to identify it as a Ovs bridge and look for a bridge named myovs . When connecting other nodes to a bridge, the bridge endpoint must be present in the links section. Note When choosing names of the interfaces that need to be connected to the bridge make sure that these names are not clashing with existing interfaces. In the example above we attach a single interfaces named ovsp1 to the Ovs bridge with a name myovs . Before that, we ensured that no other interfaces named ovsp1 existed. As a result of such topology definition, you will see the Ovs bridge br-clab with three interfaces attached to it: \u276f ovs-vsctl show 918a3466-4368-4167-9162-f2cf80a0c106 Bridge myovs Port myovs Interface myovs type: internal Port eth1 Interface ovsp1 ovs_version: \"2.13.1\"","title":"Using ovs-bridge kind"},{"location":"manual/kinds/sonic-vs/","text":"SONiC # SONiC is identified with sonic-vs kind in the topology file . A kind defines a supported feature set and a startup procedure of a sonic-vs node. Info vs in the name of a kind refers to a SONiC platform type. sonic-vs nodes launched with containerlab comes without any additional configuration. Managing sonic-vs nodes # SONiC node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running sonic-vs container: docker exec -it <container-name/id> bash CLI to connect to the sonic-vs CLI (vtysh) docker exec -it <container-name/id> vtysh Interfaces mapping # sonic-vs container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data (front-panel port) interface When containerlab launches sonic-vs node, it will assign IPv4/6 address to the eth0 interface. Data interface eth1 which is mapped to Ethernet0 port needs to be configured with IP addressing manually. See Lab examples for exact configurations. Lab examples # The following labs feature sonic-vs node: SR Linux and sonic-vs","title":"sonic-vs - SONiC"},{"location":"manual/kinds/sonic-vs/#sonic","text":"SONiC is identified with sonic-vs kind in the topology file . A kind defines a supported feature set and a startup procedure of a sonic-vs node. Info vs in the name of a kind refers to a SONiC platform type. sonic-vs nodes launched with containerlab comes without any additional configuration.","title":"SONiC"},{"location":"manual/kinds/sonic-vs/#managing-sonic-vs-nodes","text":"SONiC node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running sonic-vs container: docker exec -it <container-name/id> bash CLI to connect to the sonic-vs CLI (vtysh) docker exec -it <container-name/id> vtysh","title":"Managing sonic-vs nodes"},{"location":"manual/kinds/sonic-vs/#interfaces-mapping","text":"sonic-vs container uses the following mapping for its linux interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data (front-panel port) interface When containerlab launches sonic-vs node, it will assign IPv4/6 address to the eth0 interface. Data interface eth1 which is mapped to Ethernet0 port needs to be configured with IP addressing manually. See Lab examples for exact configurations.","title":"Interfaces mapping"},{"location":"manual/kinds/sonic-vs/#lab-examples","text":"The following labs feature sonic-vs node: SR Linux and sonic-vs","title":"Lab examples"},{"location":"manual/kinds/srl/","text":"Nokia SR Linux # Nokia SR Linux NOS is identified with srl kind in the topology file . A kind defines a supported feature set and a startup procedure of a node. Managing SR Linux nodes # There are many ways to manage SR Linux nodes, ranging from classic CLI management all the way up to the gNMI programming. Here is a short summary on how to access those interfaces: bash to connect to a bash shell of a running SR Linux container: docker exec -it <container-name/id> bash CLI/SSH to connect to the SR Linux CLI docker exec -it <container-name/id> sr_cli or with SSH ssh admin@<container-name> gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --skip-verify \\ -u admin -p admin \\ -e json_ietf \\ get --path /system/name/host-name Info Default user credentials: admin:admin Features and options # Types # For SR Linux nodes type defines the hardware variant that this node will emulate. The available type values are: ixr6 , ixr10 , ixrd1 , ixrd2 , ixrd3 which correspond to a hardware variant of Nokia 7250/7220 IXR chassis. By default, ixr6 type will be used by containerlab. Based on the provided type, containerlab will generate the topology file that will be mounted to SR Linux container and make it boot in a chosen HW variant. Node configuration # SR Linux nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of srl kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead. Default node configuration # When a node is defined without config statement present, containerlab will generate an empty config from this template and put it in that directory. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : srl_lab topology : nodes : srl1 : kind : srl type : ixr6 license : lic.key The generated config will be saved by the path clab-<lab_name>/<node-name>/config/config.json . Using the example topology presented above, the exact path to the config will be clab-srl_lab/srl1/config/config.json . User defined config # It is possible to make SR Linux nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container: name : srl_lab topology : nodes : srl1 : kind : srl type : ixr6 license : lic.key config : myconfig.json With such topology file containerlab is instructed to take a file myconfig.json from the current working directory, copy it to the lab directory for that specific node under the config.json name and mount that file to the container. This will result in this config to act as a startup config for the node. Saving configuration # As was explained in the Node configuration section, SR Linux containers can make their config persist because config files are provided to the containers from the host via bind mount. There are two options to make a running configuration to be saved in a file. Rewriting startup configuration # When a user configures SR Linux node via CLI the changes are saved into the running configuration stored in memory. To save the running configuration as a startup configuration the user needs to execute the tools system configuration save CLI command. This will write the config to the config.json file that holds the startup config and is exposed to the host. Generating config checkpoint # If the startup configuration must be left intact, use an alternative method of saving the configuration checkpoint: tools system configuration generate-checkpoint . This command will create a checkpoint-x.json file that you will be able to find in the same config directory. Containerlab allows to perform a bulk configuration-save operation that can be executed with containerlab save -t <path-to-topo-file> command. With this command, every node that supports the \"save\" operation will execute a command to save it's running configuration to a persistent location. For SR Linux nodes the save command will trigger the checkpoint generation: \u276f containerlab save -t srl02.clab.yml INFO[0000] Getting topology information from ../srl02.clab.yml file... INFO[0001] clab-srl02-srl1 output: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-12-03T15:12:46.854Z' and comment '' INFO[0004] clab-srl02-srl2 output: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-12-03T15:12:49.892Z' and comment '' TLS # By default containerlab will generate TLS certificates and keys for each SR Linux node of a lab. The TLS related files that containerlab creates are located in the so-called CA directory which can be located by the <lab-directory>/ca/ path. Here is a list of files that containerlab creates relative to the CA directory Root CA certificate - root/root-ca.pem Root CA private key - root/root-ca-key.pem Node certificate - <node-name>/<node-name>.pem Node private key - <node-name>/<node-name>-key.pem The generated TLS files will persist between lab deployments. This means that if you destroyed a lab and deployed it again, the TLS files from initial lab deployment will be used. In case a user-provided certificates/keys need to be used, the root-ca.pem , <node-name>.pem and <node-name>-key.pem files must be copied by the paths outlined above for containerlab to take them into account when deploying a lab. In case only root-ca.pem and root-ca-key.pem files are provided, the node certificates will be generated using these CA files. License # SR Linux containers require a license file to be provided. With a license directive it's possible to provide a path to a license file that will be used for srl nodes. Container configuration # To start an SR Linux NOS containerlab uses the configuration that is described in SR Linux Software Installation Guide Startup command sudo bash -c /opt/srlinux/bin/sr_linux Syscalls net.ipv4.ip_forward = \"0\" net.ipv6.conf.all.disable_ipv6 = \"0\" net.ipv6.conf.all.accept_dad = \"0\" net.ipv6.conf.default.accept_dad = \"0\" net.ipv6.conf.all.autoconf = \"0\" net.ipv6.conf.default.autoconf = \"0\" Environment variables SRLINUX=1 File mounts # Config directory # When a user starts a lab, containerlab creates a lab directory for storing configuration artifacts . For srl kind containerlab creates directories for each node of that kind. ~/clab/clab-srl02 \u276f ls -lah srl1 drwxrwxrwx+ 6 1002 1002 87 Dec 1 22:11 config -rw-r--r-- 1 root root 2.8K Dec 1 22:11 license.key -rw-r--r-- 1 root root 4.4K Dec 1 22:11 srlinux.conf -rw-r--r-- 1 root root 233 Dec 1 22:11 topology.clab.yml The config directory is mounted to container's /etc/opt/srlinux/ in rw mode and will effectively contain configuration that SR Linux runs of as well as the files that SR Linux keeps in its /etc/opt/srlinux/ directory: \u276f ls srl1/config banner cli config.json devices tls ztp CLI env config # Another file that SR Linux expects to have is the srlinux.conf file that contains CLI environment config. Containerlab uses a template of this file and mounts it to /home/admin/.srlinux.conf in rw mode. Topology file # The topology file that defines the emulated hardware type is driven by the value of the kinds type parameter. Depending on a specified type the appropriate content will be populated into the topology.yml file that will get mounted to /tmp/topology.yml directory inside the container in ro mode.","title":"srl - Nokia SR Linux"},{"location":"manual/kinds/srl/#nokia-sr-linux","text":"Nokia SR Linux NOS is identified with srl kind in the topology file . A kind defines a supported feature set and a startup procedure of a node.","title":"Nokia SR Linux"},{"location":"manual/kinds/srl/#managing-sr-linux-nodes","text":"There are many ways to manage SR Linux nodes, ranging from classic CLI management all the way up to the gNMI programming. Here is a short summary on how to access those interfaces: bash to connect to a bash shell of a running SR Linux container: docker exec -it <container-name/id> bash CLI/SSH to connect to the SR Linux CLI docker exec -it <container-name/id> sr_cli or with SSH ssh admin@<container-name> gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --skip-verify \\ -u admin -p admin \\ -e json_ietf \\ get --path /system/name/host-name Info Default user credentials: admin:admin","title":"Managing SR Linux nodes"},{"location":"manual/kinds/srl/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/srl/#types","text":"For SR Linux nodes type defines the hardware variant that this node will emulate. The available type values are: ixr6 , ixr10 , ixrd1 , ixrd2 , ixrd3 which correspond to a hardware variant of Nokia 7250/7220 IXR chassis. By default, ixr6 type will be used by containerlab. Based on the provided type, containerlab will generate the topology file that will be mounted to SR Linux container and make it boot in a chosen HW variant.","title":"Types"},{"location":"manual/kinds/srl/#node-configuration","text":"SR Linux nodes have a dedicated config directory that is used to persist the configuration of the node. It is possible to launch nodes of srl kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead.","title":"Node configuration"},{"location":"manual/kinds/srl/#default-node-configuration","text":"When a node is defined without config statement present, containerlab will generate an empty config from this template and put it in that directory. # example of a topo file that does not define a custom config # as a result, the config will be generated from a template # and used by this node name : srl_lab topology : nodes : srl1 : kind : srl type : ixr6 license : lic.key The generated config will be saved by the path clab-<lab_name>/<node-name>/config/config.json . Using the example topology presented above, the exact path to the config will be clab-srl_lab/srl1/config/config.json .","title":"Default node configuration"},{"location":"manual/kinds/srl/#user-defined-config","text":"It is possible to make SR Linux nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container: name : srl_lab topology : nodes : srl1 : kind : srl type : ixr6 license : lic.key config : myconfig.json With such topology file containerlab is instructed to take a file myconfig.json from the current working directory, copy it to the lab directory for that specific node under the config.json name and mount that file to the container. This will result in this config to act as a startup config for the node.","title":"User defined config"},{"location":"manual/kinds/srl/#saving-configuration","text":"As was explained in the Node configuration section, SR Linux containers can make their config persist because config files are provided to the containers from the host via bind mount. There are two options to make a running configuration to be saved in a file.","title":"Saving configuration"},{"location":"manual/kinds/srl/#rewriting-startup-configuration","text":"When a user configures SR Linux node via CLI the changes are saved into the running configuration stored in memory. To save the running configuration as a startup configuration the user needs to execute the tools system configuration save CLI command. This will write the config to the config.json file that holds the startup config and is exposed to the host.","title":"Rewriting startup configuration"},{"location":"manual/kinds/srl/#generating-config-checkpoint","text":"If the startup configuration must be left intact, use an alternative method of saving the configuration checkpoint: tools system configuration generate-checkpoint . This command will create a checkpoint-x.json file that you will be able to find in the same config directory. Containerlab allows to perform a bulk configuration-save operation that can be executed with containerlab save -t <path-to-topo-file> command. With this command, every node that supports the \"save\" operation will execute a command to save it's running configuration to a persistent location. For SR Linux nodes the save command will trigger the checkpoint generation: \u276f containerlab save -t srl02.clab.yml INFO[0000] Getting topology information from ../srl02.clab.yml file... INFO[0001] clab-srl02-srl1 output: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-12-03T15:12:46.854Z' and comment '' INFO[0004] clab-srl02-srl2 output: /system: Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-12-03T15:12:49.892Z' and comment ''","title":"Generating config checkpoint"},{"location":"manual/kinds/srl/#tls","text":"By default containerlab will generate TLS certificates and keys for each SR Linux node of a lab. The TLS related files that containerlab creates are located in the so-called CA directory which can be located by the <lab-directory>/ca/ path. Here is a list of files that containerlab creates relative to the CA directory Root CA certificate - root/root-ca.pem Root CA private key - root/root-ca-key.pem Node certificate - <node-name>/<node-name>.pem Node private key - <node-name>/<node-name>-key.pem The generated TLS files will persist between lab deployments. This means that if you destroyed a lab and deployed it again, the TLS files from initial lab deployment will be used. In case a user-provided certificates/keys need to be used, the root-ca.pem , <node-name>.pem and <node-name>-key.pem files must be copied by the paths outlined above for containerlab to take them into account when deploying a lab. In case only root-ca.pem and root-ca-key.pem files are provided, the node certificates will be generated using these CA files.","title":"TLS"},{"location":"manual/kinds/srl/#license","text":"SR Linux containers require a license file to be provided. With a license directive it's possible to provide a path to a license file that will be used for srl nodes.","title":"License"},{"location":"manual/kinds/srl/#container-configuration","text":"To start an SR Linux NOS containerlab uses the configuration that is described in SR Linux Software Installation Guide Startup command sudo bash -c /opt/srlinux/bin/sr_linux Syscalls net.ipv4.ip_forward = \"0\" net.ipv6.conf.all.disable_ipv6 = \"0\" net.ipv6.conf.all.accept_dad = \"0\" net.ipv6.conf.default.accept_dad = \"0\" net.ipv6.conf.all.autoconf = \"0\" net.ipv6.conf.default.autoconf = \"0\" Environment variables SRLINUX=1","title":"Container configuration"},{"location":"manual/kinds/srl/#file-mounts","text":"","title":"File mounts"},{"location":"manual/kinds/srl/#config-directory","text":"When a user starts a lab, containerlab creates a lab directory for storing configuration artifacts . For srl kind containerlab creates directories for each node of that kind. ~/clab/clab-srl02 \u276f ls -lah srl1 drwxrwxrwx+ 6 1002 1002 87 Dec 1 22:11 config -rw-r--r-- 1 root root 2.8K Dec 1 22:11 license.key -rw-r--r-- 1 root root 4.4K Dec 1 22:11 srlinux.conf -rw-r--r-- 1 root root 233 Dec 1 22:11 topology.clab.yml The config directory is mounted to container's /etc/opt/srlinux/ in rw mode and will effectively contain configuration that SR Linux runs of as well as the files that SR Linux keeps in its /etc/opt/srlinux/ directory: \u276f ls srl1/config banner cli config.json devices tls ztp","title":"Config directory"},{"location":"manual/kinds/srl/#cli-env-config","text":"Another file that SR Linux expects to have is the srlinux.conf file that contains CLI environment config. Containerlab uses a template of this file and mounts it to /home/admin/.srlinux.conf in rw mode.","title":"CLI env config"},{"location":"manual/kinds/srl/#topology-file","text":"The topology file that defines the emulated hardware type is driven by the value of the kinds type parameter. Depending on a specified type the appropriate content will be populated into the topology.yml file that will get mounted to /tmp/topology.yml directory inside the container in ro mode.","title":"Topology file"},{"location":"manual/kinds/vr-sros/","text":"Nokia SR OS # Nokia SR OS virtualized router is identified with vr-sros kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-sros nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled. Managing vr-sros nodes # Note Containers with SR OS inside will take ~3min to fully boot. You can monitor the progress with watch docker ps waiting till the status will change to healthy . Nokia SR OS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-sros container: docker exec -it <container-name/id> bash CLI to connect to the SR OS CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u admin -p admin \\ capabilities Telnet serial port (console) is exposed over TCP port 5000: # from container host telnet <node-name> 5000 You can also connect to the container and use telnet localhost 5000 if telnet is not available on your container host. Info Default user credentials: admin:admin Interfaces mapping # vr-sros container uses the following mapping for its interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of SR OS line card eth2+ - second and subsequent data interface data interfaces need to be defined consequently Note, that data interfaces need to be defined in the topology file in a consequent fashion. Meaning that no gaps are allowed between the interfaces definition: links : - endpoints : [ \"srl:e1-1\" , \"sros:eth1\" ] # this is not allowed by vr-sros kind. # if eth5 is needed, define eth2-4 as well - endpoints : [ \"srl:e1-5\" , \"sros:eth5\" ] When containerlab launches vr-sros node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Variants # Virtual SR OS simulator can be run in multiple HW variants as explained in the vSIM installation guide . vr-sros container images come with pre-packaged SR OS variants as defined in the upstream repo as well as support custom variant definition . The pre-packaged variants are identified by the variant name and come up with cards and mda already configured. Custom variants, on the other hand, give users the full flexibility in emulated hardware configuration, but cards and MDAs would need to be configured manually. To make vr-sros to boot in one of the packaged variants use its name like that: topology : nodes : sros : kind : vr-sros image : vrnetlab/vr-sros:20.10.R1 type : sr-1s # if type omitted, the default sr-1 variant will be used license : license-sros20.txt Custom variant can be defined as simple as that: # for distributed chassis CPM and IOM are indicated with markers cp: and lc: # notice the delimiter string `___` that MUST be present between CPM and IOM portions # max_nics value is provided in `lc` part. # mem is provided in GB # quote the string value type : \"cp: cpu=2 ram=4 chassis=ixr-e slot=A card=cpm-ixr-e ___ lc: cpu=2 ram=4 max_nics=34 chassis=ixr-e slot=1 card=imm24-sfp++8-sfp28+2-qsfp28 mda/1=m24-sfp++8-sfp28+2-qsfp28\" # an integrated custom type definition # note, no `cp:` marker is needed type : \"cpu=2 ram=4 slot=A chassis=ixr-r6 card=cpiom-ixr-r6 mda/1=m6-10g-sfp++4-25g-sfp28\" Node configuration # vr-sros nodes come up with a basic \"blank\" configuration where only the card/mda are provisioned, as well as the management interfaces such as Netconf, SNMP, gNMI. User defined config # It is possible to make SR OS nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container and used as a startup config: name : sros_lab topology : nodes : sros : kind : vr-sros config : myconfig.txt With such topology file containerlab is instructed to take a file myconfig.txt from the current working directory, copy it to the lab directory for that specific node under the /tftpboot/config.txt name and mount that dir to the container. This will result in this config to act as a startup config for the node. Configuration save # Containerlab's save command will perform a configuration save for vr-sros nodes via Netconf. The configuration will be saved under config.txt file and can be found at the node's directory inside the lab parent directory: # assuming the lab name is \"cert01\" # and node name is \"sr\" cat clab-cert01/sr/tftpboot/config.txt License # Path to a valid license must be provided for all vr-sros nodes with a license directive. File mounts # When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For vr-sros kind containerlab creates tftpboot directory where the license file will be copied. Lab examples # The following labs feature vr-sros node: SR Linux and vr-sros","title":"vr-sros - Nokia SR OS"},{"location":"manual/kinds/vr-sros/#nokia-sr-os","text":"Nokia SR OS virtualized router is identified with vr-sros kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-sros nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.","title":"Nokia SR OS"},{"location":"manual/kinds/vr-sros/#managing-vr-sros-nodes","text":"Note Containers with SR OS inside will take ~3min to fully boot. You can monitor the progress with watch docker ps waiting till the status will change to healthy . Nokia SR OS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-sros container: docker exec -it <container-name/id> bash CLI to connect to the SR OS CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh root@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u admin -p admin \\ capabilities Telnet serial port (console) is exposed over TCP port 5000: # from container host telnet <node-name> 5000 You can also connect to the container and use telnet localhost 5000 if telnet is not available on your container host. Info Default user credentials: admin:admin","title":"Managing vr-sros nodes"},{"location":"manual/kinds/vr-sros/#interfaces-mapping","text":"vr-sros container uses the following mapping for its interfaces: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of SR OS line card eth2+ - second and subsequent data interface data interfaces need to be defined consequently Note, that data interfaces need to be defined in the topology file in a consequent fashion. Meaning that no gaps are allowed between the interfaces definition: links : - endpoints : [ \"srl:e1-1\" , \"sros:eth1\" ] # this is not allowed by vr-sros kind. # if eth5 is needed, define eth2-4 as well - endpoints : [ \"srl:e1-5\" , \"sros:eth5\" ] When containerlab launches vr-sros node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-sros/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-sros/#variants","text":"Virtual SR OS simulator can be run in multiple HW variants as explained in the vSIM installation guide . vr-sros container images come with pre-packaged SR OS variants as defined in the upstream repo as well as support custom variant definition . The pre-packaged variants are identified by the variant name and come up with cards and mda already configured. Custom variants, on the other hand, give users the full flexibility in emulated hardware configuration, but cards and MDAs would need to be configured manually. To make vr-sros to boot in one of the packaged variants use its name like that: topology : nodes : sros : kind : vr-sros image : vrnetlab/vr-sros:20.10.R1 type : sr-1s # if type omitted, the default sr-1 variant will be used license : license-sros20.txt Custom variant can be defined as simple as that: # for distributed chassis CPM and IOM are indicated with markers cp: and lc: # notice the delimiter string `___` that MUST be present between CPM and IOM portions # max_nics value is provided in `lc` part. # mem is provided in GB # quote the string value type : \"cp: cpu=2 ram=4 chassis=ixr-e slot=A card=cpm-ixr-e ___ lc: cpu=2 ram=4 max_nics=34 chassis=ixr-e slot=1 card=imm24-sfp++8-sfp28+2-qsfp28 mda/1=m24-sfp++8-sfp28+2-qsfp28\" # an integrated custom type definition # note, no `cp:` marker is needed type : \"cpu=2 ram=4 slot=A chassis=ixr-r6 card=cpiom-ixr-r6 mda/1=m6-10g-sfp++4-25g-sfp28\"","title":"Variants"},{"location":"manual/kinds/vr-sros/#node-configuration","text":"vr-sros nodes come up with a basic \"blank\" configuration where only the card/mda are provisioned, as well as the management interfaces such as Netconf, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-sros/#user-defined-config","text":"It is possible to make SR OS nodes to boot up with a user-defined config instead of a built-in one. With a config property of the node/kind a user sets the path to the config file that will be mounted to a container and used as a startup config: name : sros_lab topology : nodes : sros : kind : vr-sros config : myconfig.txt With such topology file containerlab is instructed to take a file myconfig.txt from the current working directory, copy it to the lab directory for that specific node under the /tftpboot/config.txt name and mount that dir to the container. This will result in this config to act as a startup config for the node.","title":"User defined config"},{"location":"manual/kinds/vr-sros/#configuration-save","text":"Containerlab's save command will perform a configuration save for vr-sros nodes via Netconf. The configuration will be saved under config.txt file and can be found at the node's directory inside the lab parent directory: # assuming the lab name is \"cert01\" # and node name is \"sr\" cat clab-cert01/sr/tftpboot/config.txt","title":"Configuration save"},{"location":"manual/kinds/vr-sros/#license","text":"Path to a valid license must be provided for all vr-sros nodes with a license directive.","title":"License"},{"location":"manual/kinds/vr-sros/#file-mounts","text":"When a user starts a lab, containerlab creates a node directory for storing configuration artifacts . For vr-sros kind containerlab creates tftpboot directory where the license file will be copied.","title":"File mounts"},{"location":"manual/kinds/vr-sros/#lab-examples","text":"The following labs feature vr-sros node: SR Linux and vr-sros","title":"Lab examples"},{"location":"manual/kinds/vr-veos/","text":"Arista vEOS # Arista vEOS virtualized router is identified with vr-veos kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-veos nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled. Managing vr-veos nodes # Note Containers with vEOS inside will take ~4min to fully boot. You can monitor the progress with docker logs -f <container-name> . Arista vEOS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-veos container: docker exec -it <container-name/id> bash CLI to connect to the vEOS CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh admin@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address>:6030 --insecure \\ -u admin -p admin \\ capabilities Note, gNMI service runs over 6030 port. Info Default user credentials: admin:admin Interfaces mapping # vr-veos container can have up to 144 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of vEOS line card eth2+ - second and subsequent data interface When containerlab launches vr-veos node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Node configuration # vr-veos nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the admin user and management interfaces such as NETCONF, SNMP, gNMI.","title":"vr-veos - Arista vEOS"},{"location":"manual/kinds/vr-veos/#arista-veos","text":"Arista vEOS virtualized router is identified with vr-veos kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-veos nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.","title":"Arista vEOS"},{"location":"manual/kinds/vr-veos/#managing-vr-veos-nodes","text":"Note Containers with vEOS inside will take ~4min to fully boot. You can monitor the progress with docker logs -f <container-name> . Arista vEOS node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-veos container: docker exec -it <container-name/id> bash CLI to connect to the vEOS CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh admin@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address>:6030 --insecure \\ -u admin -p admin \\ capabilities Note, gNMI service runs over 6030 port. Info Default user credentials: admin:admin","title":"Managing vr-veos nodes"},{"location":"manual/kinds/vr-veos/#interfaces-mapping","text":"vr-veos container can have up to 144 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of vEOS line card eth2+ - second and subsequent data interface When containerlab launches vr-veos node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-veos/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-veos/#node-configuration","text":"vr-veos nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the admin user and management interfaces such as NETCONF, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-vmx/","text":"Juniper vMX # Juniper vMX virtualized router is identified with vr-vmx kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-vmx nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled. Managing vr-vmx nodes # Note Containers with vMX inside will take ~7min to fully boot. You can monitor the progress with docker logs -f <container-name> . Juniper vMX node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-vmx container: docker exec -it <container-name/id> bash CLI via SSH to connect to the vMX CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh admin@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u admin -p admin@123 \\ capabilities Info Default user credentials: admin:admin@123 Interfaces mapping # vr-vmx container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of vMX line card eth2+ - second and subsequent data interface When containerlab launches vr-vmx node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Node configuration # vr-vmx nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the admin users and management interfaces such as NETCONF, SNMP, gNMI. Lab examples # The following labs feature vr-vmx node: SR Linux and Juniper vMX Known issues and limitations # when listing docker containers, vr-vmx container will always report unhealthy status. Do not rely on this status. LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab. vMX requires Linux kernel 4.17+ To check the boot log, use docker logs -f <node-name> .","title":"vr-vmx - Juniper vMX"},{"location":"manual/kinds/vr-vmx/#juniper-vmx","text":"Juniper vMX virtualized router is identified with vr-vmx kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-vmx nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.","title":"Juniper vMX"},{"location":"manual/kinds/vr-vmx/#managing-vr-vmx-nodes","text":"Note Containers with vMX inside will take ~7min to fully boot. You can monitor the progress with docker logs -f <container-name> . Juniper vMX node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-vmx container: docker exec -it <container-name/id> bash CLI via SSH to connect to the vMX CLI ssh admin@<container-name/id> NETCONF NETCONF server is running over port 830 ssh admin@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u admin -p admin@123 \\ capabilities Info Default user credentials: admin:admin@123","title":"Managing vr-vmx nodes"},{"location":"manual/kinds/vr-vmx/#interfaces-mapping","text":"vr-vmx container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of vMX line card eth2+ - second and subsequent data interface When containerlab launches vr-vmx node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-vmx/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-vmx/#node-configuration","text":"vr-vmx nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the admin users and management interfaces such as NETCONF, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-vmx/#lab-examples","text":"The following labs feature vr-vmx node: SR Linux and Juniper vMX","title":"Lab examples"},{"location":"manual/kinds/vr-vmx/#known-issues-and-limitations","text":"when listing docker containers, vr-vmx container will always report unhealthy status. Do not rely on this status. LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab. vMX requires Linux kernel 4.17+ To check the boot log, use docker logs -f <node-name> .","title":"Known issues and limitations"},{"location":"manual/kinds/vr-xrv/","text":"Cisco XRv # Cisco XRv virtualized router is identified with vr-xrv kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-xrv nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled. Warning XRv image is discontinued by Cisco and supreceded by XRv 9000 image. It was added to containerlab because the image is lightweight, compared to XRv9k. If recent features are needed, use vr-xrv9k kind. Managing vr-xrv nodes # Note Containers with XRv inside will take ~5min to fully boot. You can monitor the progress with docker logs -f <container-name> . Cisco XRv node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-xrv container: docker exec -it <container-name/id> bash CLI via SSH to connect to the XRv CLI ssh clab@<container-name/id> NETCONF NETCONF server is running over port 830 ssh clab@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u clab -p clab@123 \\ capabilities Info Default user credentials: clab:clab@123 Interfaces mapping # vr-xrv container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of XRv line card eth2+ - second and subsequent data interface When containerlab launches vr-xrv node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Node configuration # vr-xrv nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the clab user and management interfaces such as NETCONF, SNMP, gNMI. Lab examples # The following labs feature vr-xrv node: SR Linux and Cisco XRv Known issues and limitations # LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.","title":"vr-xrv - Cisco XRv"},{"location":"manual/kinds/vr-xrv/#cisco-xrv","text":"Cisco XRv virtualized router is identified with vr-xrv kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-xrv nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled. Warning XRv image is discontinued by Cisco and supreceded by XRv 9000 image. It was added to containerlab because the image is lightweight, compared to XRv9k. If recent features are needed, use vr-xrv9k kind.","title":"Cisco XRv"},{"location":"manual/kinds/vr-xrv/#managing-vr-xrv-nodes","text":"Note Containers with XRv inside will take ~5min to fully boot. You can monitor the progress with docker logs -f <container-name> . Cisco XRv node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-xrv container: docker exec -it <container-name/id> bash CLI via SSH to connect to the XRv CLI ssh clab@<container-name/id> NETCONF NETCONF server is running over port 830 ssh clab@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u clab -p clab@123 \\ capabilities Info Default user credentials: clab:clab@123","title":"Managing vr-xrv nodes"},{"location":"manual/kinds/vr-xrv/#interfaces-mapping","text":"vr-xrv container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of XRv line card eth2+ - second and subsequent data interface When containerlab launches vr-xrv node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-xrv/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-xrv/#node-configuration","text":"vr-xrv nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the clab user and management interfaces such as NETCONF, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-xrv/#lab-examples","text":"The following labs feature vr-xrv node: SR Linux and Cisco XRv","title":"Lab examples"},{"location":"manual/kinds/vr-xrv/#known-issues-and-limitations","text":"LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.","title":"Known issues and limitations"},{"location":"manual/kinds/vr-xrv9k/","text":"Cisco XRv9k # Cisco XRv9k virtualized router is identified with vr-xrv9k kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-xrv9k nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled. Warning XRv9k node is a resource hungry image. As of XRv9k 7.2.1 version the minimum resources should be set to 2vcpu/14GB. These are the default setting set with containerlab for this kind. Image will take 25 minutes to fully boot, be patient. You can monitor the loading status with docker logs -f <container-name> . Managing vr-xrv9k nodes # Cisco XRv9k node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-xrv9k container: docker exec -it <container-name/id> bash CLI via SSH to connect to the XRv9kCLI ssh clab@<container-name/id> NETCONF NETCONF server is running over port 830 ssh clab@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u clab -p clab@123 \\ capabilities Info Default user credentials: clab:clab@123 Interfaces mapping # vr-xrv9k container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of XRv9k line card eth2+ - second and subsequent data interface When containerlab launches vr-xrv9k node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols. Features and options # Node configuration # vr-xrv9k nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the clab user and management interfaces such as NETCONF, SNMP, gNMI. Lab examples # The following labs feature vr-xrv9k node: SR Linux and Cisco XRv9k Known issues and limitations # LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.","title":"vr-xrv9k - Cisco XRv9k"},{"location":"manual/kinds/vr-xrv9k/#cisco-xrv9k","text":"Cisco XRv9k virtualized router is identified with vr-xrv9k kind in the topology file . It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format. vr-xrv9k nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled. Warning XRv9k node is a resource hungry image. As of XRv9k 7.2.1 version the minimum resources should be set to 2vcpu/14GB. These are the default setting set with containerlab for this kind. Image will take 25 minutes to fully boot, be patient. You can monitor the loading status with docker logs -f <container-name> .","title":"Cisco XRv9k"},{"location":"manual/kinds/vr-xrv9k/#managing-vr-xrv9k-nodes","text":"Cisco XRv9k node launched with containerlab can be managed via the following interfaces: bash to connect to a bash shell of a running vr-xrv9k container: docker exec -it <container-name/id> bash CLI via SSH to connect to the XRv9kCLI ssh clab@<container-name/id> NETCONF NETCONF server is running over port 830 ssh clab@<container-name> -p 830 -s netconf gNMI using the best in class gnmic gNMI client as an example: gnmic -a <container-name/node-mgmt-address> --insecure \\ -u clab -p clab@123 \\ capabilities Info Default user credentials: clab:clab@123","title":"Managing vr-xrv9k nodes"},{"location":"manual/kinds/vr-xrv9k/#interfaces-mapping","text":"vr-xrv9k container can have up to 90 interfaces and uses the following mapping rules: eth0 - management interface connected to the containerlab management network eth1 - first data interface, mapped to first data port of XRv9k line card eth2+ - second and subsequent data interface When containerlab launches vr-xrv9k node, it will assign IPv4/6 address to the eth0 interface. These addresses can be used to reach management plane of the router. Data interfaces eth1+ needs to be configured with IP addressing manually using CLI/management protocols.","title":"Interfaces mapping"},{"location":"manual/kinds/vr-xrv9k/#features-and-options","text":"","title":"Features and options"},{"location":"manual/kinds/vr-xrv9k/#node-configuration","text":"vr-xrv9k nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the clab user and management interfaces such as NETCONF, SNMP, gNMI.","title":"Node configuration"},{"location":"manual/kinds/vr-xrv9k/#lab-examples","text":"The following labs feature vr-xrv9k node: SR Linux and Cisco XRv9k","title":"Lab examples"},{"location":"manual/kinds/vr-xrv9k/#known-issues-and-limitations","text":"LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.","title":"Known issues and limitations"},{"location":"rn/0.11.0/","text":"Release 0.11.0 # 2021-03-09 Multi-node labs with VxLAN tunneling # Most containerlab users are fine with running labs within a single server/VM. Truthfully speaking, a VM with 32GB RAM can host a very decent lab with dozens of nodes 1 . But yet, it is not always the case, sometimes we have a shortage of resources or the workloads are quite heavyweighters themselves. In all these scenarios it would be really nice to scope containerlab out of a single server and make it reach far horizons. And there are several way of making containerlab nodes to reach nodes/systems outside of its container host: Exposing services/ports Exposing management network with routing Connecting remote nodes with bridging Creating VxLAN tunnels across the network To help you navigate these options we've created a multi-node labs documentation article that explains each approach in details. With 0.11.0 specifically, we add the last option in that list, which is most flexible and far-reaching - VxLAN Tunneling . To help containerlab users to provision VxLAN tunnels the helper commands have been created - vxlan create , vxlan delete . The multi-node lab provides a step-by-step coverage of how this all nicely play together and builds a playground to demonstrate the multi-node capabilities. Openvswitch (ovs) bridges support # The versatility of containerlab is in its ability to run anywhere where linux runs and wire up anything that can be packaged in a docker container. But it is quite hard to package in a container a piece of a real hardware. Still, we need it more often than not to connect devices like traffic generators or physical chassis to containerlab labs. It was possible before with linux bridges , now we are adding support for ovs bridges that will allow for even more elaborated and flexible connectivity options. Enhanced topology checks # Gradually we add checks that will make containerlab fail early if anything in the topology file or the host system is not right. In this release containerlab will additionally perform these checks: if node name matches already existing container the deployment won't start if vrnetlab nodes are defined, but virtualization is not available the deployment won't start if <2 vCPU is available a warning will be emitted. Some containerized NOSes demand 2vCPU, so running with 1vCPU is discouraged abrupt deployment if host link referenced in topo file already exists in the host Shell completions # Thanks to our contributor @steiler we now have shell completions , so you can navigate containerlab CLI with guidance and confidence. Manual installation # All these time we relied on package managers to handle containerlab installation. Having support for deb/rpm packages is enough for most of us, but those nerds on Arch and Gentoo . For minorities we added Vrnetlab changes # Vrnetlab version 0.2.0 has been issued to freeze the code compatible with containerlab 0.11.0. If stopped working when you upgraded to 0.11.0, then re-build the images with vrnetlab 0.2.0 Boot delay for vrnetlab routers # To schedule the startup of vrnetlab routers the BOOT_DELAY env variable may be used. It takes a Added Arista vEOS support # Support for virtualized EOS from Arista has been added. Now it is possible to use containerized cEOS and virtualized vEOS . SR OS route to management network # SR OS routers will now have a static route in their Management routing instance to reach the docker management network. This enables SR OS initiated requests to reach services runnign on the management network. Miscellaneous # Default MTU on the management network/bridge is changed to 1500 from 1450. If you run containerlab in the environment where management network MTU is less than 1500, consider setting the needed MTU in the topo file. Fixed multiple cEOS links creation. Add bridge and linux kinds docs. To allow for multi-node labs and also to make it possible to connect nodes with the container host a new endpoint connection was created - host links . MTU on data links is set to 65000 from 1500. This will allow for jumbo frames safe passage. Additional explanations provided for vrnetlab integration and inter-dependency between projects. especially if memory optimization techniques are enabled \u21a9","title":"0.11.0"},{"location":"rn/0.11.0/#release-0110","text":"2021-03-09","title":"Release 0.11.0"},{"location":"rn/0.11.0/#multi-node-labs-with-vxlan-tunneling","text":"Most containerlab users are fine with running labs within a single server/VM. Truthfully speaking, a VM with 32GB RAM can host a very decent lab with dozens of nodes 1 . But yet, it is not always the case, sometimes we have a shortage of resources or the workloads are quite heavyweighters themselves. In all these scenarios it would be really nice to scope containerlab out of a single server and make it reach far horizons. And there are several way of making containerlab nodes to reach nodes/systems outside of its container host: Exposing services/ports Exposing management network with routing Connecting remote nodes with bridging Creating VxLAN tunnels across the network To help you navigate these options we've created a multi-node labs documentation article that explains each approach in details. With 0.11.0 specifically, we add the last option in that list, which is most flexible and far-reaching - VxLAN Tunneling . To help containerlab users to provision VxLAN tunnels the helper commands have been created - vxlan create , vxlan delete . The multi-node lab provides a step-by-step coverage of how this all nicely play together and builds a playground to demonstrate the multi-node capabilities.","title":"Multi-node labs with VxLAN tunneling"},{"location":"rn/0.11.0/#openvswitch-ovs-bridges-support","text":"The versatility of containerlab is in its ability to run anywhere where linux runs and wire up anything that can be packaged in a docker container. But it is quite hard to package in a container a piece of a real hardware. Still, we need it more often than not to connect devices like traffic generators or physical chassis to containerlab labs. It was possible before with linux bridges , now we are adding support for ovs bridges that will allow for even more elaborated and flexible connectivity options.","title":"Openvswitch (ovs) bridges support"},{"location":"rn/0.11.0/#enhanced-topology-checks","text":"Gradually we add checks that will make containerlab fail early if anything in the topology file or the host system is not right. In this release containerlab will additionally perform these checks: if node name matches already existing container the deployment won't start if vrnetlab nodes are defined, but virtualization is not available the deployment won't start if <2 vCPU is available a warning will be emitted. Some containerized NOSes demand 2vCPU, so running with 1vCPU is discouraged abrupt deployment if host link referenced in topo file already exists in the host","title":"Enhanced topology checks"},{"location":"rn/0.11.0/#shell-completions","text":"Thanks to our contributor @steiler we now have shell completions , so you can navigate containerlab CLI with guidance and confidence.","title":"Shell completions"},{"location":"rn/0.11.0/#manual-installation","text":"All these time we relied on package managers to handle containerlab installation. Having support for deb/rpm packages is enough for most of us, but those nerds on Arch and Gentoo . For minorities we added","title":"Manual installation"},{"location":"rn/0.11.0/#vrnetlab-changes","text":"Vrnetlab version 0.2.0 has been issued to freeze the code compatible with containerlab 0.11.0. If stopped working when you upgraded to 0.11.0, then re-build the images with vrnetlab 0.2.0","title":"Vrnetlab changes"},{"location":"rn/0.11.0/#boot-delay-for-vrnetlab-routers","text":"To schedule the startup of vrnetlab routers the BOOT_DELAY env variable may be used. It takes a","title":"Boot delay for vrnetlab routers"},{"location":"rn/0.11.0/#added-arista-veos-support","text":"Support for virtualized EOS from Arista has been added. Now it is possible to use containerized cEOS and virtualized vEOS .","title":"Added Arista vEOS support"},{"location":"rn/0.11.0/#sr-os-route-to-management-network","text":"SR OS routers will now have a static route in their Management routing instance to reach the docker management network. This enables SR OS initiated requests to reach services runnign on the management network.","title":"SR OS route to management network"},{"location":"rn/0.11.0/#miscellaneous","text":"Default MTU on the management network/bridge is changed to 1500 from 1450. If you run containerlab in the environment where management network MTU is less than 1500, consider setting the needed MTU in the topo file. Fixed multiple cEOS links creation. Add bridge and linux kinds docs. To allow for multi-node labs and also to make it possible to connect nodes with the container host a new endpoint connection was created - host links . MTU on data links is set to 65000 from 1500. This will allow for jumbo frames safe passage. Additional explanations provided for vrnetlab integration and inter-dependency between projects. especially if memory optimization techniques are enabled \u21a9","title":"Miscellaneous"},{"location":"rn/0.12.0/","text":"Release 0.12.0 # 2021-03-28 Identity aware sockets # A major improvement to our \"published ports\" feature has landed in 0.12.0. Now it is possible to create Identity Aware sockets for any port of your lab. Identity Aware sockets is a feature of mysocketio service that allows you to let in only authorized users. Authorization is possible via multiple OAuth providers and can be configured to let in users with specific emails and/or with specific domains. Check out how easy it is to create identity aware tunnels with containerlab: With this enhancement it is now possible to create long-running secure tunnels which only the authorized users will be able to access. On-demand veth plumbing # Sometimes it is needed to add some additional connections after the lab has been deployed. Although the labs are quickly to re-spin, sometimes one find themselves in the middle of the use case configuration and there is a need to add another connection between the nodes. With tools veth command it is now possible to add veth pairs between container<->container, containers<->host and containers<->bridge nodes. Now you can add modify your lab connections without redeploying the entire lab[^2]. Safer ways to write clab files # Containerlab got its own JSON Schema that governs the structure of the topology definition files. If you name your topo file as *.clab.yml then some editors like VS Code will automatically provide auto-suggestions and linting for your clab files. Yes, from now on we will call our topo files as clab-files . Create TLS certificates effortlessly # With the new commands tools cert ca create and tools cert sign it is now possible to create CA and node certificates with just two commands embedded into containerlab. Start from here if you always wanted to be able to reduce the number of openssl commands. We also added a lab that pictures the net positive effect of having those commands when creating TLS certificates for secured gNMI connectivity. Ansible inventory # It is imperative to create a nice automation flow that goes from infra deployment to the subsequent configuration automation. When containerlab finishes its deployment job we now create an ansible inventory file for a deployed lab. With this inventory file the users can start their configuration management playbooks and configure the lab for a use case in mind. Smart MTU for management network # Default MTU value for the management network will now be inherited from the MTU of the docker0 bridge interface. Thus, if you configured your docker daemon for a custom MTU, it will be respected by containerlab. Running nodes in bridge network # When management network name is set to bridge , containerlab nodes will be run in the default docker network named bridge . This is the network where containers end up connecting when you do docker run , so running the lab in the default docker network makes it easy to connect your lab with the workloads that have been started by someone else. Releases notification # When a new release comes out we let you know next time you deploy a lab, a tiny message will pop up in the log saying that a new one is ready to make your labs even more efficient and easy. INFO[0001] \ud83c\udf89 New containerlab version 0.12.0 is available! Release notes: https://containerlab.srlinux.dev/rn/0.12.0 Run 'containerlab version upgrade' to upgrade or go check other installation options at https://containerlab.srlinux.dev/install/ Saving SR OS config # SR OS nodes which are launched with vr-sros kind now have support for saving their configuration with containerlab save command. This is implemented via Netconf's <copy-config> RPC that is executed against SR OS node. Miscellaneous # Added support for user-defined node labels which can convey metadata for a given node. Container node needs to support live interface attachment. New TLS certificate logic for SR Linux nodes. If the CA files and node cert exist, the re-deployment of a lab won't issue new certificates and will reuse the existing ones. Additional node property network-mode has been added which allows to deploy the node in the host networking mode. If the changes containerlab makes to LLDP/TX-offload on the management bridge fail, they won't prevent the lab from proceed deploying.","title":"0.12.0"},{"location":"rn/0.12.0/#release-0120","text":"2021-03-28","title":"Release 0.12.0"},{"location":"rn/0.12.0/#identity-aware-sockets","text":"A major improvement to our \"published ports\" feature has landed in 0.12.0. Now it is possible to create Identity Aware sockets for any port of your lab. Identity Aware sockets is a feature of mysocketio service that allows you to let in only authorized users. Authorization is possible via multiple OAuth providers and can be configured to let in users with specific emails and/or with specific domains. Check out how easy it is to create identity aware tunnels with containerlab: With this enhancement it is now possible to create long-running secure tunnels which only the authorized users will be able to access.","title":"Identity aware sockets"},{"location":"rn/0.12.0/#on-demand-veth-plumbing","text":"Sometimes it is needed to add some additional connections after the lab has been deployed. Although the labs are quickly to re-spin, sometimes one find themselves in the middle of the use case configuration and there is a need to add another connection between the nodes. With tools veth command it is now possible to add veth pairs between container<->container, containers<->host and containers<->bridge nodes. Now you can add modify your lab connections without redeploying the entire lab[^2].","title":"On-demand veth plumbing"},{"location":"rn/0.12.0/#safer-ways-to-write-clab-files","text":"Containerlab got its own JSON Schema that governs the structure of the topology definition files. If you name your topo file as *.clab.yml then some editors like VS Code will automatically provide auto-suggestions and linting for your clab files. Yes, from now on we will call our topo files as clab-files .","title":"Safer ways to write clab files"},{"location":"rn/0.12.0/#create-tls-certificates-effortlessly","text":"With the new commands tools cert ca create and tools cert sign it is now possible to create CA and node certificates with just two commands embedded into containerlab. Start from here if you always wanted to be able to reduce the number of openssl commands. We also added a lab that pictures the net positive effect of having those commands when creating TLS certificates for secured gNMI connectivity.","title":"Create TLS certificates effortlessly"},{"location":"rn/0.12.0/#ansible-inventory","text":"It is imperative to create a nice automation flow that goes from infra deployment to the subsequent configuration automation. When containerlab finishes its deployment job we now create an ansible inventory file for a deployed lab. With this inventory file the users can start their configuration management playbooks and configure the lab for a use case in mind.","title":"Ansible inventory"},{"location":"rn/0.12.0/#smart-mtu-for-management-network","text":"Default MTU value for the management network will now be inherited from the MTU of the docker0 bridge interface. Thus, if you configured your docker daemon for a custom MTU, it will be respected by containerlab.","title":"Smart MTU for management network"},{"location":"rn/0.12.0/#running-nodes-in-bridge-network","text":"When management network name is set to bridge , containerlab nodes will be run in the default docker network named bridge . This is the network where containers end up connecting when you do docker run , so running the lab in the default docker network makes it easy to connect your lab with the workloads that have been started by someone else.","title":"Running nodes in bridge network"},{"location":"rn/0.12.0/#releases-notification","text":"When a new release comes out we let you know next time you deploy a lab, a tiny message will pop up in the log saying that a new one is ready to make your labs even more efficient and easy. INFO[0001] \ud83c\udf89 New containerlab version 0.12.0 is available! Release notes: https://containerlab.srlinux.dev/rn/0.12.0 Run 'containerlab version upgrade' to upgrade or go check other installation options at https://containerlab.srlinux.dev/install/","title":"Releases notification"},{"location":"rn/0.12.0/#saving-sr-os-config","text":"SR OS nodes which are launched with vr-sros kind now have support for saving their configuration with containerlab save command. This is implemented via Netconf's <copy-config> RPC that is executed against SR OS node.","title":"Saving SR OS config"},{"location":"rn/0.12.0/#miscellaneous","text":"Added support for user-defined node labels which can convey metadata for a given node. Container node needs to support live interface attachment. New TLS certificate logic for SR Linux nodes. If the CA files and node cert exist, the re-deployment of a lab won't issue new certificates and will reuse the existing ones. Additional node property network-mode has been added which allows to deploy the node in the host networking mode. If the changes containerlab makes to LLDP/TX-offload on the management bridge fail, they won't prevent the lab from proceed deploying.","title":"Miscellaneous"}]}